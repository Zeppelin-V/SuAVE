"AwardNumber#long","Title","NSFOrganization","Program(s)#multi","StartDate#date","LastAmendmentDate#date","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate#date","AwardedAmountToDate#number","Co-PIName(s)#long","PIEmailAddress#hidden","OrganizationStreet#hidden","OrganizationCity","OrganizationState","Address#textlocation#hidden","OrganizationZip#hidden","OrganizationPhone#hidden","NSFDirectorate","ProgramElementCode(s)#multi","ProgramReferenceCode(s)#multi","ARRAAmount#number","Abstract#long","#href","Show Award#link#hidden","#img","#name"
"1606994","2016 Software Infrastructure for Sustained Innovation (SI2) Principal Investigators Workshop","ACI","Software Institutes","1/15/2016","2/8/2016","Francis Timmes","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","12/31/2016","$104,947.00 ","Stanley Ahalt, Shaowen Wang, Matthew Turk","fxt44@mac.com","ORSPA","TEMPE","AZ","ORSPA, TEMPE, AZ","852816011","4809655479","CSE","8004","7433|7556|8004","$0.00 ","This project will host a 1.5-day workshop in Arlington, VA, which will bring together the community of SI2 awardees (with the goal of involving one principal investigator from each SSE and SSI project, many of which are collaborative awards) from approximately 250 awards. The workshop will have participation from SI2 EAGER and RAPID awardees and selected awardees of the ACI VOSS program that examines cyberinfrastructures from the social and organizational perspective. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (a) providing a focused forum for PIs to share technical information with each other and with NSF Program Officers, (b) encouraging exploration of emerging topics, (c) identifying emerging best practices across the supported software projects, (d) stimulating thinking on new ways of achieving software sustainability, and (d) disseminating the shared experiences of the researchers via an online web portal.<br/><br/>The workshop is expected to host close to 150 SI2 and other awardees, other speakers and panelists. The workshop will use a hybrid style, blending a traditional, top-down driven agenda with a interactive, adaptive, participant-driven agenda. <br/><br/>The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability. Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The hybrid approach is innovative and holds the promise of creating rich interactions among PIs, resulting in richer collaboration and learning. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1606994","fuchsia_atom","2016 Software Infrastructure for Sustained Innovation (SI2) Principal Investigators Workshop"
"1256100","The Role of Software and Software Institutes in Computational Science Over Time","ACI","CI-TEAM, Software Institutes","10/1/2012","9/4/2012","Ewa Deelman","CA","University of Southern California","Standard Grant","Daniel Katz","9/30/2014","$74,430.00 ","Miron Livny","deelman@isi.edu","University Park","Los Angeles","CA","University Park, Los Angeles, CA","900890001","2137407762","CSE","7477, 8004","8005|8009|7477","$0.00 ","The workshop will bring together Principle Investigators of the leading software cyberinfrastructure projects and discuss issues relevant to the community as we move into the future. In 2011 and 2012 the OCI Software Infrastructure for Sustained Innovation (SI2) program funded software efforts in small development efforts that can provide software pieces that can be integrated into the larger cyberinfrastructure, and larger collaborations that were delivering significant community software. In addition, new SI2 awards aimed at conceptualizing large-scale software institutes, aimed at providing a fabric for the software needed by domain scientists to achieve breakthroughs in their intra and inter-disciplinary efforts, will be awarded. New NSF initiatives such as EarthCube are defining roadmaps for cyberinfrastructure development in Earth sciences. This workshop will bring together the Principle Investigators of the recent SI2 awards to discuss potential synergies and collaborations, define challenges ahead, discuss the relationship of the SI2 efforts to the planned Software Institutes, and explore the relationship of the OCI-funded software in the context of the broad NSF initiatives such as EarthCube, DataWay, and other planned community-focused efforts.<br/><br/>To achieve its goals, the workshop will focus on these main themes: (i) Discussing SI2 projects within the context of SI2 Institutes and NSF-wide initiative such as EarthCube, DataWay, and others; (ii) sharing experiences in building quality software and services; (iii) fostering collaboration and providing incentives for collaboration and cyberinfrastructure development as a career; and (iv) sustaining the software capabilities in the long term, defining software value.<br/><br/>The workshop will solicit participation from major science and engineering projects that rely on the national cyberinfrastructure for their computations and data management needs. Their participation will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to a number of science and engineering domains. The results of this workshop will then potentially guide cyberinfrastructure development and testing in the future.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1256100","fuchsia_atom","The Role of Software and Software Institutes in Computational Science Over Time"
"1419139","Collaborative Research: Software Sustainability: an SI^2 PI Workshop","ACI","Software Institutes","1/15/2014","1/13/2014","Matthew Jones","CA","University of California-Santa Barbara","Standard Grant","Daniel Katz","12/31/2014","$13,635.00 ","","jones@nceas.ucsb.edu","Office of Research","SANTA BARBARA","CA","Office of Research, SANTA BARBARA, CA","931062050","8058934188","CSE","8004","7556|7433","$0.00 ","This award will support a 1.5 day workshop in Arlington, VA to bring together the community of SI2 awardees with the aims of: 1) serving as a forum for focused PI technical exchange, through an early evening poster session; 2) serving as a forum for discussion of topics of relevance to the PIs from topics emerging both from within NSF and from the broader community, by informing the attendees of emerging best practices, and stimulating thinking on new ways of achieving sustainability and of ensuring that the foundation laid by SI2 is preserved into the future; and 3) gathering experiences and a shared sense of best practice that results in a published workshop report.<br/><br/>The workshop will bring together researchers who are a proto-community of NSF open source software developers. The meeting will examine the characteristics of the community, and consider whether the products from the program can be enhanced by giving the community a new identify and new way of looking at itself. The meeting will also address citation, attribution, and reproducibility, which are three related topics often discussed in the context of data, but less so in the context of software. The attendees will consider practical steps that could be taken to advance software citation and science reproducibility. Finally, sustainability of software is a major topic for NSF and for the SI2 PIs. The meeting will highlight new ways of thinking about software sustainability, drawing on experts in the field and on recent SI2 EAGER funded projects that are studying the community to help the workshop attendees in their thinking about sustainability.<br/><br/>The community outputs of the workshop will be: posters developed by the SI2 PIs that will be shared amongst the attendees and shared more broadly on the workshop web site; an experiences report (licensed under a Creative Commons license) produced by the award PIs, distributed via the workshop web site, via email to participants who will be asked to disseminate among their project colleagues and peers, and via an archive repository through which it will be accessible through a persistent ID; and attendee journalism during the event in the form of a public Google doc and public Twitter stream.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1419139","fuchsia_atom","Collaborative Research: Software Sustainability: an SI^2 PI Workshop"
"1419131","Collaborative Research: Software Sustainability: an SI^2 PI Workshop","ACI","Software Institutes","1/15/2014","1/13/2014","Beth Plale","IN","Indiana University","Standard Grant","Daniel Katz","12/31/2014","$66,224.00 ","","plale@cs.indiana.edu","509 E 3RD ST","Bloomington","IN","509 E 3RD ST, Bloomington, IN","474013654","8128550516","CSE","8004","7556|7433","$0.00 ","This award will support a 1.5 day workshop in Arlington, VA to bring together the community of SI2 awardees with the aims of: 1) serving as a forum for focused PI technical exchange, through an early evening poster session; 2) serving as a forum for discussion of topics of relevance to the PIs from topics emerging both from within NSF and from the broader community, by informing the attendees of emerging best practices, and stimulating thinking on new ways of achieving sustainability and of ensuring that the foundation laid by SI2 is preserved into the future; and 3) gathering experiences and a shared sense of best practice that results in a published workshop report.<br/><br/>The workshop will bring together researchers who are a proto-community of NSF open source software developers. The meeting will examine the characteristics of the community, and consider whether the products from the program can be enhanced by giving the community a new identify and new way of looking at itself. The meeting will also address citation, attribution, and reproducibility, which are three related topics often discussed in the context of data, but less so in the context of software. The attendees will consider practical steps that could be taken to advance software citation and science reproducibility. Finally, sustainability of software is a major topic for NSF and for the SI2 PIs. The meeting will highlight new ways of thinking about software sustainability, drawing on experts in the field and on recent SI2 EAGER funded projects that are studying the community to help the workshop attendees in their thinking about sustainability.<br/><br/>The community outputs of the workshop will be: posters developed by the SI2 PIs that will be shared amongst the attendees and shared more broadly on the workshop web site; an experiences report (licensed under a Creative Commons license) produced by the award PIs, distributed via the workshop web site, via email to participants who will be asked to disseminate among their project colleagues and peers, and via an archive repository through which it will be accessible through a persistent ID; and attendee journalism during the event in the form of a public Google doc and public Twitter stream.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1419131","fuchsia_atom","Collaborative Research: Software Sustainability: an SI^2 PI Workshop"
"1419132","Collaborative Research: Software Sustainability: an SI^2 PI Workshop","ACI","Software Institutes","1/15/2014","1/13/2014","Douglas Thain","IN","University of Notre Dame","Standard Grant","Daniel Katz","12/31/2014","$19,868.00 ","","dthain@nd.edu","940 Grace Hall","NOTRE DAME","IN","940 Grace Hall, NOTRE DAME, IN","465565708","5746317432","CSE","8004","7556|7433","$0.00 ","This award will support a 1.5 day workshop in Arlington, VA to bring together the community of SI2 awardees with the aims of: 1) serving as a forum for focused PI technical exchange, through an early evening poster session; 2) serving as a forum for discussion of topics of relevance to the PIs from topics emerging both from within NSF and from the broader community, by informing the attendees of emerging best practices, and stimulating thinking on new ways of achieving sustainability and of ensuring that the foundation laid by SI2 is preserved into the future; and 3) gathering experiences and a shared sense of best practice that results in a published workshop report.<br/><br/>The workshop will bring together researchers who are a proto-community of NSF open source software developers. The meeting will examine the characteristics of the community, and consider whether the products from the program can be enhanced by giving the community a new identify and new way of looking at itself. The meeting will also address citation, attribution, and reproducibility, which are three related topics often discussed in the context of data, but less so in the context of software. The attendees will consider practical steps that could be taken to advance software citation and science reproducibility. Finally, sustainability of software is a major topic for NSF and for the SI2 PIs. The meeting will highlight new ways of thinking about software sustainability, drawing on experts in the field and on recent SI2 EAGER funded projects that are studying the community to help the workshop attendees in their thinking about sustainability.<br/><br/>The community outputs of the workshop will be: posters developed by the SI2 PIs that will be shared amongst the attendees and shared more broadly on the workshop web site; an experiences report (licensed under a Creative Commons license) produced by the award PIs, distributed via the workshop web site, via email to participants who will be asked to disseminate among their project colleagues and peers, and via an archive repository through which it will be accessible through a persistent ID; and attendee journalism during the event in the form of a public Google doc and public Twitter stream.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1419132","fuchsia_atom","Collaborative Research: Software Sustainability: an SI^2 PI Workshop"
"1265788","Collaborative Research: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis","CHE","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC","9/1/2013","8/28/2013","Shantenu Jha","NJ","Rutgers University New Brunswick","Standard Grant","Evelyn M. Goldfield","8/31/2017","$617,005.00 ","","shantenu.jha@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","33 Knightsbridge Road, Piscataway, NJ","88543925","8489320150","MPS","8004|1253","5918|5946|5950|7433|8009","$0.00 ","Collaborative Research: SI2-CHE<br/>ExTASY Extensible Tools for Advanced Sampling and analYsis<br/><br/>An international team consisting of Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY -- Extensible Toolkit for Advanced Sampling and analYsis, -- a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. <br/><br/> The primary impacts of this project are in the biological sciences. This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins ? a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265788","chartreuse_atom","Collaborative Research: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis"
"1521388","Software Infrastructure for Sustained Innovation - A 2015 SI^2 PI Workshop","ACI","Software Institutes","1/15/2015","1/29/2015","Francis Timmes","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","12/31/2016","$75,527.00 ","Matthew Turk, Stanley Ahalt","fxt44@mac.com","ORSPA","TEMPE","AZ","ORSPA, TEMPE, AZ","852816011","4809655479","CSE","8004","7433|7556|8004","$0.00 ","This project will host a 1.5-day workshop in Arlington, VA, which will bring together the community of SI2 awardees (with the goal of involving one principal investigator from each SSE and SSI project, many of which are collaborative awards) from 214 awards. The workshop will also solicit participation from NSF-supported science and engineering projects that rely on the national research cyberinfrastructure for their computation, communication, and data management needs. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (1) Providing a focused forum for PIs to share technical information with each other and with NSF Program Officers; (2) Encouraging exploration of emerging topics; (3) Identifying emerging best practices across the supported software projects; (4) Stimulating thinking on new ways of achieving software sustainability; and (5) Disseminating the shared experiences of the researchers via an online web portal. The workshop is expected to host 85 SI2 awardees and several other speakers and panelists. The workshop will use a hybrid style, blending a traditional, top-down driven agenda with a interactive, adaptive, participant-driven agenda. <br/><br/>The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability. Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The hybrid approach is innovative and holds the promise of creating rich interactions among PIs, resulting in richer collaboration and learning. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1521388","fuchsia_atom","Software Infrastructure for Sustained Innovation - A 2015 SI^2 PI Workshop"
"1265929","Collaborative Research: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","9/1/2013","8/28/2013","Cecilia Clementi","TX","William Marsh Rice University","Standard Grant","Evelyn M. Goldfield","8/31/2017","$585,000.00 ","","cecilia@rice.edu","6100 MAIN ST","Houston","TX","6100 MAIN ST, Houston, TX","770051827","7133484820","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","Collaborative Research: SI2-CHE<br/>ExTASY Extensible Tools for Advanced Sampling and analYsis<br/><br/>An international team consisting of Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY: Extensible Toolkit for Advanced Sampling and analYsis, a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. <br/><br/> The primary impacts of this project are in the biological sciences. This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins; a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265929","chartreuse_atom","Collaborative Research: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis"
"1265920","Collaborative Proposal: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","9/1/2013","8/28/2013","Mauro Maggioni","NC","Duke University","Standard Grant","Evelyn M. Goldfield","8/31/2016","$148,000.00 ","","mauro.maggioni@duke.edu","2200 W. Main St, Suite 710","Durham","NC","2200 W. Main St, Suite 710, Durham, NC","277054010","9196843030","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","Collaborative Research: SI2-CHE<br/>ExTASY Extensible Tools for Advanced Sampling and analYsis<br/><br/>An international team consisting of Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY -- Extensible Toolkit for Advanced Sampling and analYsis, -- a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. <br/><br/> The primary impacts of this project are in the biological sciences. This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins; a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265920","chartreuse_atom","Collaborative Proposal: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis"
"1453123","Collaborative: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","7/1/2014","9/3/2014","Paul Nerenberg","CA","California Institute of Technology","Standard Grant","Evelyn M. Goldfield","4/30/2016","$38,417.00 ","","psn@caltech.edu","1200 E California Blvd","PASADENA","CA","1200 E California Blvd, PASADENA, CA","911250600","6263956219","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1453123","chartreuse_atom","Collaborative: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces"
"1265889","Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","5/15/2013","5/15/2013","Mark Tuckerman","NY","New York University","Standard Grant","Evelyn M. Goldfield","4/30/2016","$225,375.00 ","","mark.tuckerman@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","70 WASHINGTON SQUARE S, NEW YORK, NY","100121019","2129982121","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265889","chartreuse_atom","Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces"
"1265731","Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","5/15/2013","5/15/2013","Teresa Head-Gordon","CA","University of California-Berkeley","Standard Grant","Evelyn M. Goldfield","4/30/2016","$531,527.00 ","Martin Head-Gordon","thg@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265731","chartreuse_atom","Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces"
"1047577","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory","ACI","OFFICE OF MULTIDISCIPLINARY AC|PROJECTS|Theory|Models|Comput. Method|Software Institutes","10/1/2010","7/18/2013","Todd Martinez","CA","Stanford University","Continuing grant","Evelyn M. Goldfield","9/30/2015","$832,000.00 ","","Todd.Martinez@stanford.edu","3160 Porter Drive","Palo Alto","CA","3160 Porter Drive, Palo Alto, CA","943041212","6507232300","CSE","1253|1978|6881|8004","1303|7573|9216|9263|1253|1712|1765|1978|6881|7564|7569|7644|8004","$0.00 ","This project focuses on implementing newly developed numerical methods and software engineering approaches into widely used computer codes used in biology, chemistry, materials science and engineering, physics, chemical and mechanical engineering and many other science and engineering fields. GAMESS (General Atomic and Molecular Electronic Structure System) is the most broadly used highly scalable computational chemistry program with more than 100,000 users worldwide. NWChem is likewise a very popular computational chemistry code, as are MPQC (Massively Parallel Quantum Chemistry) and AIMS (Ab Initio Multiple Spawning). All of these codes are distributed at no cost via the Web. While these codes have all been developed by computational chemists, they impact a multitude of disciplines in science and engineering. <br/><br/>The new software is highly scalable, thereby enabling the study of fundamentally critical problems, including the structure of liquids (water among them), the formation of atmosrpheric aerosols, heterogeneous catalysis, and photochemistry and photobiology. In addition, the new software is implemented so as to take advantage of new hardware such as the graphical processing units (GPU). Three of the principal investigators (Gordon, Martinez, Windus) are among the world leaders in such developments.<br/><br/>All of the new developments are to be incorporated into both graduate and undergraduate courses in both chemistry and computer science. In addition, downloadable modules are placed on an accessible web site. The results of the research are to be reported at national meetings, such as those organized by the American Chemical Society, IEEE, and the Materials Research Society. Videos of these presentation are placed on the Web, so that researchers and educators who are unable to attend the meetings can access the results of this project. The Web site contains also a set of lessons learned to aid subsequent researchers.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation. Its funding sources include the Division of Chemistry (MPS/CHE), the Office of Multidisciplinary Activities (OMA) of the Directorate of Mathematical and Physical Sciences (MPS), the Office of Cyberinfrastructure (OCI), the Division of Civil, Mechanical and Manufacturing Innovation (ENG/CMMI), and the Division of Materials Research (MPS/DMR).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047577","fuchsia_tree","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory"
"1339624","SI2-SSE: yt: Reusable Components for Simulating, Analyzing and Visualizing Astrophysical Systems","ACI","EXTRAGALACTIC ASTRON & COSMOLO|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","10/1/2013","10/2/2014","Matthew Turk","NY","Columbia University","Standard Grant","Rajiv Ramnath","4/30/2015","$493,793.00 ","Greg Bryan, Matthew Turk","mjturk@illinois.edu","2960 Broadway","NEW YORK","NY","2960 Broadway, NEW YORK, NY","100276902","2128546851","CSE","1217|1253|8004","1206|7433|8005","$0.00 ","Computational modeling of astrophysical phenomena has grown in sophistication and realism, leading to a diversity of complex simulation platforms, each utilizing its own mechanism and format for representing particles and fluids. Similarly, most of the data analysis is conducted with tools developed in isolation and targeted to a specific simulation platform or research domain; very little systematic and direct technology transfer between astrophysical researchers exists. The yt project is a parallel analysis and visualization toolkit designed to support a collaborative community of researchers as they focus on answering physical questions, rather than the technical mechanics of reading, processing and visualizing data formats. This project will enable the development of advanced, physics-based modules that apply universally across simulation codes, advancing scientific inquiry and enabling more efficient utilization of computational and human resources. In doing so, it will help advance a myriad of research goals from the study of black hole binaries to the growth of cosmic structure. In addition, the project will serve as a touchstone for collaboration and cross-code utilization between many groups studying diverse phenomena. Moreover, the project will be developed through a community-oriented process, engaging a wide range of participants.<br/><br/>The infrastructure development in this research will enable these capabilities by broadening the applicable simulation platforms within yt, enabling cross-code utilization of microphysical solvers and physics modules and in situ analysis, and developing collaborative platforms for the exploration of astrophysical datasets. In particular, it will develop the capabilities of yt in three primary mechanisms. The first is to enable support for additional, fundamentally different simulation platforms such as smoothed particle hydrodynamics, unstructured mesh, and non-Cartesian coordinate systems. The second is to provide simulation instrumentation components to ease the process of developing simulation codes, interfacing and exchanging technology between those simulation codes, and to enable deeper, on-the-fly integration of astrophysical simulation codes with yt and other analysis toolkits. The final focus is on developing interface components to enable collaborative and interactive exploration of data utilizing web-based platforms. An explicit goal of this SI2-SSE project is the development of collaborative relationships between scientists, furthering the development of the field as a whole. By conducting all business in the open with a focus on developing and encouraging collaborative, welcoming environments for contributors and researchers, this SSE will help to foster a level playing field that is more accessible to all parties, particularly women and underrepresented minorities. An explicit milestone of this project is to streamline the process of conducting direct outreach through scientific visualization, greatly expanding the domains and individuals engaged in STEM-based public outreach.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339624","fuchsia_atom","SI2-SSE: yt: Reusable Components for Simulating, Analyzing and Visualizing Astrophysical Systems"
"1265712","Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","5/15/2013","5/15/2013","Jay Ponder","MO","Washington University","Standard Grant","Evelyn M. Goldfield","4/30/2017","$257,400.00 ","","ponder@dasher.wustl.edu","CAMPUS BOX 1054","SAINT LOUIS","MO","CAMPUS BOX 1054, SAINT LOUIS, MO","631304899","3147474134","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265712","chartreuse_atom","Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces"
"1265817","SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","6/1/2013","5/31/2013","Emre Brookes","TX","University of Texas Health Science Center San Antonio","Standard Grant","Evelyn M. Goldfield","5/31/2017","$263,926.00 ","","emre@biochem.uthscsa.edu","7703 FLOYD CURL DR","SAN ANTONIO","TX","7703 FLOYD CURL DR, SAN ANTONIO, TX","782293901","2105672340","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program. The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users. Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols. Software is being developed and disseminated with continual feedback from the research teams' large international user-base. These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems. <br/><br/>A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS), or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models. It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite. This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. <br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265817","chartreuse_atom","SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter"
"1265850","SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","6/1/2013","5/31/2013","Jianhan Chen","KS","Kansas State University","Standard Grant","Evelyn M. Goldfield","5/31/2017","$224,492.00 ","","jianhanc@ksu.edu","2 FAIRCHILD HALL","MANHATTAN","KS","2 FAIRCHILD HALL, MANHATTAN, KS","665061103","7855326804","MPS","1253|8004","5918|5946|5950|7433|8009|9150","$0.00 ","An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program. The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users. Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols. Software is being developed and disseminated with continual feedback from the research teams' large international user-base. These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems. <br/><br/>A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS), or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models. It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite. This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. <br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265850","chartreuse_atom","SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter"
"1265704","Collaborative Research: SI2 CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","5/15/2013","5/15/2013","David Case","NJ","Rutgers University New Brunswick","Standard Grant","Evelyn M. Goldfield","4/30/2016","$328,229.00 ","","case@biomaps.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","33 Knightsbridge Road, Piscataway, NJ","88543925","8489320150","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265704","chartreuse_atom","Collaborative Research: SI2 CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces"
"1047696","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory","ACI","Software Institutes|PROJECTS|OFFICE OF MULTIDISCIPLINARY AC|Theory|Models|Comput. Method","10/1/2010","8/7/2013","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Continuing grant","Evelyn M. Goldfield","9/30/2015","$396,000.00 ","","evaleev@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","Sponsored Programs 0170, BLACKSBURG, VA","240610001","5402315281","CSE","8004|1978|1253|6881","1303|7573|9216|9263|1978|1253|1712|1765|6881|7564|7569|7644|8004","$0.00 ","This project focuses on implementing newly developed numerical methods and software engineering approaches into widely used computer codes used in biology, chemistry, materials science and engineering, physics, chemical and mechanical engineering and many other science and engineering fields. GAMESS (General Atomic and Molecular Electronic Structure System) is the most broadly used highly scalable computational chemistry program with more than 100,000 users worldwide. NWChem is likewise a very popular computational chemistry code, as are MPQC (Massively Parallel Quantum Chemistry) and AIMS (Ab Initio Multiple Spawning). All of these codes are distributed at no cost via the Web. While these codes have all been developed by computational chemists, they impact a multitude of disciplines in science and engineering. <br/><br/>The new software is highly scalable, thereby enabling the study of fundamentally critical problems, including the structure of liquids (water among them), the formation of atmosrpheric aerosols, heterogeneous catalysis, and photochemistry and photobiology. In addition, the new software is implemented so as to take advantage of new hardware such as the graphical processing units (GPU). Three of the principal investigators (Gordon, Martinez, Windus) are among the world leaders in such developments.<br/><br/>All of the new developments are to be incorporated into both graduate and undergraduate courses in both chemistry and computer science. In addition, downloadable modules are placed on an accessible web site. The results of the research are to be reported at national meetings, such as those organized by the American Chemical Society, IEEE, and the Materials Research Society. Videos of these presentation are placed on the Web, so that researchers and educators who are unable to attend the meetings can access the results of this project. The Web site contains also a set of lessons learned to aid subsequent researchers.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation. Its funding sources include the Division of Chemistry (MPS/CHE), the Office of Multidisciplinary Activities (OMA) of the Directorate of Mathematical and Physical Sciences (MPS), the Office of Cyberinfrastructure (OCI), the Division of Civil, Mechanical and Manufacturing Innovation (ENG/CMMI), and the Division of Materials Research (MPS/DMR).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047696","fuchsia_tree","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory"
"1047772","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|PROJECTS|Theory|Models|Comput. Method|COLLABORATIVE RESEARCH|COMMS|CIRCUITS & SENS SYS|Software Institutes","10/1/2010","7/15/2013","Mark Gordon","IA","Iowa State University","Continuing grant","Evelyn M. Goldfield","9/30/2015","$1,816,394.00 ","Theresa Windus, Masha Sosonkina","mark@si.msg.chem.iastate.edu","1138 Pearson","AMES","IA","1138 Pearson, AMES, IA","500112207","5152945225","CSE","1253|1712|1978|6881|7298|7564|8004","1303|1765|1978|5912|5978|7433|7569|7573|7644|9150|9215|9216|9263|1253|1712|6881|7564|8004|8009","$0.00 ","This project focuses on implementing newly developed numerical methods and software engineering approaches into widely used computer codes used in biology, chemistry, materials science and engineering, physics, chemical and mechanical engineering and many other science and engineering fields. GAMESS (General Atomic and Molecular Electronic Structure System) is the most broadly used highly scalable computational chemistry program with more than 100,000 users worldwide. NWChem is likewise a very popular computational chemistry code, as are MPQC (Massively Parallel Quantum Chemistry) and AIMS (Ab Initio Multiple Spawning). All of these codes are distributed at no cost via the Web. While these codes have all been developed by computational chemists, they impact a multitude of disciplines in science and engineering. <br/><br/>The new software is highly scalable, thereby enabling the study of fundamentally critical problems, including the structure of liquids (water among them), the formation of atmosrpheric aerosols, heterogeneous catalysis, and photochemistry and photobiology. In addition, the new software is implemented so as to take advantage of new hardware such as the graphical processing units (GPU). Three of the principal investigators (Gordon, Martinez, Windus) are among the world leaders in such developments.<br/><br/>All of the new developments are to be incorporated into both graduate and undergraduate courses in both chemistry and computer science. In addition, downloadable modules are placed on an accessible web site. The results of the research are to be reported at national meetings, such as those organized by the American Chemical Society, IEEE, and the Materials Research Society. Videos of these presentation are placed on the Web, so that researchers and educators who are unable to attend the meetings can access the results of this project. The Web site contains also a set of lessons learned to aid subsequent researchers.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation. Its funding sources include the Division of Chemistry (MPS/CHE), the Office of Multidisciplinary Activities (OMA) of the Directorate of Mathematical and Physical Sciences (MPS), the Office of Cyberinfrastructure (OCI), the Division of Civil, Mechanical and Manufacturing Innovation (ENG/CMMI), and the Division of Materials Research (MPS/DMR).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047772","fuchsia_tree","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory"
"1147794","Collaborative Research: SI2-SSI: Sustainable Development of Next-Generation Software in Quantum Chemistry","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|CHEMISTRY PROJECTS|Software Institutes","6/1/2012","7/6/2012","Thomas Crawford","VA","Virginia Polytechnic Institute and State University","Standard Grant","Daniel Katz","5/31/2015","$325,140.00 ","","crawdad@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","Sponsored Programs 0170, BLACKSBURG, VA","240610001","5402315281","CSE","1253|1712|1991|8004","7433|7683|8009|9216|9263|1253|1712|1991|8004","$0.00 ","Quantum chemistry can provide highly accurate results for arbitrary molecular systems, making it a vital component in many different disciplines such as materials science, biology, physics, chemical engineering, mechanical engineering, environmental science, geology, and others. It is particularly critical in the rational design of drugs, catalysts, organic electronics, nanostructured materials, and other designed materials. Because of their steep computational costs, quantum chemistry codes must exploit parallel computing and must constantly adapt to rapidly changing high performance computing technologies. This creates a significant barrier for the adoption of new technologies into quantum chemistry codes. Our project involves the development of a parallel, highly reusable library for advanced numerical approximations in quantum chemistry. This will be the first unified library of such techniques, designed for high performance and also reusability by independent research groups. The PANACHE (PArallel Numerical Approximations in CHemistry Engine) library will fill this need. To maximize its impact, PANACHE is being designed to be used by multiple quantum chemistry software packages. PANACHE dramatically speeds up quantum computations, making it much easier to gain insight into a wide array of problems, from studies of reaction mechanisms in catalysis to the design of improved organic photoelectronic devices. <br/><br/>Our highly interdisciplinary project (involving two theoretical chemists and one computational scientist as co-PI?s) provides excellent opportunities for training graduate students and postdocs in the areas of numerical methods, high-performance computing, quantum mechanics, and computational chemistry. Computer code resulting from this project will be released as freely-available open-source software, enabling its use with any other software package. Workshops on the new software will be held to introduce these new tools to other software developers, and online training material and graduate course material will be developed to improve education in the use of numerical methods in computational science and quantum chemistry.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147794","fuchsia_atom","Collaborative Research: SI2-SSI: Sustainable Development of Next-Generation Software in Quantum Chemistry"
"1147843","Collaborative Research: SI2-SSI: Sustainable Development of Next-Generation Software in Quantum Chemistry","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|CHEMISTRY PROJECTS|Software Institutes","6/1/2012","7/6/2012","Charles Sherrill","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","5/31/2017","$614,654.00 ","Edmond Chow","sherrill@gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","1253|1712|1991|8004","7433|7683|8009|9216|9263|1253|1712|1991|8004","$0.00 ","Quantum chemistry can provide highly accurate results for arbitrary molecular systems, making it a vital component in many different disciplines such as materials science, biology, physics, chemical engineering, mechanical engineering, environmental science, geology, and others. It is particularly critical in the rational design of drugs, catalysts, organic electronics, nanostructured materials, and other designed materials. Because of their steep computational costs, quantum chemistry codes must exploit parallel computing and must constantly adapt to rapidly changing high performance computing technologies. This creates a significant barrier for the adoption of new technologies into quantum chemistry codes. Our project involves the development of a parallel, highly reusable library for advanced numerical approximations in quantum chemistry. This will be the first unified library of such techniques, designed for high performance and also reusability by independent research groups. The PANACHE (PArallel Numerical Approximations in CHemistry Engine) library will fill this need. To maximize its impact, PANACHE is being designed to be used by multiple quantum chemistry software packages. PANACHE dramatically speeds up quantum computations, making it much easier to gain insight into a wide array of problems, from studies of reaction mechanisms in catalysis to the design of improved organic photoelectronic devices. <br/><br/>Our highly interdisciplinary project (involving two theoretical chemists and one computational scientist as co-PI?s) provides excellent opportunities for training graduate students and postdocs in the areas of numerical methods, high-performance computing, quantum mechanics, and computational chemistry. Computer code resulting from this project will be released as freely-available open-source software, enabling its use with any other software package. Workshops on the new software will be held to introduce these new tools to other software developers, and online training material and graduate course material will be developed to improve education in the use of numerical methods in computational science and quantum chemistry.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147843","fuchsia_atom","Collaborative Research: SI2-SSI: Sustainable Development of Next-Generation Software in Quantum Chemistry"
"1535651","SI2-SSE: yt: Reusable Components for Simulating, Analyzing and Visualizing Astrophysical Systems","ACI","EXTRAGALACTIC ASTRON & COSMOLO|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","11/1/2014","3/9/2015","Matthew Turk","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rajiv Ramnath","9/30/2016","$427,003.00 ","","mjturk@illinois.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","1217|1253|8004","1206|7433|8005|8004","$0.00 ","Computational modeling of astrophysical phenomena has grown in sophistication and realism, leading to a diversity of complex simulation platforms, each utilizing its own mechanism and format for representing particles and fluids. Similarly, most of the data analysis is conducted with tools developed in isolation and targeted to a specific simulation platform or research domain; very little systematic and direct technology transfer between astrophysical researchers exists. The yt project is a parallel analysis and visualization toolkit designed to support a collaborative community of researchers as they focus on answering physical questions, rather than the technical mechanics of reading, processing and visualizing data formats. This project will enable the development of advanced, physics-based modules that apply universally across simulation codes, advancing scientific inquiry and enabling more efficient utilization of computational and human resources. In doing so, it will help advance a myriad of research goals from the study of black hole binaries to the growth of cosmic structure. In addition, the project will serve as a touchstone for collaboration and cross-code utilization between many groups studying diverse phenomena. Moreover, the project will be developed through a community-oriented process, engaging a wide range of participants.<br/><br/>The infrastructure development in this research will enable these capabilities by broadening the applicable simulation platforms within yt, enabling cross-code utilization of microphysical solvers and physics modules and in situ analysis, and developing collaborative platforms for the exploration of astrophysical datasets. In particular, it will develop the capabilities of yt in three primary mechanisms. The first is to enable support for additional, fundamentally different simulation platforms such as smoothed particle hydrodynamics, unstructured mesh, and non-Cartesian coordinate systems. The second is to provide simulation instrumentation components to ease the process of developing simulation codes, interfacing and exchanging technology between those simulation codes, and to enable deeper, on-the-fly integration of astrophysical simulation codes with yt and other analysis toolkits. The final focus is on developing interface components to enable collaborative and interactive exploration of data utilizing web-based platforms. An explicit goal of this SI2-SSE project is the development of collaborative relationships between scientists, furthering the development of the field as a whole. By conducting all business in the open with a focus on developing and encouraging collaborative, welcoming environments for contributors and researchers, this SSE will help to foster a level playing field that is more accessible to all parties, particularly women and underrepresented minorities. An explicit milestone of this project is to streamline the process of conducting direct outreach through scientific visualization, greatly expanding the domains and individuals engaged in STEM-based public outreach.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535651","fuchsia_atom","SI2-SSE: yt: Reusable Components for Simulating, Analyzing and Visualizing Astrophysical Systems"
"1265821","SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","6/1/2013","5/31/2013","Paul Butler","TN","University of Tennessee Knoxville","Standard Grant","Evelyn M. Goldfield","5/31/2017","$611,582.00 ","","pbutler@utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","1 CIRCLE PARK, KNOXVILLE, TN","379960003","8659743466","MPS","1253|8004","5918|5946|5950|7433|8009|9150","$0.00 ","An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program. The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users. Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols. Software is being developed and disseminated with continual feedback from the research teams' large international user-base. These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems. <br/><br/>A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS), or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models. It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite. This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. <br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265821","chartreuse_atom","SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter"
"1265660","Collaborative: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","5/15/2013","5/15/2013","Paul Nerenberg","CA","Claremont McKenna College","Standard Grant","Evelyn M. Goldfield","10/31/2014","$56,983.00 ","","psn@caltech.edu","500 E. Ninth St.","Claremont","CA","500 E. Ninth St., Claremont, CA","917115929","9096218117","MPS","1253|8004","5918|5946|5950|7433|8009","$0.00 ","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265660","chartreuse_atom","Collaborative: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces"
"1450217","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC","8/1/2015","6/24/2015","Mark Gordon","IA","Iowa State University","Standard Grant","Rajiv Ramnath","7/31/2019","$1,200,000.00 ","Theresa Windus","mark@si.msg.chem.iastate.edu","1138 Pearson","AMES","IA","1138 Pearson, AMES, IA","500112207","5152945225","CSE","8004|1253","7433|8009|9150","$0.00 ","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost. This project is supported by programs in the Division of Chemistry in MPS and the Division of Advanced Cyberinfrastructure in CISE. <br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450217","fuchsia_atom","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science"
"1440811","SI2-SSE: Development and Implementation of Software Elements using State-of-the-Art Computational Methodology to Advance Modeling Heterogeneities and Mixing in Earth's Mantle","ACI","Software Institutes|EarthCube|GEOPHYSICS","8/1/2014","8/12/2014","Elbridge Puckett","CA","University of California-Davis","Standard Grant","Rajiv Ramnath","7/31/2017","$487,115.00 ","Magali Billen","egpuckett@ucdavis.edu","OR/Sponsored Programs","Davis","CA","OR/Sponsored Programs, Davis, CA","956186134","5307547700","CSE","8004|8074|1574","7433|8005","$0.00 ","This project involves the development and implementation of scientific software elements (SSEs), based on modern, high-resolution numerical methods for modeling steep gradients and sharp interfaces of material properties in viscous fluids in the presence of thermal convection. The goal of this project is to address a compelling need in geodynamics, in which continuum mechanics is applied to the study of geophysical processes, such as convection in the Earth?s mantle. A primary tool of geodynamics research is computational models of the flow of the extremely viscous interior of the Earth over hundreds of millions to billions of years. A long-standing challenge for these models is the need to accurately model sharp interfaces in temperature, viscosity, and other properties. These arise when, for example, modeling subduction (in which a cold tectonic plate plunges into the hot interior) or rising plumes (in which a hot boundary layer instability rises through the mantle and encounters the cold boundary layer of the tectonic plates). The project will foster interdisciplinary communication and the application of state-of-the-art applied and computational mathematics to fundamental problems in geophysics. It involves early-career mathematical scientists in the application of state-of-the-art numerical algorithms to geodynamics and, in particular, will provide an opportunity to increase the participation of women in mathematics and geodynamics research.<br/><br/>This project involves the design and implementation of state-of-the-art SSEs for computing the evolution of significant processes in the Earth's mantle in which an essential feature of the problem is the presence of one or more moving boundaries, interfaces, or steep gradients in temperature, composition, or viscosity. The SSEs will address two critical issues that currently limit modern mantle convection simulations. All computational models of mantle convection currently in use produce significant overshoot and undershoot in the neighborhood of sharp gradients in temperature and viscosity. The cause of these overshoots and undershoots is a numerical artifact, which is well-known and well-understood in other fields, such as the computational shock physics community. Over the past thirty years researchers in computational shock physics have developed a variety of high-order accurate, monotone numerical methods, which preserve the physically correct maximum and minimum values of the computed quantities, while producing a high-order accurate numerical approximation of these quantities. Another compelling need in computational geodynamics is the ability to track discontinuous jumps in quantities such as material composition. Here high-order accurate interface tracking algorithms are required, since these fields undergo large-scale deformation, yet quantities such as the viscosity must be accurately approximated at the interface between two materials.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440811","fuchsia_atom","SI2-SSE: Development and Implementation of Software Elements using State-of-the-Art Computational Methodology to Advance Modeling Heterogeneities and Mixing in Earth's Mantle"
"1148116","Collaborative Research: SI2-SSI: Open Source Support for Massively Parallel, Generic Finite Element Methods","ACI","Software Institutes","8/1/2012","8/9/2012","Wolfgang Bangerth","TX","Texas A&M Research Foundation","Standard Grant","Rajiv Ramnath","7/31/2017","$1,311,834.00 ","Yassin Hassan","bangerth@colostate.edu","400 Harvey Mitchell Parkway, S","College Station","TX","400 Harvey Mitchell Parkway, S, College Station, TX","778454321","9798458600","CSE","8004","7433|8009|8004","$0.00 ","Partial differential equations are used in a wide variety of applications as<br/>mathematical models. Their numerical solution is, consequently, of prime<br/>importance for the accurate simulation and optimization of processes in the<br/>sciences, engineering, and beyond.<br/>The last decade saw the emergence of large and successful libraries that<br/>support such applications. While these libraries provide most of what such<br/>codes need for small-scale computations, many realistic applications yield<br/>problems of hundreds of millions or billions of unknowns and require clusters<br/>with thousands of processor cores, but there is currently little generic<br/>support for such problems, limiting access to the many large publicly<br/>supported computing facilities to experts in computational science and<br/>excluding scientists from many fields for whom computational simulation would<br/>be a useful tool. This project intends to build the software infrastructure that will allow a<br/>wide cross section of scientists to utilize these large resources.<br/><br/><br/>This project intends to support the software infrastructure for the<br/>large-scale solution of partial differential equations on massively parallel<br/>computational resources in a generic way. It will build on two of the most<br/>successful libraries for scientific computing, the finite element library<br/>deal.II, and Trilinos that provides the parallel linear algebra capabilities<br/>for the former. Specifically, we will: (i) Make support for massively parallel<br/>computations ubiquitous in deal.II; (ii) Research and develop seamless support<br/>for problems with billions of unknowns in both libraries and improve the<br/>interaction between the two; (iii) Exploit intra-node parallelism on today's<br/>clusters; (iv) Ensure the applicability of our work on a broad basis by<br/>implementing two real-world applications. <br/>Both deal.II and Trilinos have large, active and diverse developer and user<br/>communities, and this project will actively engage these communities through<br/>user meetings, short courses, regularly taught classes, mailing lists, and<br/>direct contact in focused projects.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148116","fuchsia_atom","Collaborative Research: SI2-SSI: Open Source Support for Massively Parallel, Generic Finite Element Methods"
"1339840","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments","ACI","Software Institutes","10/1/2013","9/13/2013","Henry Childs","OR","University of Oregon Eugene","Standard Grant","Rajiv Ramnath","9/30/2017","$235,961.00 ","","hankchilds@gmail.com","5219 UNIVERSITY OF OREGON","Eugene","OR","5219 UNIVERSITY OF OREGON, Eugene, OR","974035219","5413465131","CSE","8004","7433|8009","$0.00 ","Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this work. <br/><br/>To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. <br/><br/>This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339840","fuchsia_atom","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments"
"1148287","SI2-SSE: General Tensor Software Elements for Quantum Chemistry, Tensor Network Theories, and Beyond","ACI","OFFICE OF MULTIDISCIPLINARY AC|CHEMISTRY PROJECTS|Software Institutes","6/1/2012","6/25/2012","Garnet Chan","NY","Cornell University","Standard Grant","Daniel Katz","10/31/2012","$386,818.00 ","","garnetc@caltech.edu","373 Pine Tree Road","Ithaca","NY","373 Pine Tree Road, Ithaca, NY","148502820","6072555014","CSE","1253|1991|8004","7433|7683|8005|9216|9263|1253|1991|8004","$0.00 ","The PI will develop reusable software components to accelerate innovation in science that relies on high-dimensional tensor computations. While the components are informed by use-cases taken from quantum chemistry, the challenges of tensor computation are universal, and span diverse areas of science and engineering. These problems range from simulations of nuclear spin spectra, to quantum chemical calculations on molecules, to psychometric analysis, to numerical general relativity. The overarching aim of the funded work is to develop reusable tensor software elements, based on modern sustainable software practices, to benefit tensor algorithmic development in the scientific community at large. In addition, beyond the broad scientific impacts of the software, our education and outreach agenda comprises a multi-tiered effort to uplift the ability of the science and engineering community to reason about high-dimensional tensor and matrix computations.<br/><br/>Important aspects of the software elements will include (i) expressive programming interfaces for rapid prototyping of tensor based theories (ii) layered tensor libraries for dense, block-sparse, and out-of-core tensors that provide peak-performance implementations of the above interfaces, (iii) a multi-linear algebra package for general high dimensional computations, based on the matrix product state approach, and (iv) tensor virtual machine technology that abstracts algorithm development from hardware, and which provides a framework for optimizing compiler transformations to adapt algorithms to the memory access, communication networks, and processor characteristics of modern computer architectures.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148287","fuchsia_atom","SI2-SSE: General Tensor Software Elements for Quantum Chemistry, Tensor Network Theories, and Beyond"
"1450180","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","PHYSICAL & DYNAMIC METEOROLOGY|Software Institutes|EarthCube","8/1/2015","8/4/2015","Mohan Ramamurthy","CO","University Corporation For Atmospheric Res","Standard Grant","Rajiv Ramnath","7/31/2018","$98,702.00 ","","mohan@ucar.edu","3090 Center Green Drive","Boulder","CO","3090 Center Green Drive, Boulder, CO","803012252","3034971000","CSE","1525|8004|8074","4444|7433|8009","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450180","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1550350","Collaborative Research: Si2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure","ACI","Software Institutes","8/1/2016","8/2/2016","Lucas Wilson","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","7/31/2017","$120,374.00 ","","lwilson@tacc.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","101 E. 27th Street, Suite 5.300, Austin, TX","787121532","5124716424","CSE","8004","7433|8004|8009","$0.00 ","The aggregate computing power of consumer devices - desktop and laptop computers, tablets, smartphones - far exceeds that of institutional computing resources. ""Volunteer computing"" uses these consumer devices, volunteered by their owners, to do scientific computing. In addition to providing additional, much-needed computational resources to scientists, volunteer computing publicizes scientific research and engages citizens in science. BOINC is the primary software system for volunteer computing. It was developed at UC Berkeley with NSF support starting in 2002. Until now, BOINC has been based on a model of independent competing projects. Scientists set up their own BOINC servers, port their applications to run on BOINC, and publicize their projects to attract volunteers. There are about 40 such projects, in many areas of science: examples include Einstein@home, CERN, and SETI@home (astrophysics), Rosetta@home and GPUGrid.net (biomedicine), Climateprediction.net (climate study), and IBM World Community Grid (multiple applications). Together these projects have about 400,000 active volunteers and 12 PetaFLOPS of computing throughput. This model, while successful to an extent, has reached a limit. The number of projects and volunteers has stagnated. Volunteer computing is supplying lots of computing power, but only to a few research projects. For other scientists, there are two major barriers. First, creating a BOINC project has significant overhead: learning a new technology, creating a public web site, generating publicity, and so on. Second, volunteer computing is risky and uncertain; there is no guarantee that a new project will attract volunteers. This project aims to break this barrier, and to make volunteer computing available to all scientists doing high-throughput computing, by replacing the competing-projects model with a new ""central broker"" model. The new model has two related parts: 1) the integration of BOINC with existing high-throughput computing facilities such as supercomputing centers and science portals. Jobs currently run on cluster nodes will be transparently offloaded to volunteer computers. Scientists using these facilities will see faster turnaround times; they'll benefit from volunteer computing without even knowing it's there. 2) The project will change the volunteer interface so that participants sign up for scientific areas and goals rather then for particular projects. For example, a participant might sign up to contribute to cancer research. A central broker, to be developed as part of this project, would dynamically assign their computing resources to projects doing that type of research. This project mobilizes public support for and interest in scientific research by encouraging ""volunteer computing"" and engaging citizens in the conduct of the research itself. It simultaneously advances NSF's mission to advance science while broadening citizen engagement.<br/><br/>The first year of this project will prototype each of these parts, and will integrate BOINC with TACC and nanoHub. Integrating BOINC with existing HTC systems involves several subtasks: 1) Job routing: modifying existing job processing systems used by TACC and nanoHub (Launcher and Rappture respectively) to decide when a group of jobs should be offloaded to BOINC. This decision might involve the estimated runtime of the jobs, input and output file sizes, data sensitivity, the deadline or priority of the jobs, and the identity of the job submitter. 2) Job format conversion: mapping job descriptions (input/output file specifications, resource and timing requirements) to their BOINC equivalents. 3) Application packaging: adapting existing applications (such as nanoHub's simulation tools and TACC's Autodock) to run under BOINC. We will use BOINC's virtual machine facility, which packages an application as a virtual machine image (VirtualBox or Docker) and a program to be run within the VM. This allows existing Linux applications to run on consumer desktop platforms such as Windows and Mac, as well as providing a strong security sandbox and an efficient application-independent checkpoint/restart mechanism. 4) File handling: moving input and output files between existing storage systems (typically inaccessible from outside firewalls) to Internet-visible servers. This will use existing BOINC components that manage files based on hashes to eliminate duplicate transfer and storage of files. 5) Job monitoring and control: adapting existing web- or command-line based tools for monitoring the progress of batches of jobs, and for aborting jobs, to work with BOINC. This will use existing Web RPCs provided by BOINC for these purposes. This project will carry out these tasks by designing and implementing new software as needed, testing for correctness, performance, and scalability, and deploying it in a production environment. The second part of the project - a brokering system for allocating computing power based on volunteer scientific preferences - will be designed and prototyped. This involves several subtasks: 1) Designing a schema for volunteer preferences, including scientific areas and sub-areas, project nationality and institutions, specific projects and applications, inclusions/exclusions, and so on. 2) Designing a schema for assigning attributes to job streams (e.g. their area, sub-area, institution, etc.), and for assigning quotas or priorities to job streams. 3) Designing a relational database for storing the above information. 4) Designing and implementing policies for assigning volunteer resources to job streams in a way that respects volunteer preferences and optimizes quota, fairness, and throughput criteria. This will be implemented as a BOINC ""account manager"" so that volunteers see a single interface rather than lots of separate projects and web sites.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550350","fuchsia_atom","Collaborative Research: Si2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure"
"1339873","SI2-SSI: Sustaining Globus Toolkit for the NSF Community (Sustain-GT)","ACI","Software Institutes","10/1/2013","7/25/2016","Steven Tuecke","IL","University of Chicago","Standard Grant","Rajiv Ramnath","9/30/2018","$1,439,888.00 ","","tuecke@uchicago.edu","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004","7433|8004|8009","$0.00 ","Science and engineering depend increasingly on the ability to collaborate and federate resources across distances. This observation holds whether a single investigator is accessing a remote computer, a small team is analyzing data from an engineering experiment, or an international collaboration is involved in a multi-decade project such as the Large Hadron Collider (LHC). Any distributed collaboration and resource federation system requires methods for authentication and authorization, data movement, and remote computation. Of the many solutions that have been proposed to these problems, the Globus Toolkit (GT) has proven the most persistently applicable across multiple fields, geographies, and project scales. GT resource gateway services and client libraries are used by tens of thousands of people every day to perform literally tens of millions of tasks at thousands of sites, enabling discovery across essentially every science and engineering discipline supported by the NSF. As new, innovative techniques and technologies for collaboration and scientific workflows are developed, and as new computing and instrument resources are added to the national cyberinfrastructure, these technologies and other improvements must be added and integrated into GT so that it can continue to provide an advanced and robust technology for solving scientific research problems.<br/><br/>The Sustain-GT project builds on past success to ensure that GT resource gateway services will continue to meet the challenges faced by NSF science and engineering communities. These challenges include: multiple-orders-of-magnitude increases in the volume of data generated, stored, and transmitted; much bigger computer systems and correspondingly larger and more complex computations; much faster networks; many more researchers, educators, and students engaged in data-intensive and computational research; and rapidly evolving commodity Web and Cloud computing environments. With the help of a new User Requirements Board, Sustain-GT will respond to community demands to evolve the GT resource gateway services with superior functionality, scalability, availability, reliability, and manageability. Sustain-GT will also provide the NSF community with high quality support and rapid-response bug fix services, as is required to sustain a heavily used, production system like GT.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339873","fuchsia_atom","SI2-SSI: Sustaining Globus Toolkit for the NSF Community (Sustain-GT)"
"1047980","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|COFFES|Software Institutes","9/15/2010","9/7/2010","George Biros","GA","Georgia Tech Research Corporation","Standard Grant","Thomas F. Russell","11/30/2012","$250,000.00 ","","biros@ices.utexas.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","1253|7478|7552|8004","1253|7478|7552","$0.00 ","We propose to develop and deploy mathematical software for boundary-value problems in three-dimensional complex geometries. The algorithms in the library will be based on integral equation formulations. The library will be designed to scale on novel computing platforms that comprise special accelerators and manycore architectures. <br/><br/>Integral equations can be used to conduct simulations on many problems in science and engineering with significant societal impact. Three example applications on which the proposed simulation technologies will have an impact in this project are microfluidic chips, biomolecular electrostatics, and plasma physics. First, microfluidic chips are submillimeter-sized devices used for medical diagnosis and drug design. Optimizing the function of such devices at low cost requires efficient computer simulation tools, such as the ones we propose to develop. Second, understanding the structure and function of biomolecules such as DNA and proteins is crucial in biotechnology. The proposed technologies can be used to resolve bimolecular electrostatic interactions. Third, plasma physics, which is related to fusion nuclear reactors, includes electrostatic interactions in complex geometries, and the proposed work will enable large-scale three-dimensional simulations. <br/><br/>The key features of the proposed software are: (1) parallel fast multipole methods, (2) efficient geometric modeling techniques for complex geometries, (3) simple library interfaces that allow use of the proposed software by non-experts, and (4) scalability on heterogeneous architectures.<br/><br/>Along with our research activities, an educational and dissemination program will be designed to communicate the results of this work to students and researchers. Several postdoctoral, graduate, and undergraduate students will be involved with the project. Additional educational activities will include research experiences for undergraduates, leveraging ongoing programs such as NSF REUs. We will encourage participation by women, minorities, and underrepresented groups.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047980","fuchsia_atom","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures"
"1339798","SI2-SSE: Collaborative Research: Software Elements for Transfer and Analysis of Large-Scale Scientific Data","ACI","Software Institutes","9/1/2013","8/29/2013","Rajkumar Kettimuthu","IL","University of Chicago","Standard Grant","Patricia Knezek","8/31/2017","$99,995.00 ","","kettimut@mcs.anl.gov","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004","7433|8005","$0.00 ","As science has become increasingly data-driven, and as data volumes and velocity are increasing, scientific advance in many areas will only be feasible if critical `big-data' problems are addressed - and even more importantly, software tools embedding these solutions are readily available to the scientists. Particularly, the major challenge being faced by current data-intensive scientific research efforts is that while the dataset sizes continue to grow rapidly, neither among network bandwidths, memory capacity of parallel machines, memory access speeds, and disk bandwidths are increasing at the same rate.<br/>Building on top of recent research at Ohio State University, which includes work on automatic data virtualization, indexing methods for scientific data, and a novel bit-vectors based sampling method, the goal of this project is to fully develop, disseminate, deploy, and support robust software elements addressing challenges in data transfers and analysis. The prototypes that have been already developed at Ohio State are being extended into two robust software elements: an extention of GridFTP (Grid Partial-File Transport Protocol)that allows users to specify a subset of the file to be transferred, avoiding unnecessary transfer of the entire file; and Parallel Readers for NetCDF and HDF5 for Paraview and VTK, data subsetting and sampling tools for NetCDF and HDF5 that perform data selection and sampling at the I/O level, and in parallel.<br/>This project impacts a number of scientific areas, i.e., any area that involves big (and growing) dataset sizes and need for data transfers and/or visualization. This project also contributes to computer science research in `big data', including scientific (array-based) databases, and visualization. Another contribution will be towards preparation of the broad science and engineering research community for big data handling and analytics.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339798","fuchsia_atom","SI2-SSE: Collaborative Research: Software Elements for Transfer and Analysis of Large-Scale Scientific Data"
"1339756","SI2-SSI: Collaborative: The XScala Project: A Community Repository for Model-Driven Design and Tuning of Data-Intensive Applications for Extreme-Scale Accelerator-Based Systems","ACI","CYBERINFRASTRUCTURE","10/1/2013","9/16/2013","Viktor Prasanna","CA","University of Southern California","Standard Grant","Rajiv Ramnath","9/30/2016","$748,914.00 ","","prasanna@usc.edu","University Park","Los Angeles","CA","University Park, Los Angeles, CA","900890001","2137407762","CSE","7231","7433|8009|9145","$0.00 ","The increasing gap between processor and memory performance -- referred to as the memory wall -- has led high-performance computing vendors to design and incorporate new accelerators into their next-generation systems. Representative accelerators include recon&#64257;gurable hardware such as FPGAs, heterogeneous processors such as CPU+GPU processors, highly multicore and multithreaded processors, and manycore co-processors and general-purpose graphics processing units, among others. These accelerators contain myriad innovative architectural features, including explicit control of data motion, large-scale SIMD/vector processing, and multithreaded stream processing. Such features provide abundant opportunities for developers to achieve high-performance for applications that were previously deemed hard to optimize. This project aims to develop tools that will assist developers in using hardware accelerators (co-processors) productively and effectively.<br/><br/>This project's specific technical focus is on data-intensive kernels including large dictionary string matching, dynamic programming, graph theory, and sparse matrix computations that arise in the domains of biology, network security, and the social sciences. The project is developing XScala, a software framework for designing efficient accelerator kernels. The framework contains a variety of design time and run-time performance optimization tools. The project concentrates on data-intensive kernels, bound by data movement. It proposes optimization techniques including (a) enhancing and exploiting maximal concurrency to hide data movement; (b) algorithmic reorganization to improve spatial and/or temporal locality; (c) data structure transformations to improve locality or reduce the size of the data (compressed structures); and (d) prefetching, among others. The project is also developing a public software repository and forum, called the XBazaar, for community-developed accelerator kernels. This project includes workshops, tutorials, and the PIs class and summer projects as various means by which to increase community involvement. The broader impacts include productive use of emerging classes of accelerator-augmented computer systems; creation of an open and accessible community repository, the XBazaar, for distributing accelerator-tuned computational kernels, software, and models; training of graduate and undergraduate students; and dissemination through publications, presentations at scientific meetings, lectures, workshops, and tutorials. The framework itself will be released as open-source code and as precompiled binaries for several common platforms, through the XBazaar, as an initial step toward building a community around accelerator kernels.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339756","fuchsia_atom","SI2-SSI: Collaborative: The XScala Project: A Community Repository for Model-Driven Design and Tuning of Data-Intensive Applications for Extreme-Scale Accelerator-Based Systems"
"1339774","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)","ACI","CROSS-EF ACTIVITIES|Software Institutes","10/1/2013","8/29/2013","Marlon Pierce","IN","Indiana University","Standard Grant","Rajiv Ramnath","9/30/2018","$2,500,000.00 ","Suresh Marru","marpierc@indiana.edu","509 E 3RD ST","Bloomington","IN","509 E 3RD ST, Bloomington, IN","474013654","8128550516","CSE","7275|8004","7433|8009","$0.00 ","Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources' complexities. Given Science Gateways' demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.<br/><br/>SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP's adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339774","fuchsia_atom","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)"
"1450310","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","ACI","OFFICE OF MULTIDISCIPLINARY AC|COMPUTATIONAL PHYSICS|Software Institutes","5/1/2015","5/6/2016","Kyle Cranmer","NY","New York University","Continuing grant","Rajiv Ramnath","4/30/2019","$809,189.00 ","","kyle.cranmer@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","70 WASHINGTON SQUARE S, NEW YORK, NY","100121019","2129982121","CSE","1253|7244|8004","7433|8009|8084","$0.00 ","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces. However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community. Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450310","fuchsia_tree","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)"
"1440396","SI2-SSE: Dynamic Adaptive Runtime Systems for Advanced Multipole Method-based Science Achievement","ACI","SPECIAL PROJECTS - CCF|Software Institutes","10/1/2014","6/16/2016","Matthew Anderson","IN","Indiana University","Standard Grant","Almadena Y. Chtchelkanova","9/30/2017","$522,676.00 ","Bo Zhang, Jackson DeBuhr, Thomas Sterling","andersmw@indiana.edu","509 E 3RD ST","Bloomington","IN","509 E 3RD ST, Bloomington, IN","474013654","8128550516","CSE","2878|8004","7433|8004|8005|9251","$0.00 ","Multipole methods, including the fast multipole method and the Barnes-Hut algorithm, contribute to a broad range of end-user science applications extending from molecular dynamics to galaxy formation. Multipole methods are widely applied to N-body like problems where the individual interactions of a large number of distant objects can be treated as a single interaction under the appropriate conditions. This simplification eliminates the need for computing individual pairwise interactions and results in a drastic speed-up of computation. However, conventional parallel multipole methods are facing serious challenges to remain competitive as computational resources approach Exascale. Many applications employing multipole methods describe very dynamic physical processes, both in their time dependence and in their range of relevant spatial scales, while conventional implementations of multipole methods are essentially static in nature leading to computational inefficiencies. <br/>This project provides a fine-grained data-driven approach for multipole methods in order to address the limitations of conventional practices and improve scalability and efficiency. The software library employs dynamic adaptive execution methods with multipole-specific strategies for fault tolerance and exception handling while simplifying the implementation of the fast multipole method and the Barnes-Hut algorithm for end-users. The project software library immediately impacts science applications based on multipole methods by improving application scalability and efficiency and providing fault tolerance, a global address space, and an Exascale-ready execution model which integrates the entire system stack. The software library provides a portable and easy-to-use interface that allows scientists to work more efficiently and take advantage of high performance computing resources more effectively. The software library also serves to inform the evolution and development of other languages and programming models aiming to improve performance by shifting to message-driven techniques.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440396","fuchsia_atom","SI2-SSE: Dynamic Adaptive Runtime Systems for Advanced Multipole Method-based Science Achievement"
"1450262","Collaborative Research: SI2-SSI:Task-based Environment for Scientific Simulation at Extreme Scale (TESSE)","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|Software Institutes","5/15/2015","5/20/2015","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Standard Grant","Rajiv Ramnath","4/30/2018","$600,000.00 ","","evaleev@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","Sponsored Programs 0170, BLACKSBURG, VA","240610001","5402315281","CSE","1253|1712|8004","7433|8009|8084|9216","$0.00 ","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US. TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects. TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450262","fuchsia_atom","Collaborative Research: SI2-SSI:Task-based Environment for Scientific Simulation at Extreme Scale (TESSE)"
"1450377","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","ACI","OFFICE OF MULTIDISCIPLINARY AC|COMPUTATIONAL PHYSICS|Software Institutes","5/1/2015","7/12/2016","G.J. Peter Elmer","NJ","Princeton University","Continuing grant","Rajiv Ramnath","4/30/2019","$1,015,564.00 ","","gelmer@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","Off. of Research & Proj. Admin., Princeton, NJ","85442020","6092583090","CSE","1253|7244|8004","7433|8009|8084|8005","$0.00 ","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces. However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community. Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450377","fuchsia_tree","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)"
"1450177","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","PHYSICAL & DYNAMIC METEOROLOGY|Software Institutes|EarthCube","8/1/2015","8/4/2015","Brian Ancell","TX","Texas Tech University","Standard Grant","Rajiv Ramnath","7/31/2018","$166,428.00 ","","brian.ancell@ttu.edu","349 Administration Bldg","Lubbock","TX","349 Administration Bldg, Lubbock, TX","794091035","8067423884","CSE","1525|8004|8074","4444|7433|8009","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450177","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1450344","Collaborative Research: SI2-SSI: Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)","ACI","DMR SHORT TERM SUPPORT|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|EDUCATION AND WORKFORCE","5/15/2015","7/15/2016","Robert Harrison","NY","SUNY at Stony Brook","Standard Grant","Rajiv Ramnath","4/30/2018","$614,808.00 ","","robert.harrison@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","WEST 5510 FRK MEL LIB, Stony Brook, NY","117940001","6316329949","CSE","1712|1253|8004|7361","7433|8009|9216|8084|7556","$0.00 ","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US. TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects. TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450344","fuchsia_atom","Collaborative Research: SI2-SSI: Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)"
"1450327","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS|CDS&E","8/1/2015","7/5/2016","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Standard Grant","Rajiv Ramnath","7/31/2020","$932,376.00 ","Robert O'Bara","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","104 AIRPORT DR STE 2200, CHAPEL HILL, NC","275991350","9199663411","CSE","1253|8004|8069|8084","7433|8004|8009|8084|9251","$0.00 ","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450327","fuchsia_atom","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction"
"1147606","EAGER: : Best Practices and Models for Sustainability for Robust Cyberinfrastructure Software","ACI","Software Institutes","9/1/2011","8/25/2011","Craig Stewart","IN","Indiana University","Standard Grant","Daniel Katz","8/31/2013","$296,637.00 ","Von Welch, Eric Wernert, Richard Knepper","stewart@iu.edu","509 E 3RD ST","Bloomington","IN","509 E 3RD ST, Bloomington, IN","474013654","8128550516","CSE","8004","7916","$0.00 ","The NSF Software Infrastructure for Sustained Innovation (SI2) program solicitation states that software is ""central to NSF's vision of a Cyberinfrastructure Framework for 21st Century Science and Engineering (CIF21),"" and goes on to emphasize that in general software is essential to computational and data-enabled science. Indeed, the SI2 program is one vehicle by which the NSF hopes to enable sustained and well supported software providing services and functionality needed by the US science and engineering community. This yearlong study of cyberinfrastructure projects will identify best practices in the development, deployment, and support of robust cyberinfrastructure software. <br/><br/>Through a combination of detailed case studies and surveys of software producers and users, the investigators will identify best practices for the process of moving software from a ""discovery"" process to well maintained and sustainable infrastructure for 21st century science and engineering, focusing in particular on the following: Given a piece of software that provides interesting capabilities and a community that wants to use (and possibly contribute to the further development of) that software, what steps are necessary to transform that software from ""interesting tool"" to ""robust and widely used element of national infrastructure, contributing to the NSF vision for CIF21"" - ands then support and maintain that tool sustainably? This research will lead to greater availability of widely usable software tools and curriculum materials, increasing the quality of education in computer science, computational science, and STEM disciplines.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147606","fuchsia_atom","EAGER: : Best Practices and Models for Sustainability for Robust Cyberinfrastructure Software"
"1147247","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics","ACI","OFFICE OF MULTIDISCIPLINARY AC|OPERATIONS RESEARCH|DYNAMICAL SYSTEMS|Software Institutes|CDS&E-MSS","6/1/2012","6/19/2012","Anne Schilling","CA","University of California-Davis","Standard Grant","Rajiv Ramnath","5/31/2016","$216,626.00 ","","anne@math.ucdavis.edu","OR/Sponsored Programs","Davis","CA","OR/Sponsored Programs, Davis, CA","956186134","5307547700","CSE","1253|5514|7478|8004|8069","7433|7683|8005|1253|5514|7478|8004","$0.00 ","Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is ""to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area"". There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s. The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thiery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.<br/><br/>The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147247","fuchsia_atom","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics"
"1148090","Collaborative Research: SI2-SSI: An Interactive Software Infrastructure for Sustaining Collaborative Community Innovation in the Hydrologic Sciences","ACI","ADVANCES IN BIO INFORMATICS|Software Institutes","7/1/2012","7/16/2012","Ray Idaszak","NC","University of North Carolina at Chapel Hill","Standard Grant","Rajiv Ramnath","6/30/2017","$2,128,078.00 ","Lawrence Band, Xiaohui Carol Song, Venkatesh Merwade, David Valentine","rayi@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","104 AIRPORT DR STE 2200, CHAPEL HILL, NC","275991350","9199663411","CSE","1165|8004","7433|8009|1165|8004","$0.00 ","Water, its quality, quantity, accessibility, and management, is crucial to society. However, our ability to model and quantitatively understand the complex interwoven environmental processes that control water and its availability is severely hampered by inadequate tools related to hydrologic data discovery, systems integration, modeling/ simulation, and education. This project develops sustainable cyberinfrastructure for better access to water-related data and models in the hydrologic sciences, enabling hydrologists and other associated communities to collaborate and combine data and models from multiple sources. It will provide new ways in which hydrologic knowledge is created and applied to better understand water availability, quality, and dynamics. It will also help to provide a more comprehensive understanding of the interactions between natural and engineered aspects of the water cycle. These goals will be achieved through the development of interoperable cyberinfrastructure tools and the creation of an online collaborative environment, called HydroShare, which enables scientists to easily discover and access hydrologic and related data and models, retrieve them to their desktop, and perform analyses in a high performance computing environment. The software to be developed will take advantage of existing NSF cyberinfrastructure (iRODS, HUBzero, CSDMS, CUAHSI HIS) and be created as open source code. Its development will be end user-driven. In terms of broader impacts, the project builds essential infrastructure for science by developing software tools and computing environments to allow better understanding of the impacts of climate change (i.e., floods, droughts, biofuels, etc.) and to allow improved water resource development and the management of freshwater resources both above and below ground. Resulting software will be made publicly available and provides a strong student and workforce training/education component. In addition, the project supports an institution in an EPSCoR state and engages, as a PI, a person who is from a group under-represented in the sciences and engineering.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148090","fuchsia_atom","Collaborative Research: SI2-SSI: An Interactive Software Infrastructure for Sustaining Collaborative Community Innovation in the Hydrologic Sciences"
"1550461","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","ACI","Software Institutes","7/1/2016","7/11/2016","Pablo Laguna","GA","Georgia Tech Research Corporation","Continuing grant","Rajiv Ramnath","6/30/2020","$231,504.00 ","David Bader","pablo.laguna@physics.gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","8004","7433|7569|8009|8084","$0.00 ","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies. A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building a simulation data repository. The repository will allows user to compare results, contribute data, test innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550461","fuchsia_tree","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration"
"1550436","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","ACI","Software Institutes","7/1/2016","7/11/2016","Manuela Campanelli","NY","Rochester Institute of Tech","Continuing grant","Rajiv Ramnath","6/30/2020","$218,761.00 ","Joshua Faber, Yosef Zlochower","manuela@astro.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","1 LOMB MEMORIAL DR, ROCHESTER, NY","146235603","5854757987","CSE","8004","7433|7569|8009|8084","$0.00 ","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies. A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building a simulation data repository. The repository will allows user to compare results, contribute data, test innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550436","fuchsia_tree","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration"
"1550300","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","ACI","Software Institutes","7/1/2016","7/6/2016","Abhijit Majumder","MI","Wayne State University","Continuing grant","Bogdan Mihaila","6/30/2020","$968,467.00 ","Ron Soltz, Joern Putschke","abhijit.majumder@wayne.edu","5057 Woodward","Detroit","MI","5057 Woodward, Detroit, MI","482023622","3135772424","CSE","8004","026Z|7433|7569|8009|8084","$0.00 ","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550300","fuchsia_tree","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)"
"1060067","Collaborative Research: SI2-SSI: Scalable Hierarchical Algorithms for Extreme Computing (SHARE)","ACI","OFFICE OF MULTIDISCIPLINARY AC|COMPUTATIONAL PHYSICS","9/15/2010","8/4/2011","Lincoln Greenhill","MA","Harvard University","Continuing grant","Bradley D. Keister","8/31/2015","$230,755.00 ","","greenhill@cfa.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","1033 MASSACHUSETTS AVE, Cambridge, MA","21385366","6174955501","CSE","1253|7244","1253|7244","$0.00 ","This award supports the development of software tools for advanced algorithms on a cluster of high performance graphics processing units(GPUs). The initial goal of this library will focus on a single driver application, the fundamental numerical study of Nuclear Forces (Lattice Quantum Chromodynamics: LQCD), and a second target application, the numerical simulation of the exciting nano-technology of graphene. These are both multi-fermion problems well suited to solution via multi-scale algorithms on many-core architectures. This pilot project draws on experience gained by the small team at Boston University and Harvard in developing Dirac solvers for lattice field theory. Two building blocks from prior research are (1) the construction of an adaptive multigrid (MG) solver for the Wilson Dirac operator of LQCD, which demonstrates a 20x speedup compared to the best Krylov solvers in production code and (2) a highly optimized Krylov solver for the same operator on GPUs (but without multigrid), realizing a 10x improvement in price/performance over traditional clusters. The library will unite these feature and generalize their domain of applicability. <br/><br/>As an example of the broader impact, it is estimated that combining these two technologies (MG algorithms and GPU architectures) will yield a 100-fold improvement in price/performance for the most compute-intensive component of LQCD simulations. Such an advance would be truly transformative, making an immediate impact in nuclear and particle physics. At the same time, it will serve as a prototype of the more generic problem of mapping hierarchical algorithms onto heterogeneous architectures, a challenge of paramount importance on the path to the exascale. The software library will be designed to bring similar benefits to graphene technology and to evolve to accommodate additional target application and additional domain decomposition algorithm to mitigate the communication bottleneck of Exascale designs. The award will provide partial support for two postdoctoral scholars who play essential roles in this project.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1060067","fuchsia_tree","Collaborative Research: SI2-SSI: Scalable Hierarchical Algorithms for Extreme Computing (SHARE)"
"1060012","Collaborative Research: SI2-SSI: Scalable Hierarchical Algorithms for Extreme Computing (SHARE)","ACI","OFFICE OF MULTIDISCIPLINARY AC|COMPUTATIONAL PHYSICS|Software Institutes","9/15/2010","6/15/2012","Richard Brower","MA","Trustees of Boston University","Continuing grant","Bogdan Mihaila","2/28/2015","$374,088.00 ","Lorena Barba, Claudio Rebbi","brower@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","881 COMMONWEALTH AVE, BOSTON, MA","22151300","6173534365","CSE","1253|7244|8004","1253|7244","$0.00 ","This award supports the development of software tools for advanced algorithms on a cluster of high performance graphics processing units(GPUs). The initial goal of this library will focus on a single driver application, the fundamental numerical study of Nuclear Forces (Lattice Quantum Chromodynamics: LQCD), and a second target application, the numerical simulation of the exciting nano-technology of graphene. These are both multi-fermion problems well suited to solution via multi-scale algorithms on many-core architectures. This pilot project draws on experience gained by the small team at Boston University and Harvard in developing Dirac solvers for lattice field theory. Two building blocks from prior research are (1) the construction of an adaptive multigrid (MG) solver for the Wilson Dirac operator of LQCD, which demonstrates a 20x speedup compared to the best Krylov solvers in production code and (2) a highly optimized Krylov solver for the same operator on GPUs (but without multigrid), realizing a 10x improvement in price/performance over traditional clusters. The library will unite these feature and generalize their domain of applicability. <br/><br/>As an example of the broader impact, it is estimated that combining these two technologies (MG algorithms and GPU architectures) will yield a 100-fold improvement in price/performance for the most compute-intensive component of LQCD simulations. Such an advance would be truly transformative, making an immediate impact in nuclear and particle physics. At the same time, it will serve as a prototype of the more generic problem of mapping hierarchical algorithms onto heterogeneous architectures, a challenge of paramount importance on the path to the exascale. The software library will be designed to bring similar benefits to graphene technology and to evolve to accommodate additional target application and additional domain decomposition algorithm to mitigate the communication bottleneck of Exascale designs. The award will provide partial support for two postdoctoral scholars who play essential roles in this project.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1060012","fuchsia_tree","Collaborative Research: SI2-SSI: Scalable Hierarchical Algorithms for Extreme Computing (SHARE)"
"1047896","SI2-SSE: Reducing the Complexity of Comparative Genomics with Online Analytical Processing","ACI","ADVANCES IN BIO INFORMATICS|Software Institutes","9/15/2010","10/23/2012","Robert Kosara","NC","University of North Carolina at Charlotte","Standard Grant","Anne Maglia","8/31/2015","$448,253.00 ","Cynthia Gibas","rkosara@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","9201 University City Boulevard, CHARLOTTE, NC","282230001","7046871888","CSE","1165|8004","1165","$0.00 ","Genome comparison is a common bioinformatics analysis task, but a survey of the literature suggests that comparative genomic studies are done in an ad hoc, investigator-dependent, and non-reproducible fashion. Comparative genomics analysis questions can generally be formulated as set queries: what differentiates genome A from genome B, or from a broader group of its taxonomic neighbors? This project will develop a data warehouse-type database system optimized for comparative genomics that is particularly suited to answer these kinds of questions. It will store sequence-linked biological data in a way that supports OLAP (On-Line Analytical Processing) and complex set-based queries. A workflow tool will be developed for guiding the user through core comparative genomic operations, and will serve as an interface for populating the data warehouse. An interactive query tool will allow the user to easily construct complex questions about the data. Set-based as well as individual record results will be presented to the user in a way that can be easily browsed, compared, and exported.<br/><br/>The system will enable the user to generate and compare genomic feature sets following a guided workflow defined by and incorporating common elements of analysis used in current microbial genome studies. Parameters and results from each step in the process will be tracked for later reporting. The system will enable both biology-driven comparison of genomic feature sets, and perhaps more importantly systematic inquiry into and comparison of bioinformatics analysis results obtained at each workflow stage. All software and database structures developed in this project will be made available under an open-source license and as runnable virtual machine images. The latter will make it possible for scientists to get started without complicated installation and configuration procedures, and instead focus on their research questions. All user-facing parts of the software will be accessible through a web browser, making use of the latest developments in browser-based interaction and HTML5. A diverse group of graduate students will receive inter-disciplinary training in this project, and K-12 and underrepresented groups will be included in through ongoing partnerships with the STARS Alliance program.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047896","fuchsia_atom","SI2-SSE: Reducing the Complexity of Comparative Genomics with Online Analytical Processing"
"1047857","Collaborative Research: SI2-SSI: Development of an Integrated Molecular Design Environment for Lubrication Systems (iMoDELS)","ACI","NANOSCALE: INTRDISCPL RESRCH T|Software Institutes","10/1/2011","7/16/2014","Adri van Duin","PA","Pennsylvania State Univ University Park","Continuing grant","Daniel Katz","9/30/2015","$457,319.00 ","","acv13@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","110 Technology Center Building, UNIVERSITY PARK, PA","168027000","8148651372","CSE","1674|8004","1674|8004","$0.00 ","This project is focused on developing, deploying and distributing the Integrated Molecular Design Environment for Lubrication Systems (iMoDELS), an open-source simulation and design environment (SDE) that encapsulates the expertise of specialists in first principles, forcefields and molecular simulation related to nanoscale lubrication in a simple web-based interface. The iMoDELS SDE is being developed using model-integrated computing (MIC), a state-of-the-art powerful, well-established, extensible, community-supported, and application-hardened software engineering framework that supports scientific and engineering workflows. Making iMoDELS broadly accessible is motivated by the high cost (over $800B/yr in the US) of friction and wear, which, along with the methodology to overcome them, lubrication, are collectively known as tribology. Tribology involves molecular mechanisms occurring on a nanometer scale, and hence understanding tribological behavior on this scale is critical to developing new technologies for reducing wear due to friction. Deployment of iMoDELS will enable non-computational specialists to be able to evaluate, design and optimize nanoscale lubrication systems, such as hard disk drives, NEMS (nanoelectromechanical systems) and MEMS (microelectromechanical systems), and experiments involving rheological measurements via atomic force microscopes (AFMs) and surface force apparatuses (SFAs).<br/><br/>The iMoDELS SDE brings together a unique combination of materials and computer scientists who will combine their skills to abstract the deep human expertise currently required for the development of simulation-based experiments, thus making broadly available easy-to-use tools to an empirically driven area of science and engineering (nanotribology) of rapidly growing technological importance. iMoDELS includes the creation and open dissemination of forcefield and simulation results databases that will benefit the simulation community worldwide and catalyze broad-based activity in this area. The proposed research also includes the interdisciplinary training of undergraduate and graduate students, as well as postdoctoral researchers at the interface of tribology, computational materials sciences, and computer science. The PIs will use the iMoDELS SDE, and results from it, in presentations used in outreach to local area high school and in classes given to undergraduate students. The iMoDELS SDE will be vigorously promoted through workshops and presentations at national conferences, and via the dedicated website for development and dissemination.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047857","fuchsia_tree","Collaborative Research: SI2-SSI: Development of an Integrated Molecular Design Environment for Lubrication Systems (iMoDELS)"
"1048052","SI2-SSE: Cloud-Computing-Clusters for Scientific Research","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|Software Institutes","9/15/2010","9/7/2010","John Rehr","WA","University of Washington","Standard Grant","Daryl W. Hess","8/31/2013","$489,188.00 ","","jjr@phys.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","4333 Brooklyn Ave NE, Seattle, WA","981950001","2065434043","CSE","1253|1712|8004","1765|7237|7569|1253|1712","$0.00 ","This award is made on a proposal to the Software Infrastructure for Sustained Innovation. The Office of Cyberinfrastructure and the Division of Materials Research contribute funds to this award. <br/><br/>This award supports developing an internet-based scientific computing environment, particularly for computational materials research. The PI will advance into computational materials research a recent innovation known as cloud computing ? the ability to utilize computer resources connected via the internet, which may be geographically separated by vast distances, on demand to do computing. The PI will develop a computing environment that will be robust, flexible, and relatively easy for the materials research community to use. The PI will focus on specific codes that calculate the states of electrons in materials starting from the identities of the atoms in the material and their positions. Forces between atoms may be calculated from the electronic states. These codes will be used in the short term to calculate complicated motions of atoms and molecules, to understand data from experiments that involve scattering light from materials, to study how specific materials absorb and interact with light to advance solar cell technology.<br/><br/>This award also supports education and outreach activities to enable the research community to effectively use the new computational tools developed under this award, and expose high school students to cutting edge advances in computation.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1048052","fuchsia_atom","SI2-SSE: Cloud-Computing-Clusters for Scientific Research"
"1339782","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","ACI","PHYSICAL OCEANOGRAPHY|SPECIAL PROJECTS - CISE|SPECIAL PROJECTS - CCF|Software Institutes|EarthCube","10/1/2014","8/26/2014","Hartmut Kaiser","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Rajiv Ramnath","9/30/2018","$970,835.00 ","Robert Twilley","hkaiser@cct.lsu.edu","202 Himes Hall","Baton Rouge","LA","202 Himes Hall, Baton Rouge, LA","708032701","2255782760","CSE","1610|1714|2878|8004|8074","7433|8009|9150","$0.00 ","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339782","fuchsia_atom","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling"
"1358118","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis","ACI","OFFICE OF MULTIDISCIPLINARY AC|Mechanics of Materials and Str|DYNAMICAL SYSTEMS|COFFES|Software Institutes","9/1/2013","9/13/2013","Shawn Shadden","CA","University of California-Berkeley","Standard Grant","Daniel Katz","12/31/2014","$120,478.00 ","","shadden@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","CSE","1253|1630|7478|7552|8004","","$0.00 ","The Lagrangian Coherent Structures (LCS) software elements developed by this project will provide a valuable tool set for fluid mechanics research to extract new discoveries from the vast and growing body of computational and experimental fluid mechanics data. The computation of LCS enables a systematic approach to accurately characterize transport phenomena in complex systems that pose insurmountable challenges to traditional Eulerian approaches. Prior, ah hoc implementations of LCS have already helped in important, real-world challenges including, tracking pollutants in the ocean, developing novel diagnoses and therapies for cardiovascular disease, and helping airplanes to avoid turbulence. We will produce an open LCS software system to provide a modular, extensible and flexible infrastructure to broaden the community of scientists and engineers that benefit from LCS, in problems ranging from fluid dynamics to general dynamical systems. Prof. Shadden will lead the LCS algorithm design and numerical analysis, and Prof. Hart will oversee the package's architectural design and the efficient parallel implementation of its elements.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1358118","fuchsia_atom","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis"
"1339738","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","ACI","PHYSICAL OCEANOGRAPHY|SPECIAL PROJECTS - CISE|SPECIAL PROJECTS - CCF|Software Institutes|EarthCube","10/1/2014","10/20/2014","Joannes Westerink","IN","University of Notre Dame","Standard Grant","Rajiv Ramnath","9/30/2018","$730,000.00 ","Tim Stitt, Damrongsak Wirasaet","jjw@nd.edu","940 Grace Hall","NOTRE DAME","IN","940 Grace Hall, NOTRE DAME, IN","465565708","5746317432","CSE","1610|1714|2878|8004|8074","7433|8009","$0.00 ","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339738","fuchsia_atom","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling"
"1339801","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC|EarthCube|CDS&E-MSS|PHYSICAL OCEANOGRAPHY|SPECIAL PROJECTS - CISE|SPECIAL PROJECTS - CCF","10/1/2014","2/27/2015","Clinton Dawson","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","9/30/2018","$540,012.00 ","Craig Michoski","clint@ices.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","101 E. 27th Street, Suite 5.300, Austin, TX","787121532","5124716424","CSE","8004|1253|8074|8069|1610|1714|2878","7433|8009|8251","$0.00 ","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339801","fuchsia_atom","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling"
"1339763","SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI","ACI","Software Institutes","9/1/2013","8/29/2013","Edgar Gabriel","TX","University of Houston","Standard Grant","Rajiv Ramnath","8/31/2016","$147,236.00 ","","gabriel@cs.uh.edu","4800 Calhoun Boulevard","Houston","TX","4800 Calhoun Boulevard, Houston, TX","772042015","7137435773","CSE","8004","7433|8005","$0.00 ","High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today's architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.<br/>The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.<br/>MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339763","fuchsia_atom","SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI"
"1339856","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)","ACI","Software Institutes|CROSS-EF ACTIVITIES","10/1/2013","8/29/2013","Mark Miller","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","9/30/2018","$1,742,099.00 ","Amitava Majumdar","mmiller@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8004|7275","7433|8009","$0.00 ","Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources' complexities. Given Science Gateways' demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.<br/><br/>SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP's adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339856","fuchsia_atom","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)"
"1339773","Collaborative Research: SI2-SSE: A Petascale Numerical Library for Multiscale Phenomena Simulations","ACI","CDS&E|Software Institutes","10/1/2013","9/11/2013","Diego Donzis","TX","Texas A&M Engineering Experiment Station","Standard Grant","Rajiv Ramnath","9/30/2016","$139,879.00 ","","donzis@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","TEES State Headquarters Bldg., College Station, TX","778454645","9798477635","CSE","8084|8004","7433|8005|8084","$0.00 ","Multiscale phenomena are a grand challenge for theory, simulations and experiments. It is difficult to understand phenomena at both highest and lowest scales, as well as all those in between. This challenge shows up in diverse fields. One of the long-standing problems in cosmology is understanding cosmological structure formation. In computer simulations of this process the challenge is increasing grid resolution while retaining the essential physics. In all-atom molecular dynamics simulations of enzymes the challenge is simulating systems with a large number of atoms while resolving long-range interactions and having sufficiently high throughput. Such simulations are critical in understanding important biological processes and eventually designing new drugs. Another prime example of multiscale phenomena is turbulent flows, a rich and complex subject of great relevance to many of the main technological issues of the day, including climate, energy, and the management of oil and biohazards. Understanding turbulent flows is critical for design of new transportation vehicles, improving efficiency of combustion processes and managing their environment pollution. Here simulations have been historically limited, and remain so, due to extremely high computational cost, even using the high-end computational systems available to researchers today. Reducing this cost, and efficiently using the computational resources, often requires specialized expertise, as well as significant development time and cost many research groups cannot afford. This project will develop a powerful suite of critical software components to provide tools for performing simulations of multiscale phenomena. <br/><br/>The suite will implement state-of-the-art techniques for reducing communication cost, which has become the most important contributing factor to the total simulation cost, especially at larger scales. It will provide a flexible set of features that will make it usable in a great number of codes across the disciplines. In particular, the library will include user-friendly interfaces for Fourier transforms, spectral and compact differentiation in three dimensions, in addition to widely used communication routines (transposes, halo exchanges). This combination of emphasis on scalable performance and richness of features makes this suite unique among other libraries in existence today. Given the extraordinary challenge of simulations of multiscale phenomena, this library will provide a realistic path towards the Exascale. The suite will be available under an open source license. User outreach will be undertaken through the project website, mailing list, user surveys and presentations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339773","fuchsia_atom","Collaborative Research: SI2-SSE: A Petascale Numerical Library for Multiscale Phenomena Simulations"
"1339781","SI2-SSE: A Sustainable Wireless Sensor Software Development Framework for Science and Engineering Researchers","ACI","SPECIAL PROJECTS - CCF|Software Institutes","9/1/2013","4/10/2015","Raheem Beyah","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","8/31/2017","$506,000.00 ","A. Selcuk Uluagac","rbeyah@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","2878|8004","7433|8005|9251","$0.00 ","In the last decade, wireless sensor networking has been one of the most popular research areas for computer engineers and scientists. The use of wireless sensors has also started to gain popularity among researchers other than computer engineers and scientists. Today, sensors are used to enable civil engineers to monitor the structural health of deteriorating infrastructure such as highways and bridges, farmers to develop precision-agriculture techniques, ecologists to observe wildlife in their natural habitat, geophysicists to capture seismic activity of volcanoes, and in many other application areas. However, the task of developing software applications for wireless sensors is challenging for researchers because sensors have limited technical capabilities and software implementations for sensors require meticulous procedures to provide a desired level of services (e.g., reliability, security) for applications. This situation is even further exacerbated for researchers in other fields of engineering and science (e.g., civil engineers and geophysicists) as they may not have a rigorous programming background. Therefore, to facilitate the design, development and implementation of wireless sensor applications, this project provides a new framework called PROVIZ, which integrates visualization and programming functionalities into a common platform. PROVIZ is an open-source, platform independent, modular, and extensible framework for heterogeneous wireless sensor monitoring and application development. It consists of a set of easy-to-use simplified languages (one domain specific scripting language, one icon-based drag-and-drop style visual language) and a simple programming editor for developing wireless sensor applications and a mechanism for (re)programming wireless sensor nodes remotely over-the-air by distributing the generated application image. PROVIZ has the capability to visualize wireless sensor data captured from (1) a packet sniffer, (2) a binary packet trace file (e.g., PSD format), and (3) an external simulator running a wireless sensor application. PROVIZ also has the capability to process data from multiple sniffers simultaneously to visualize a large wireless sensor deployment.<br/><br/>PROVIZ will be instrumental to scientists and engineers working with wireless sensors in many disciplines (e.g., civil engineering, ecology, agriculture). PROVIZ will allow these researchers to easily program sensors and to focus more on the tasks in their domains by significantly reducing the overhead of learning how to program sensors. The PROVIZ project will be conducted as an open source project, enabling interested software developers to benefit from and add to it. Further, a variation of PROVIZ will be used to present sensor networking concepts to middle school underrepresented minority students in Georgia. Given the proliferation of wireless sensor utilization in various engineering and science fields, it is envisioned that the success of the PROVIZ project will help contribute to the growth of the future cyber workforce in the U.S.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339781","fuchsia_atom","SI2-SSE: A Sustainable Wireless Sensor Software Development Framework for Science and Engineering Researchers"
"1450429","SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)","ACI","Software Institutes|CDS&E","9/1/2015","8/27/2015","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Rajiv Ramnath","8/31/2019","$2,126,446.00 ","Anthony Danalis, Heike Jagode","dongarra@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","1 CIRCLE PARK, KNOXVILLE, TN","379960003","8659743466","CSE","8004|8084","7433|8009|9150|8084","$0.00 ","Modern High Performance Computing (HPC) systems continue to increase in size and complexity. Tools to measure application performance in these increasingly complex environments must also increase the richness of their measurements to provide insights into the increasingly intricate ways in which software and hardware interact. The PAPI performance-monitoring library has provided a clear, portable interface to the hardware performance counters available on all modern CPUs and some other components of interest (scattered across the chip and system). Widely deployed and widely used, PAPI has established itself as fundamental software infrastructure, not only for the scientific computing community, but for many industry users of HPC as well. But the radical changes in processor and system design that have occurred over the past several years pose new challenges to PAPI and the HPC software infrastructure as a whole. The PAPI-EX project integrates critical PAPI enhancements that flow from both governmental and industry research investments, focusing on processor and system design changes that are expected to be present in every extreme scale platform on the path to exascale computing.<br/><br/>The primary impact of PAPI-EX is a direct function of the importance of the PAPI library. PAPI has been in predominant use by tool developers, major national HPC centers, system vendors, and application developers for over 15 years. PAPI-EX builds on that foundation. As important research infrastructure, the PAPI-EX project allows PAPI to continue to play its essential role in the face of the revolutionary changes in the design and scale of new systems. In terms of enhancing discovery and education, the list of partners working with PAPI-EX includes NSF computing centers, major tool developers, major system vendors, and individual community leaders, and this diverse group will help facilitate training sessions, targeted workshops, and mini-symposia at national and international meetings. Finally, the active promotion of PAPI by many major system vendors means that PAPI, and therefore PAPI-EX, will continue to deliver major benefits for government and industry in many domains.<br/><br/>PAPI-EX addresses a hardware environment in which the cores of current and future multicore CPUs share various performance-critical resources (a.k.a., 'inter-core' resources), including power management, on-chip networks, the memory hierarchy, and memory controllers between cores. Failure to manage contention for these 'inter-core' resources has already become a major drag on overall application performance. Consequently, the lack of ability to reveal the actual behavior of these resources at a low level, has become very problematic for the users of the many performance tools (e.g., TAU, HPCToolkit, Open|SpeedShop, Vampir, Scalasca, CrayPat, Active Harmony, etc.). PAPI-EX enhances and extends PAPI to solve this critical problem and prepare it to play its well-established role in HPC performance optimization. Accordingly, PAPI-EX targets the following objectives: (1) Develop shared hardware counter support that includes system-wide and inter-core measurements; (2) Provide support for data-flow based runtime systems; (3) Create a sampling interface to record streams of performance data with relevant context; (4) Combine an easy-to-use tool for text-based application performance analysis with updates to PAPI?s high-level API to create a basic, ?out of the box? instrumentation API.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450429","fuchsia_atom","SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)"
"1407834","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis","ACI","CDS&E|Software Institutes","9/20/2013","11/12/2013","Shawn Shadden","CA","University of California-Berkeley","Standard Grant","Rajiv Ramnath","9/30/2017","$306,276.00 ","","shadden@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","CSE","8084|8004","7433|8009|8084","$0.00 ","The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. <br/><br/>This project is addressing the above roadblocks through the following aims: 1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community. Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1407834","fuchsia_atom","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis"
"1450405","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","PHYSICAL & DYNAMIC METEOROLOGY|EarthCube|Software Institutes","8/1/2015","8/4/2015","Steven Greybush","PA","Pennsylvania State Univ University Park","Standard Grant","Rajiv Ramnath","7/31/2018","$99,953.00 ","","sjg213@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","110 Technology Center Building, UNIVERSITY PARK, PA","168027000","8148651372","CSE","1525|8074|8004","7433|8009|4444","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450405","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1450179","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC","8/1/2015","6/24/2015","Todd Martinez","CA","Stanford University","Standard Grant","Rajiv Ramnath","7/31/2019","$600,000.00 ","","Todd.Martinez@stanford.edu","3160 Porter Drive","Palo Alto","CA","3160 Porter Drive, Palo Alto, CA","943041212","6507232300","CSE","8004|1253","7433|8009","$0.00 ","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450179","fuchsia_atom","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science"
"1450338","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","ACI","Software Institutes|EarthCube|GEOMORPHOLOGY & LAND USE DYNAM","8/1/2015","7/14/2015","Nicole Gasparini","LA","Tulane University","Standard Grant","Rajiv Ramnath","7/31/2020","$532,320.00 ","","ngaspari@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","6823 ST CHARLES AVENUE, NEW ORLEANS, LA","701185698","5048654000","CSE","8004|8074|7458","7433|8009|9150","$0.00 ","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450338","fuchsia_atom","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics"
"1440581","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions","ACI","CDS&E|Software Institutes|SPECIAL PROJECTS - CCF","2/1/2015","8/8/2014","Amarda Shehu","VA","George Mason University","Standard Grant","Rajiv Ramnath","1/31/2018","$217,288.00 ","","ashehu@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","4400 UNIVERSITY DR, FAIRFAX, VA","220304422","7039932295","CSE","8084|8004|2878","7433|8005|8004|9216|8084|2878","$0.00 ","This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.<br/><br/>The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440581","fuchsia_atom","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions"
"1607042","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS|CDS&E","8/1/2015","1/28/2016","Matthew Knepley","TX","William Marsh Rice University","Standard Grant","Rajiv Ramnath","7/31/2020","$262,655.00 ","","knepley@rice.edu","6100 MAIN ST","Houston","TX","6100 MAIN ST, Houston, TX","770051827","7133484820","CSE","1253|8004|8069|8084","7433|8004|8009|8084","$0.00 ","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1607042","fuchsia_atom","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction"
"1613155","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud","ACI","DMR SHORT TERM SUPPORT|CDS&E|Software Institutes","10/22/2015","11/10/2015","Coray Colina","FL","University of Florida","Standard Grant","Rajiv Ramnath","9/30/2017","$149,283.00 ","","colina@chem.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","1 UNIVERSITY OF FLORIDA, GAINESVILLE, FL","326112002","3523923516","CSE","1712|8084|8004","7433|8005|8400|7237|9216|024E|085E","$0.00 ","The use of polymers, their composites, and nanostructures is growing at a fast pace, both by displacing traditional materials and by enabling emerging technologies. Examples range from the all-composite airframe of the Boeing 787 and the new Airbus 350 to wearable electronics. This project aims to develop a software infrastructure to simulate these materials with atomic resolution and make these tools universally accessible and useful via on-line computing. These simulations have the potential to accelerate the development of optimized material formulations that can benefit society and reduce the associated costs by combining physical with computational experiments. Making these advanced tools available for free online simulations and complementing them with tutorials and educational material will encourage their use in the classroom and will impact the education of new generations of engineers and scientists familiar with these powerful tools that will be required to address tomorrow's challenges.<br/><br/>The objective of this effort is to enable pervasive, reproducible molecular simulations of polymeric materials using state-of-the-art tools and with quantified uncertainties building on recent breakthroughs in molecular simulations, cyber-infrastructure and uncertainty quantification. The framework will consist of three main components: i) powerful simulation tools for polymer nano structures including: state-of-the-art molecular builders, a parallel MD engine with stencils that enable efficient structure relaxation and property characterization and post-processing codes; ii) a UQ framework to orchestrate the molecular simulations and propagate uncertainties in input parameters to predictions and compare the predictions to experimental values; iii) databases of force fields and molecular structures as well as predicted and experimental properties. The simulation framework will be deployed via NSF's nanoHUB where users will be able to run online simulations without downloading or installing any software while expert users will have the option to download, modify and contribute to the infrastructure. Usage of and contributions to the software framework will be facilitated and encouraged via online and in-person user guides, learning modules and research tutorials.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1613155","fuchsia_atom","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud"
"1147337","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Linear Equations","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|Software Institutes|CDS&E-MSS","6/1/2012","6/20/2012","Dan Negrut","WI","University of Wisconsin-Madison","Standard Grant","Rajiv Ramnath","5/31/2015","$251,119.00 ","","negrut@wisc.edu","21 North Park Street","MADISON","WI","21 North Park Street, MADISON, WI","537151218","6082623822","CSE","1253|7478|8004|8069","7433|8005","$0.00 ","Drs. Negrut, Sameh, and Knepley will investigate, produce, and maintain a methodology and its software implementation that leverage emerging heterogeneous hardware architectures to solve billion-unknowns linear systems in a robust, scalable, and efficient fashion. The two classes of problems targeted under this project are banded dense and sparse general linear systems.<br/><br/>This project is motivated by the observation that the task of solving a linear system is one of the most ubiquitous ingredients in the numerical solution of Applied Mathematics problems. It is relied upon for the implicit integration of Ordinary Differential Equation (ODE) and Differential Algebraic Equation (DAE) problems, in the numerical solution of Partial Differential Equation (PDE) problems, in interior point optimization methods, in least squares approximations, in solving eigenvalue problems, and in data analysis. In fact, the vast majority of nonlinear problems in Scientific Computing are solved iteratively by drawing on local linearizations of nonlinear operators and the solution of linear systems. Recent advances in (a) hardware architecture; i.e., the emergence of General Purpose Graphics Processing Unit (GP-GPU) cards, and (b) scalable solution algorithms, provide an opportunity to develop a new class of parallel algorithms, called SPIKE, which can robustly and efficiently solve very large linear systems of equations.<br/><br/>Drawing on its divide-and-conquer paradigm, SPIKE builds on several algorithmic primitives: matrix reordering strategies, dense linear algebra operations, sparse direct solvers, and Krylov subspace methods. It provides a scalable solution that can be deployed in a heterogeneous hardware ecosystem and has the potential to solve billion-unknown linear systems in the cloud or on tomorrow?s exascale supercomputers. Its high degree of scalability and improved efficiency stem from (i) optimized memory access pattern owing to an aggressive pre-processing stage that reduces a generic sparse matrix to a banded one through a novel reordering strategy; (ii) good exposure of coarse and fine grain parallelism owing to a recursive, divide-and-conquer solution strategy; (iii) efficient vectorization in evaluating the coupling terms in the divide-and-conquer stage owing to a CPU+GPU heterogeneous computing approach; and (iv) algorithmic polymorphism, given that SPIKE can serve both as a direct solver or an effective preconditioner in an iterative Krylov-type method.<br/><br/>In Engineering, SPIKE will provide the Computer Aided Engineering (CAE) community with a key component; i.e., fast solution of linear systems, required by the analysis of complex problems through computer simulation. Examples of applications that would benefit from this technology are Structural Mechanics problems (Finite Element Analysis in car crash simulation), Computational Fluid Dynamics problems (solving Navier-Stokes equations in the simulation of turbulent flow around a wing profile), and Computational Multibody Dynamics problems (solving Newton-Euler equations in large granular dynamics problems).<br/><br/>SPIKE will also be interfaced to the Portable, Extensible Toolkit for Scientific Computation (PETSc), a two decades old flexible and scalable framework for solving Science and Engineering problems on supercomputers. Through PETSc, SPIKE will be made available to a High Performance Computing user community with more than 20,000 members worldwide. PETSc users will be able to run SPIKE without any modifications on vastly different supercomputer architectures such as the IBM BlueGene/P and BlueGene/Q, or the Cray XT5. SPIKE will thus run scalably on the largest machines in the world and will be tuned for very different network and hardware topologies while maintaining a simple code base.<br/><br/>The experience collected and lessons learned in this project will augment a graduate level class, ?High Performance Computing for Engineering Applications? taught at the University of Wisconsin-Madison. A SPIKE tutorial and research outcomes will be presented each year at the International Conference for High Performance Computing, Networking, Storage and Analysis. A one day High Performance Computing Boot Camp will be organized each year in conjunction with the American Society of Mechanical Engineers (ASME) conference and used to disseminate the software outcomes of this effort. Finally, this project will shape the research agendas of two graduate students working on advanced degrees in Computational Science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147337","fuchsia_atom","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Linear Equations"
"1147910","Collaborative Research SI2-SSE: Sustained Innovation in Acceleration of Molecular Dynamics on Future Computational Environments: Power to the People in the Cloud and on Accelerator","ACI","OFFICE OF MULTIDISCIPLINARY AC|CHEMISTRY PROJECTS|DMR SHORT TERM SUPPORT|Software Institutes|Theory|Models|Comput. Method","6/1/2012","12/17/2012","Adrian Roitberg","FL","University of Florida","Standard Grant","Rajiv Ramnath","5/31/2015","$227,634.00 ","","roitberg@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","1 UNIVERSITY OF FLORIDA, GAINESVILLE, FL","326112002","3523923516","CSE","1253|1991|1712|8004|6881","8005|7433|9216|9263|7237|7569|7573|7683|9232","$0.00 ","This collaborative project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA, Intel and Amazon is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using next generation accelerator hardware including Intel's MIC system and Graphics Processing Units (GPU). It will extend support to include all major MD techniques and develop open source accelerated analysis libraries. A priority is enhanced sampling techniques including Thermodynamic Integration, constant pH algorithms, Multi-Dimensional Hamiltonian Replica Exchange and Metadynamics. These elements will then be combined, in collaboration with Amazon to support MD as-a-service through easily accessible web front ends to cloud services, including Amazon's EC2 GPU hardware. Transitioning large scale MD workflows from requiring access to large supercomputer hardware to being accessible to all on desktop and cloud resources provides the critical software infrastructure to support transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy.<br/><br/>The software elements created through this project have an extremely broad impact. The integration of comprehensive support for next generation hardware acceleration into the AMBER software alone benefits a very large user base. With over 10,000 downloads of the latest AMBER Tools package from unique IPs and >800 sites using the AMBER MD engines testify to the scope of the community of researchers this work impacts. The development of simple web based front ends for use of elastically scalable cloud resources makes simulations routine for all researchers. Meanwhile education and outreach efforts train the next generation of scientists not just in how to use the MD acceleration libraries and advanced MD simulation techniques developed here but also gets them thinking about how their approach can be transformed given that performance that was previously restricted to large scale supercomputers is now available on individual desktops.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147910","fuchsia_atom","Collaborative Research SI2-SSE: Sustained Innovation in Acceleration of Molecular Dynamics on Future Computational Environments: Power to the People in the Cloud and on Accelerator"
"1147680","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Linear Equations","ACI","Software Institutes|CDS&E-MSS|OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS","6/1/2012","6/20/2012","Matthew Knepley","IL","University of Chicago","Standard Grant","Rajiv Ramnath","5/31/2015","$117,710.00 ","","knepley@rice.edu","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004|8069|1253|7478","8005|7433","$0.00 ","Drs. Negrut, Sameh, and Knepley will investigate, produce, and maintain a methodology and its software implementation that leverage emerging heterogeneous hardware architectures to solve billion-unknowns linear systems in a robust, scalable, and efficient fashion. The two classes of problems targeted under this project are banded dense and sparse general linear systems.<br/><br/>This project is motivated by the observation that the task of solving a linear system is one of the most ubiquitous ingredients in the numerical solution of Applied Mathematics problems. It is relied upon for the implicit integration of Ordinary Differential Equation (ODE) and Differential Algebraic Equation (DAE) problems, in the numerical solution of Partial Differential Equation (PDE) problems, in interior point optimization methods, in least squares approximations, in solving eigenvalue problems, and in data analysis. In fact, the vast majority of nonlinear problems in Scientific Computing are solved iteratively by drawing on local linearizations of nonlinear operators and the solution of linear systems. Recent advances in (a) hardware architecture; i.e., the emergence of General Purpose Graphics Processing Unit (GP-GPU) cards, and (b) scalable solution algorithms, provide an opportunity to develop a new class of parallel algorithms, called SPIKE, which can robustly and efficiently solve very large linear systems of equations.<br/><br/>Drawing on its divide-and-conquer paradigm, SPIKE builds on several algorithmic primitives: matrix reordering strategies, dense linear algebra operations, sparse direct solvers, and Krylov subspace methods. It provides a scalable solution that can be deployed in a heterogeneous hardware ecosystem and has the potential to solve billion-unknown linear systems in the cloud or on tomorrow?s exascale supercomputers. Its high degree of scalability and improved efficiency stem from (i) optimized memory access pattern owing to an aggressive pre-processing stage that reduces a generic sparse matrix to a banded one through a novel reordering strategy; (ii) good exposure of coarse and fine grain parallelism owing to a recursive, divide-and-conquer solution strategy; (iii) efficient vectorization in evaluating the coupling terms in the divide-and-conquer stage owing to a CPU+GPU heterogeneous computing approach; and (iv) algorithmic polymorphism, given that SPIKE can serve both as a direct solver or an effective preconditioner in an iterative Krylov-type method.<br/><br/>In Engineering, SPIKE will provide the Computer Aided Engineering (CAE) community with a key component; i.e., fast solution of linear systems, required by the analysis of complex problems through computer simulation. Examples of applications that would benefit from this technology are Structural Mechanics problems (Finite Element Analysis in car crash simulation), Computational Fluid Dynamics problems (solving Navier-Stokes equations in the simulation of turbulent flow around a wing profile), and Computational Multibody Dynamics problems (solving Newton-Euler equations in large granular dynamics problems).<br/><br/>SPIKE will also be interfaced to the Portable, Extensible Toolkit for Scientific Computation (PETSc), a two decades old flexible and scalable framework for solving Science and Engineering problems on supercomputers. Through PETSc, SPIKE will be made available to a High Performance Computing user community with more than 20,000 members worldwide. PETSc users will be able to run SPIKE without any modifications on vastly different supercomputer architectures such as the IBM BlueGene/P and BlueGene/Q, or the Cray XT5. SPIKE will thus run scalably on the largest machines in the world and will be tuned for very different network and hardware topologies while maintaining a simple code base.<br/><br/>The experience collected and lessons learned in this project will augment a graduate level class, ?High Performance Computing for Engineering Applications? taught at the University of Wisconsin-Madison. A SPIKE tutorial and research outcomes will be presented each year at the International Conference for High Performance Computing, Networking, Storage and Analysis. A one day High Performance Computing Boot Camp will be organized each year in conjunction with the American Society of Mechanical Engineers (ASME) conference and used to disseminate the software outcomes of this effort. Finally, this project will shape the research agendas of two graduate students working on advanced degrees in Computational Science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147680","fuchsia_atom","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Linear Equations"
"1148371","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack","ACI","INFORMATION TECHNOLOGY RESEARC|Software Institutes","6/1/2012","6/4/2012","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Patricia Knezek","5/31/2016","$1,251,644.00 ","Karen Tomko","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","Office of Sponsored Programs, Columbus, OH","432101016","6146888735","CSE","1640|8004","7433|8009|1640|8004","$0.00 ","The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc. Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by<br/>system administrators, or by end-users. These default parameters may or may not be optimal for all system configurations and applications.<br/><br/>The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications. Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: ""Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?"" The investigators, involving computer<br/>scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.<br/><br/>The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time? 2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs? 3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface? 4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications? and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework? The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU. The proposed designs will be integrated into the open-source MVAPICH2 library.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148371","fuchsia_atom","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack"
"1148258","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments","ACI","ADVANCES IN BIO INFORMATICS|ECOSYSTEM STUDIES|Software Institutes|Cyber Secur - Cyberinfrastruc","8/1/2012","8/7/2012","Corinna Gries","WI","University of Wisconsin-Madison","Standard Grant","Daniel Katz","7/31/2015","$203,327.00 ","","cgries@wisc.edu","21 North Park Street","MADISON","WI","21 North Park Street, MADISON, WI","537151218","6082623822","CSE","1165|1181|8004|8027","7434|8009|1165|1181|8004|8027","$0.00 ","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148258","fuchsia_atom","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments"
"1147892","SI2-SSI Collaborative Research: A Computational Materials Data and Design Environment","ACI","CHEMISTRY PROJECTS|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|DMR SHORT TERM SUPPORT","10/1/2012","9/20/2012","Kristin Persson","DC","Department of Energy","Interagency Agreement","Rajiv Ramnath","9/30/2017","$250,000.00 ","","kapersson@lbl.gov","1000 Independence Avenue, SW","Washington","DC","1000 Independence Avenue, SW, Washington, DC","205850002","","CSE","1991|1253|8004|1712","7433|8009|7237|7569|7644|9216|9263|1982|1253|1712|1991|8004","$0.00 ","",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147892","fuchsia_cloud","SI2-SSI Collaborative Research: A Computational Materials Data and Design Environment"
"1153402","NSF Workshop: Building Communities for Software Infrastructure for Sustained Innovation","ACI","Software Institutes","9/1/2011","9/8/2011","Jay Alameda","IL","University of Illinois at Urbana-Champaign","Standard Grant","Daniel Katz","8/31/2014","$54,851.00 ","","jalameda@ncsa.uiuc.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","8004","7556|8004","$0.00 ","The workshop for ""Building Communities for Software Infrastructure for Sustained Innovation"" brings together the practitioners of software sustainability, including NSF SI2 participants, to discuss issues and challenges of common interest, to formulate responses benefitting the software enterprise as a whole, and working to build a community of software developers, maintainers, supporters, and users. This workshop also builds concrete affiliations between SI2 projects and other NSF CIF21 projects to help these programs function as coherent providers of cyberinfrastructure in the service of science and engineering research.<br/><br/>This workshop addresses numerous challenges to success with software; for instance, sustaining a community of developers, building a community of users, fostering innovation and transforming innovation into a reliable, high quality component of software, building liaisons/affiliations with other efforts, developing and maintaining software requirements, managing change against an unstable software substrate, targeting innovative new software and hardware platforms; developing mechanisms to train new users and educate the next generations in ways to use software in scientific and engineering discovery, as well as to be developers of the next set of software capabilities, etc. The workshop will cultivate the software sustainability community, in order to bring innovative, useable, sustainable software to the user community, as diverse as they are, and to foster even more innovation, both in science and engineering research, as well as in the development of new software to enable better utilization of a comprehensive cyberinfrastructure.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1153402","fuchsia_atom","NSF Workshop: Building Communities for Software Infrastructure for Sustained Innovation"
"1148124","SI2:SSE-Collaborative Research: Advanced Software Infrastructure for Biomechanical Inverse Problems","ACI","Software Institutes|Mechanics of Materials and Str|Biomechanics & Mechanobiology","6/1/2012","6/20/2012","Paul Barbone","MA","Trustees of Boston University","Standard Grant","Rajiv Ramnath","5/31/2016","$240,450.00 ","","barbone@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","881 COMMONWEALTH AVE, BOSTON, MA","22151300","6173534365","CSE","8004|1630|7479","8005|1630|7479|8004","$0.00 ","Biomechanical imaging refers to the remote measurement of the mechanical properties of tissues, in-situ and in-vivo. Images of the tissue can be thus created by visualizing the mechanical property distributions. This technique relies on imaging tissue while it is deformed by a set of externally applied forces. Through image processing, the displacement field everywhere in the region of interest is inferred. An inverse problem for the relevant mechanical properties is then solved, given the measured displacement fields, an assumed form of the tissue's constitutive equation, and the law of conservation of momentum. Images of reconstructed parameters find applications in the detection, diagnosis and treatment monitoring of disease, and in designing patient specific models for surgical training and planning. Over the last decade we have developed a software package (NLACE) to solve this inverse problem eciently in different application domains. Through this award we will make enhancements to NLACE that will make it easier to utilize and modify, and extend its user base to a wider community.<br/><br/>The specific tasks for enhancing NLACE can be divided into two categories: (a) Steps to transition from a working prototype of NLACE to a software resource for the community. These include establishing an Input/Output standard for NLACE, creation of a GUI for input data and for monitoring the progress of the solution, hosting NLACE distributions, and creating sets of test data and documentation for its release. (b) Tasks that would enhance the functional capability of NLACE. These include the creation of a user-defined hyperelastic material model module to address a large class of tissue and material types, parallelization of NLACE on distributed memory, shared memory and GPU platforms, and quantifying uncertainty in the spatial distribution of the reconstructed parameters. We will measure our progress through user-feedback obtained during annual validation tests performed by a committed focus user-group that will test all aspects of the proposed research.<br/><br/>The proposed improvements to NLACE will further its application in the detection, diagnosis and treatment monitoring of diseases, generation of patient-specic models for surgical planning and image-guidance applications, and studies in biomechanics and mechanobiology. The parallel and uncertainty quantification strategies developed for NLACE can be applied to a broad class of inverse problem with PDE constraints, including acoustic and electromagnetic scattering, seismic inversion, diffuse optical tomography, aquifer permeability and thermometry. Our outreach plans ensure the dissemination of NLACE to our focus group and to a broader community through hosting on the Simtk NIH center website. Finally, two graduate students will be trained in the elds of computational science and mathematics and biomechanics, and results from the proposed research will be presented at conferences on computational and biomedical science and engineering.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148124","fuchsia_atom","SI2:SSE-Collaborative Research: Advanced Software Infrastructure for Biomechanical Inverse Problems"
"1148111","SI2:SSE-Collaborative Research: Advanced Software Infrastructure for Biomechanical Inverse Problems","ACI","Biomechanics & Mechanobiology|Mechanics of Materials and Str|Software Institutes","6/1/2012","6/20/2012","Assad Oberai","NY","Rensselaer Polytechnic Institute","Standard Grant","Rajiv Ramnath","5/31/2016","$290,301.00 ","Christopher Carothers","oberaa@rpi.edu","110 8TH ST","Troy","NY","110 8TH ST, Troy, NY","121803522","5182766000","CSE","7479|1630|8004","8005|1630|7479|8004","$0.00 ","Biomechanical imaging refers to the remote measurement of the mechanical properties of tissues, in-situ and in-vivo. Images of the tissue can be thus created by visualizing the mechanical property distributions. This technique relies on imaging tissue while it is deformed by a set of externally applied forces. Through image processing, the displacement field everywhere in the region of interest is inferred. An inverse problem for the relevant mechanical properties is then solved, given the measured displacement fields, an assumed form of the tissue's constitutive equation, and the law of conservation of momentum. Images of reconstructed parameters find applications in the detection, diagnosis and treatment monitoring of disease, and in designing patient specific models for surgical training and planning. Over the last decade we have developed a software package (NLACE) to solve this inverse problem eciently in different application domains. Through this award we will make enhancements to NLACE that will make it easier to utilize and modify, and extend its user base to a wider community.<br/><br/>The specific tasks for enhancing NLACE can be divided into two categories: (a) Steps to transition from a working prototype of NLACE to a software resource for the community. These include establishing an Input/Output standard for NLACE, creation of a GUI for input data and for monitoring the progress of the solution, hosting NLACE distributions, and creating sets of test data and documentation for its release. (b) Tasks that would enhance the functional capability of NLACE. These include the creation of a user-defined hyperelastic material model module to address a large class of tissue and material types, parallelization of NLACE on distributed memory, shared memory and GPU platforms, and quantifying uncertainty in the spatial distribution of the reconstructed parameters. We will measure our progress through user-feedback obtained during annual validation tests performed by a committed focus user-group that will test all aspects of the proposed research.<br/><br/>The proposed improvements to NLACE will further its application in the detection, diagnosis and treatment monitoring of diseases, generation of patient-specic models for surgical planning and image-guidance applications, and studies in biomechanics and mechanobiology. The parallel and uncertainty quantification strategies developed for NLACE can be applied to a broad class of inverse problem with PDE constraints, including acoustic and electromagnetic scattering, seismic inversion, diffuse optical tomography, aquifer permeability and thermometry. Our outreach plans ensure the dissemination of NLACE to our focus group and to a broader community through hosting on the Simtk NIH center website. Finally, two graduate students will be trained in the elds of computational science and mathematics and biomechanics, and results from the proposed research will be presented at conferences on computational and biomedical science and engineering.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148111","fuchsia_atom","SI2:SSE-Collaborative Research: Advanced Software Infrastructure for Biomechanical Inverse Problems"
"1535031","SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data","ACI","Software Institutes","9/1/2015","8/5/2015","Ye Zhao","OH","Kent State University","Standard Grant","Rajiv Ramnath","8/31/2018","$300,000.00 ","Xinyue Ye","zhao@cs.kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","OFFICE OF THE COMPTROLLER, KENT, OH","442420001","3306722070","CSE","8004","7433|8005","$0.00 ","Advanced technologies in sensing and computing have created urban trajectory datasets of humans and vehicles travelling over urban networks. Understanding and analyzing the large-scale, complex data reflecting city dynamics is of great importance to enhance both human lives and urban environments. Domain practitioners, researchers, and decision-makers need to manage, query and visualize such big and dynamics data to conduct both exploratory and analytical tasks. To support these tasks, this project develops a open source software tool, named TrajAnalytics, which integrates computational techniques of scalable trajectory database, intuitive and interactive visualization, and high-end computers, to explicitly facilitate interactive data analytics of the emerging trajectory data.<br/><br/>The software provides a new platform for researchers in transportation assessment and planning to directly study moving populations in urban spaces. Researchers in social, economic and behavior sciences can use the software to understand the complicated mechanisms of urban security, economic activities, behavior trends, and so on. Specifically, this software advances the research activities in the community of transportation studies. The software also acts as an outreach platform allowing government agencies to communicate more effectively with the public with the real-world dynamics of city traffic, vehicles, and networks. The integration of research and education prepares the next generation workforce, where students across multi-disciplines can benefit through their participation in the development and use of the software.<br/><br/>In order to lay the foundations for effective analysis of urban trajectory datasets, this software (1) facilitates easy access and unprecedented capability for researchers and analysts with a cloud-based storage and computing infrastructure; (2) develops a parallel graph based data model, named TrajGraph, where massive trajectories are managed on a large-scale graph created from urban networks which is naturally amenable for studying urban dynamics; and (3) provides a visualization interface, named TrajVis, which supports interactive visual analytics tasks with a set of visualization tools and interaction functions. A variety of transportation-related researchers from geography, civil engineering, computer science participate in the design, evaluation, and use of the software. This robust and easy-to-use software enables the users to conduct iterative, evolving information foraging and sense making.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535031","fuchsia_atom","SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data"
"1535150","SI2-SSE: Development of a Software Framework for Formalizing Forcefield Atom-Typing for Molecular Simulation","ACI","DMREF|DMR SHORT TERM SUPPORT|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","9/1/2015","8/24/2015","Christopher Iacovella","TN","Vanderbilt University","Standard Grant","Rajiv Ramnath","8/31/2018","$501,836.00 ","Janos Sallai","christopher.r.iacovella@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","Sponsored Programs Administratio, Nashville, TN","372350002","6153222631","CSE","8292|1712|1253|8004","7433|8005|8400|9150","$0.00 ","Molecular simulation plays a key role in understanding the atomistic and molecular level interactions that underlie many natural and man-made materials and processes. Classical molecular simulations rely upon forcefields to describe the various interactions that exist between atoms and/or groups of atoms. The availability of forcefields for molecular simulation has reduced the effort researchers must devote to the difficult and costly task of determining the interactions between species, allowing them to instead focus on the motivating scientific questions. However, determining which parameters in a forcefield to use is still often a tedious and error prone task. This difficulty is related to the strong dependence of the parameters on the chemical context of the atoms; the chemical context may depend on the local bonded environment of an atom in a molecule, the local environment of neighboring atoms, the type of molecule(s) being considered, the phase of the molecule(s), etc. Forcefields can contain tens or hundreds of different types of the same element, where each type represents the element in a different chemical context. Atom typing can be challenging, often requiring the user to consult textual comments scattered in parameter files or the scientific literature where the parameters were published. Unfortunately, as of today, the documentation of a typical forcefield tends to be scarce and unstructured, commonly expressed in plain English or in an ad-hoc shorthand notation, leading to ambiguities and increasing the likelihood of incorrect usage. While there are freely available tools to aid in atom-typing, these are typically specific to a particular forcefield or simulator and capture the atom-typing and parameterization rules in ways that are hard to maintain, debug, and evolve. The central tenet of this project is that there is an imminent need in the research community for a forcefield agnostic formalism to express atom-typing and parameterization rules in a way that is expressive enough for human consumption, while being machine readable to enable automation in complex scientific workflows.<br/><br/>This work proposes to establish a formalism to express the chemical context for which a particular forcefield parameter is applicable (i.e., forcefield usage semantics) and an atom-typing tool that interprets this formalism to generate forcefield parameterizations that are provably correct. Annotating forcefields with this formalism will serve as clear, unambiguous documentation of the atom-types and parameter usage, and also allows ambiguities or inconsistencies in forcefield specifications to be programmatically pinpointed during development. Successfully developing this framework will simplify the rules needed for atom-typing, which is crucial as forcefields continue to grow, specialize, and become more complex. The machine-readable annotations of forcefield usage semantics will enable automating tedious and error prone tasks and have the potential to enable new application areas, ranging from automated forcefield comparison and cross-validation, to complex simulation workflows integrating multiple forcefields and simulator tools. An open online forcefield repository containing the annotated forcefields, associated open source software, and documentation on how to use, annotate, and develop forcefields within the proposed framework will be developed to disseminate results and foster community involvement. ",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535150","fuchsia_atom","SI2-SSE: Development of a Software Framework for Formalizing Forcefield Atom-Typing for Molecular Simulation"
"1534688","SI2-SSE: Collaborative Research: An Intelligent and Adaptive Parallel CPU/GPU Co-Processing Software Library for Accelerating Reactive-Flow Simulations","ACI","Software Institutes|COMBUSTION|FIRE|& PLASMA SYS","9/1/2015","8/3/2015","Chih-Jen Sung","CT","University of Connecticut","Standard Grant","Rajiv Ramnath","8/31/2018","$214,357.00 ","","cjsung@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","438 Whitney Road Ext., Storrs, CT","62691133","8604863622","CSE","8004|1407","7433|8005|148E","$0.00 ","In order to develop the next generation of clean and efficient vehicle engines and power-generating combustors, engineers need the next generation of computational modeling tools. Accurately describing the chemistry of conventional and alternative liquid transportation fuels is vital to predict harmful emission levels and other important quantities, but the high computational cost of detailed models for chemistry poses a significant barrier to use by designers. In order to use such accurate models, software is needed that can efficiently handle chemistry in practical simulations. This collaborative project aims to develop such tools, employing the computational power of modern parallelized central processing units (CPUs) and graphics processing units (GPUs). In addition to helping designers create clean and efficient engine technology, the advances made in this project are widely applicable to other computational modeling problems including astrophysics, nuclear reactions, atmospheric chemistry, biochemical networks, and even cardiac electrophysiology.<br/><br/>The objective of the proposed effort is to develop software elements specifically targeted at co-processing on GPUs, CPUs, and other many-core accelerator devices to reduce the computational cost of using detailed chemistry and enable high-fidelity yet affordable reactive-flow simulations. This will be achieved by (1) developing and comparing chemical kinetics integration algorithms for parallel operation on CPUs and GPUs/accelerators, (2) developing a method for detecting local stiffness due to chemical kinetics and adaptively selecting the most efficient solver based on available hardware, (3) implementing a computational cell clustering strategy to group similar spatial locations, (4) demonstrating the improved performance offered by these software elements using commercial and open-source computational fluid dynamics codes for modeling reactive flows, and (5) designing a portable and sustainable software library based on the above software elements, including building a community of users. The result of this program will be an open source software library that significantly decreases the cost of using detailed, accurate chemistry in realistic combustion simulations; the success of the program will be determined based on achieving order-of-magnitude performance improvement or better.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1534688","fuchsia_atom","SI2-SSE: Collaborative Research: An Intelligent and Adaptive Parallel CPU/GPU Co-Processing Software Library for Accelerating Reactive-Flow Simulations"
"1534965","SI2-SSE: Quantum Monte Carlo Software for a Broad Electronic Structure Research Community via Minimal Explicit Dependency (MED) programming","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|Software Institutes|DMREF","10/1/2015","8/25/2015","Cyrus Umrigar","NY","Cornell University","Standard Grant","Rajiv Ramnath","9/30/2018","$420,798.00 ","","cyrusumrigar@cornell.edu","373 Pine Tree Road","Ithaca","NY","373 Pine Tree Road, Ithaca, NY","148502820","6072555014","CSE","1253|1712|8004|8292","7237|7433|7569|8005|8400|9215|9216","$0.00 ","This project, jointly supported by the Advanced Cyberinfrastructure (ACI) Division, Division of Materials Research (DMR), and Division of Chemistry (CHE) at NSF, will provide a robust, usable software infrastructure for methods for materials analysis, which, when run on high-performance computers, have the potential to revolutionize and accelerate the discovery of materials for a variety of engineering applications ranging from better catalysts, to solar cells to thermoelectrics used for refrigeration. The big hurdle to doing this is that these methods for solving the fundamental equation of quantum mechanics, known as the Schroedinger equation, are either not sufficiently accurate or are too computationally expensive. The purpose of this proposal is to develop software for a class of stochastic techniques, called quantum Monte Carlo methods, that will allow scientists and engineers to solve the equation more accurately and thereby predict materials properties with greater accuracy. The software uses a new programming paradigm to make the use as well as the extension of the software by developers easier. If researchers find this programming paradigm useful, it may propagate to a much larger research community and may eventually become an integral part of a new programming language. Thus this project will contribute to the basic sciences and potentially contribute to the field of computer science as well.<br/><br/>The goal of this proposal is to develop a software package consisting of a variety of quantum Monte Carlo (QMC) analysis tools. The software will have the capability to compute static and dynamic response properties, thereby greatly enhancing the range of applicability of QMC methods. Past research on optimizing many-body wave functions will aid in this goal since some of the formalism is the same. Both real space QMC methods, namely variational Monte Carlo (VMC) and diffusion Monte Carlo (DMC), and, determinant space QMC methods, namely semistochastic QMC (SQMC), which is an extension of the full configuration interaction QMC (FCIQMC) method, will be included, since each of these methods is the method of choice for some set of applications. The software uses the Minimal Explicit Dependency (MED) programming paradigm that greatly facilitates the development of complex programs. In particular, it lowers the barrier for researchers, who are not familiar with the inner workings of the program to contribute new functionality, thereby contributing to the long term survival of the software.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1534965","fuchsia_atom","SI2-SSE: Quantum Monte Carlo Software for a Broad Electronic Structure Research Community via Minimal Explicit Dependency (MED) programming"
"1216898","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms","ACI","ADVANCES IN BIO INFORMATICS|SPECIAL PROJECTS - CCF|Software Institutes","10/1/2012","9/13/2012","Viktor Prasanna","CA","University of Southern California","Standard Grant","Rudolf Eigenmann","9/30/2014","$350,000.00 ","Yogesh Simmhan","prasanna@usc.edu","University Park","Los Angeles","CA","University Park, Los Angeles, CA","900890001","2137407762","CSE","1165|2878|8004","7433|8211|1165|2878","$0.00 ","Solving scientific grand challenges requires effective use of cyber infrastructure. Future computing platforms, including Field Programmable Gate Arrays (FPGAs), General Purpose Graphics Processing Units (GPGPUs), multi-core and multi-threaded processors, and Cloud computing platforms, can dramatically accelerate innovation to solve complex problems of societal importance when supported by a critical mass of sustainable software.<br/><br/>This project will organize scientific communities to help leverage the disruptive potential of future computing platforms through sustainable software. Grand challenge problems in biological science, social science, and security domains will be targeted based on their under-served needs and demonstrated possibilities. Users will be engaged through interdisciplinary workshops that bring together domain experts with software technologists with the goals of identifying core opportunity areas, determining critical software infrastructure, and discovering software sustainability challenges. The outcome will be an in-depth conceptual design for a Center for Sustainable Software on Future Computing Platforms, as part of the Software Infrastructure for Sustained Innovation (SI2) program. The design, scoped toward grand challenge problems, will identify common and specialized software infrastructure, research, development and outreach priorities, and coordination with the SSE and SSI components of the SI2 program. The interactions will offer a comprehensive understanding of grand challenges that best map to future computing platforms and the software infrastructure to best support scientists' needs. The workshops will enhance understanding of future platforms' potential for transformative research and lead to key insights into cross-cutting problems in leveraging their potential. Published results will help guide future research and reduce barriers to entry for under-represented groups.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1216898","fuchsia_atom","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms"
"1216696","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms","ACI","Software Institutes","10/1/2012","9/13/2012","Manish Parashar","NJ","Rutgers University New Brunswick","Standard Grant","Rudolf Eigenmann","9/30/2014","$50,000.00 ","Shantenu Jha","parashar@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","33 Knightsbridge Road, Piscataway, NJ","88543925","8489320150","CSE","8004","7433|8211","$0.00 ","Solving scientific grand challenges requires effective use of cyber infrastructure. Future computing platforms, including Field Programmable Gate Arrays (FPGAs), General Purpose Graphics Processing Units (GPGPUs), multi-core and multi-threaded processors, and Cloud computing platforms, can dramatically accelerate innovation to solve complex problems of societal importance when supported by a critical mass of sustainable software.<br/><br/>This project will organize scientific communities to help leverage the disruptive potential of future computing platforms through sustainable software. Grand challenge problems in biological science, social science, and security domains will be targeted based on their under-served needs and demonstrated possibilities. Users will be engaged through interdisciplinary workshops that bring together domain experts with software technologists with the goals of identifying core opportunity areas, determining critical software infrastructure, and discovering software sustainability challenges. The outcome will be an in-depth conceptual design for a Center for Sustainable Software on Future Computing Platforms, as part of the Software Infrastructure for Sustained Innovation (SI2) program. The design, scoped toward grand challenge problems, will identify common and specialized software infrastructure, research, development and outreach priorities, and coordination with the SSE and SSI components of the SI2 program. The interactions will offer a comprehensive understanding of grand challenges that best map to future computing platforms and the software infrastructure to best support scientists' needs. The workshops will enhance understanding of future platforms' potential for transformative research and lead to key insights into cross-cutting problems in leveraging their potential. Published results will help guide future research and reduce barriers to entry for under-represented groups.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1216696","fuchsia_atom","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms"
"1339839","SI2-SSE: BenchLab: Open Community Tools and Infrastructure for Performance Research in Cloud, Mobile and Green Computing","ACI","SPECIAL PROJECTS - CCF|Software Institutes","10/1/2013","8/29/2013","Prashant Shenoy","MA","University of Massachusetts Amherst","Standard Grant","Rajiv Ramnath","9/30/2017","$500,000.00 ","David Irwin","shenoy@cs.umass.edu","Research Administration Building","AMHERST","MA","Research Administration Building, AMHERST, MA","10039242","4135450698","CSE","2878|8004","7433|8005","$0.00 ","The emergence of large-scale Internet applications and services has driven a surge in research on cloud platforms, virtualization, data center architectures, and green computing. However, realistic performance evaluation of new research prototypes continues to be a major challenge. Home-grown performance evaluation tools that are often used by researchers are no longer able to capture the complexity of today's real systems and applications. To address this drawback, the project seeks to develop BenchLab, an open, flexible community infrastructure comprising applications, workloads and tools to enable realistic performance evaluation and benchmarking by systems researchers. BenchLab is an open framework where source code and workload datasets are freely available for modification and use by researchers for their specific experiments. The framework consists of a suite of server-side benchmark applications and workloads that represent cloud, mobile web, and green computing environments. BenchLab employs a modular, extensible architecture that is designed to support a range of server applications and workloads, with the ability to add support for newer applications and workloads and retire outdated ones. BenchLab is designed to be easy to use for experimental systems research ""at scale"" in commercial clouds, such as Amazon EC2, or virtualized clusters in laboratory settings. <br/> BenchLab will provide open tools and workloads for the research community to enable researchers to run larger and more realistic performance evaluation experiments that better emulate today's real-word systems. BenchLab will be incorporated into hands-on lab assignments to teach students the science and art of experimental performance evaluation.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339839","fuchsia_atom","SI2-SSE: BenchLab: Open Community Tools and Infrastructure for Performance Research in Cloud, Mobile and Green Computing"
"1550597","SI2-SSI: Lidar Radar Open Software Environment (LROSE)","ACI","PHYSICAL & DYNAMIC METEOROLOGY|Software Institutes|EarthCube","8/1/2016","8/4/2016","Michael Bell","HI","University of Hawaii","Standard Grant","Rajiv Ramnath","7/31/2020","$2,499,996.00 ","Gary Barnes, Wen-Chau Lee, Michael Dixon","mmbell@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","2440 Campus Road, Box 368, HONOLULU, HI","968222234","8089567800","CSE","1525|8004|8074","4444|7433|8004|8009|9150","$0.00 ","Modern radars and lidars are a diverse class of instruments, capable of detecting molecules, aerosols, birds, bats and insects, winds, moisture, clouds, and precipitation. Scientists and engineers use them to perform research into air quality and pollution, dangerous biological plumes, cloud physics, cloud extent, climate models, numerical weather prediction, road weather, aviation safety, severe convective storms, tornadoes, hurricanes, floods, and movement patterns of birds, bats and insects. Radars and lidars are critical for protecting society from high impact weather and understanding the atmosphere and biosphere, but they are complex instruments that produce copious quantities of data that pose many challenges for researchers, students, and instrument developers. This project will develop a new set of tools called the Lidar Radar Open Software Environment (LROSE) to meet these challenges and help address the 'big data' problem faced by users in the research and education communities. This project will open new avenues of scientific investigation, including data assimilation to improve weather forecasts, and help to maximize returns on NSF investments in weather and climate research by providing better software tools to researchers, students, and educators. Improving the effectiveness of NSF research will provide significant scientific and societal benefits through an improved understanding of many diverse scientific topics that are relevant to public safety, national defense, and the global economy.<br/><br/>The LROSE project will develop a 'Virtual Toolbox' with a set of software tools needed for a diverse set of scientific applications. LROSE will be packaged so that it can be run on a virtual machine (VM), either locally or in the cloud, and stocked with core algorithm modules for those typical processing steps that are well understood and documented in the peer-reviewed literature. LROSE will enable the user community to use the core toolset to develop new research modules that address the specific needs of the latest scientific research. Through the VM Toolbox and a core software framework, other developers of open-source radar software can then provide their own compatible software tools to the set. By combining the open source approach with recent developments in virtual machines and cloud computing, we will develop a system that is both highly capable and easy to run on virtually any hardware, without the complexity of a compilation environment. The LROSE project will build on existing prototypes and available software elements, while facilitating community development of new techniques and algorithms to distribute a suite of documented software modules for performing radar and lidar analysis. These modules will each implement accredited scientific methods referencing published papers. The infrastructure and modules will allow researchers to run standard procedures, thereby improving the efficiency and reproducibility of the analyses, and encourage researchers to jointly develop new scientific approaches for data analysis. The use of collaborative open source methods will lead to a suite of available algorithmic modules that will allow scientists to explore radar and lidar data in new, innovative ways. Researchers will benefit from the improved toolset for advancing understanding of weather and climate, leading to a positive outcome in the advancement of scientific knowledge and societal benefits.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550597","fuchsia_atom","SI2-SSI: Lidar Radar Open Software Environment (LROSE)"
"1440665","SI2-SSE: High-Performance Software for Large-Scale Modeling of Binding Equilibria","ACI","Software Institutes","8/1/2014","6/17/2014","Emilio Gallicchio","NY","CUNY Brooklyn College","Standard Grant","Rajiv Ramnath","7/31/2016","$141,135.00 ","","egallicchio@brooklyn.cuny.edu","Office of Research & Sponsored P","Brooklyn","NY","Office of Research & Sponsored P, Brooklyn, NY","112102889","7189515622","CSE","8004","7433|8005|9216","$0.00 ","Living organisms are regulated by specific interactions among proteins and other macromolecules. Cancer and genetic diseases are due to altered interactions caused by mutations. Cellular organelles and viruses spontaneously build themselves from the ordered assembly of component molecules. While a complete understanding remains elusive, these and many other phenomena fundamentally rest on the ability of molecules to recognize and bind specifically to other molecules. The general principles of molecular recognition are being employed to design new drugs, chemical catalysts, and advanced materials. Computer models offer an important mean to disentangle and analyze molecular interactions and to produce quantitative predictions. The main objective of the project is to develop novel algorithms to model molecular recognition processes at atomic resolution on modern parallel computer architectures. This research seeks to increase the speed of the calculations and expand hardware support to enable the screening of larger sets of drug candidates and the study of multiple protein mutations under various conditions. The increased accuracy and availability of modeling software technologies by a larger community will lead to new ideas and research approaches, and ultimately to new discoveries in medicine, chemistry and material science. This effort will contribute to the establishment of in silico means to evaluate environmental and clinical claims. For example computational evidences on toxicity of substances can inform public policy in the same way that, for instance, atmospheric models are currently used for global climate projections.<br/><br/>The project targets the Binding Energy Distribution Analysis Method (BEDAM, for short), an accurate model of molecular binding, currently limited by computational performance. Outcomes of this research include deployment the BEDAM model for the first time on General Purpose Graphical Processing Units (GPGPU) and Many Integrated Core (MIC) massively parallel architectures. To this end, the mathematical formulation of the model will be tuned to best utilize the features of these modern computing architectures. Specialized recursive computational geometry algorithms will be developed to extract from the parallel hardware near optimal performance. Robust automated tools for the processing of molecular models and their analysis will be put in place for large scale applications. Accessible user interfaces will be implemented to ensure wide applicability and adoption of the software. These software applications will be distributed under an open source license to promote sharing and community contributions. Student research assistants form an integral part of the research team. While contributing meaningfully to scientific research, students from challenging socioeconomic backgrounds will acquire computer programming and software maintenance skills useful to enter the high tech job market.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440665","fuchsia_atom","SI2-SSE: High-Performance Software for Large-Scale Modeling of Binding Equilibria"
"1148362","SI2-SSE: SCIFIO: An Extensible Framework for Scientific Image Interoperability","ACI","ADVANCES IN BIO INFORMATICS|Software Institutes","7/1/2012","6/29/2012","Kevin Eliceiri","WI","University of Wisconsin-Madison","Standard Grant","Rajiv Ramnath","6/30/2015","$499,845.00 ","","eliceiri@wisc.edu","21 North Park Street","MADISON","WI","21 North Park Street, MADISON, WI","537151218","6082623822","CSE","1165|8004","8005|8004","$0.00 ","Digital imaging is one of the most commonly used tools in all of science. In a wide array of disciplines ranging from astronomy and geology to environmental science and biology, digital imaging approaches are used to capture dynamic processes in great detail. In the biological sciences, digital light microscopy has transformed the field with unprecedented levels of accessibility, functionality and performance with fewer compromises. Not only have there been great advances in the hardware and software needed to collect digital images, but also in the software tools for analysis, interpretation, storage and dissemination of the images. The accessibility of digital microscopy and imaging in general has resulted not only in widespread use and adoption but in increased innovation as well, with scientist pushing the envelope in developing core technologies in new directions such as hybrid and multimodal developments. Despite the great technological advances occurring in the field by commercial and academic researchers alike, there still is a fundamental barrier in imaging research that nearly every scientist encounters in their imaging workflow: the Proprietary File Format (PFF). PFFs are what the majority of software programs use to record images and any subsequent image analysis. While there are implemented efforts to create better open microscopy formats to help reduce the number of PFFs, it is clear that one universal format for everyone is not practical. Rather, the greatest practical need facing the community is not a universal scientific imaging format, but rather a universal scientific imaging format converter. With such a system, any current or future imaging format can be supported, including conversion from any PFF to any open standard. A universal imaging converter would enable a scientist to open a PFF from any imaging system and fully parse and analyze the full image contents without the need for any proprietary software. Such a converter would not only be of great utility to biologists but also of great benefit to instrument developers, who are equally limited by the lack of transparency and access of PFFs. We propose to develop a robust scientific software element for imaging file format interoperability. This effort that we dub ""SCIFIO"" for ""Scientific Image Format Input and Output"" would build on our current successful ""Bio-Formats"" efforts to make a file converter for light microscopy in our research domain and ""harden"" these efforts to make a robust interchange library for all of biological microscopy. The system will be generalizable, extensible and adaptable to new emerging microscopy types. It will also serve as a model for adaptation to other scientific imaging types.<br/><br/>A great practical barrier to collaborative work in imaging is the issue of proprietary file formats. One of the most fundamental needs in imaging is being able to open and freely share the original pixel information and associated text information with others using any processing software or workflow desired. We are developing ""Scientific Image Format Input and Output"" (SCIFIO) a robust software package that can read and convert any proprietary image file format. By harnessing the power of a reusable software project like SCIFIO, the community will be able to freely share all content collected on any imaging system both for visualization and quantitative analysis. SCIFIO will be developed as a robust software element, both as a library anyone can utilize and as a full software tool kit that any developer can easily use to add converter support to their application freely. This is important as our target---and thus our impact---is not only on the research scientist, but the wider community including researchers from other fields, academic software developers, commercial software developers, and educators.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148362","fuchsia_atom","SI2-SSE: SCIFIO: An Extensible Framework for Scientific Image Interoperability"
"1440607","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS","9/1/2014","8/1/2014","Andrew Sommese","IN","University of Notre Dame","Standard Grant","Rajiv Ramnath","8/31/2017","$199,847.00 ","Bei Hu, Charles Wampler","sommese@nd.edu","940 Grace Hall","NOTRE DAME","IN","940 Grace Hall, NOTRE DAME, IN","465565708","5746317432","CSE","1253|8004|8069","7433|8005|8251","$0.00 ","Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.<br/><br/>Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440607","fuchsia_atom","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials"
"1550493","Collaborative Research: SI2-SSI: Sustaining Innovation in the Linear Algebra Software Stack for Computational Chemistry and Other Sciences","ACI","Software Institutes","7/15/2016","7/26/2016","Robert van de Geijn","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","6/30/2018","$750,983.00 ","Don Batory, Margaret Myers, Victor Eijkhout, John Stanton","rvdg@cs.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","101 E. 27th Street, Suite 5.300, Austin, TX","787121532","5124716424","CSE","8004","7433|8004","$0.00 ","Scientific discovery now often involves computer simulation in addition to, or instead of, laboratory experimentation. This can accelerate, improve, and/or expand scientific insight, often at a great reduction in cost. Many such computer simulations spend much or most of their time solving linear algebra (matrix) problems. For these simulations, linear algebra problems constitute the most basic building blocks of the computation. As a result, software libraries (bundles of specialized code) that efficiently solve linear algebra problems fundamentally support sustained innovation in science. The project aims to create a next generation of software libraries for this domain and will make these libraries available to the scientific community as open source software that can be easily ported to current and future computer architectures. This will directly and indirectly impact discovery in academia, at the national labs, and in industry. The project will also impact affordable education through open course ware that is expected to reach a broad audience. The involvement of undergraduate and graduate students will strengthen the pool of qualified individuals trained to support scientific computing. The project involves research staff and students who are members of traditionally underrepresented groups.<br/><br/><br/>The BLAS (Basic Linear Algebra Subprograms) are well-known routines that provide standard building blocks for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations. Because the BLAS are efficient, portable, and widely available, they are commonly used in the development of high quality linear algebra software, such as the well-known Linear Algebra PACKage (LAPACK), as an example. However, the BLAS libraries that exist today have not evolved to new computing architectures, and hence do not perform as well as they could. The technical goal and scope of this project, therefore, is to develop a new high-performance dense linear algebra library with broad functionality that can be easily ported to current and future multi-core and many-core processors. The project builds on the BLAS-like Library Instantiation Software (BLIS) effort that has exposed low-level primitives that facilitate the high-performance implementation of BLAS. By implementing the higher-level dense linear algebra functionality in terms of these low-level primitives, portable high performance will be achieved for higher-level functionality needed by many scientific computing applications. Contributions will include the to-be developed techniques for implementing such software, the resulting open source software, and pedagogical artifacts that will include open course ware.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550493","fuchsia_atom","Collaborative Research: SI2-SSI: Sustaining Innovation in the Linear Algebra Software Stack for Computational Chemistry and Other Sciences"
"1550486","Collaborative Research: SI2-SSI: Sustaining Innovation in the Linear Algebra Software Stack for Computational Chemistry and Other Sciences","ACI","Software Institutes","7/15/2016","7/26/2016","Tze Meng Low","PA","Carnegie-Mellon University","Standard Grant","Rajiv Ramnath","6/30/2018","$265,000.00 ","","lowt@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","5000 Forbes Avenue, PITTSBURGH, PA","152133815","4122689527","CSE","8004","7433|8004","$0.00 ","Scientific discovery now often involves computer simulation in addition to, or instead of, laboratory experimentation. This can accelerate, improve, and/or expand scientific insight, often at a great reduction in cost. Many such computer simulations spend much or most of their time solving linear algebra (matrix) problems. For these simulations, linear algebra problems constitute the most basic building blocks of the computation. As a result, software libraries (bundles of specialized code) that efficiently solve linear algebra problems fundamentally support sustained innovation in science. The project aims to create a next generation of software libraries for this domain and will make these libraries available to the scientific community as open source software that can be easily ported to current and future computer architectures. This will directly and indirectly impact discovery in academia, at the national labs, and in industry. The project will also impact affordable education through open course ware that is expected to reach a broad audience. The involvement of undergraduate and graduate students will strengthen the pool of qualified individuals trained to support scientific computing. The project involves research staff and students who are members of traditionally underrepresented groups.<br/><br/><br/>The BLAS (Basic Linear Algebra Subprograms) are well-known routines that provide standard building blocks for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations. Because the BLAS are efficient, portable, and widely available, they are commonly used in the development of high quality linear algebra software, such as the well-known Linear Algebra PACKage (LAPACK), as an example. However, the BLAS libraries that exist today have not evolved to new computing architectures, and hence do not perform as well as they could. The technical goal and scope of this project, therefore, is to develop a new high-performance dense linear algebra library with broad functionality that can be easily ported to current and future multi-core and many-core processors. The project builds on the BLAS-like Library Instantiation Software (BLIS) effort that has exposed low-level primitives that facilitate the high-performance implementation of BLAS. By implementing the higher-level dense linear algebra functionality in terms of these low-level primitives, portable high performance will be achieved for higher-level functionality needed by many scientific computing applications. Contributions will include the to-be developed techniques for implementing such software, the resulting open source software, and pedagogical artifacts that will include open course ware.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550486","fuchsia_atom","Collaborative Research: SI2-SSI: Sustaining Innovation in the Linear Algebra Software Stack for Computational Chemistry and Other Sciences"
"1047828","Collaborative Research: SI2-SSI: Development of an Integrated Molecular Design Environment for Lubrication Systems (iMoDELS)","ACI","NANOSCALE: INTRDISCPL RESRCH T|Software Institutes","10/1/2011","7/22/2014","Peter Cummings","TN","Vanderbilt University","Continuing grant","Rajiv Ramnath","9/30/2017","$2,542,681.00 ","Clare McCabe, Gabor Karsai, Akos Ledeczi","peter.cummings@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","Sponsored Programs Administratio, Nashville, TN","372350002","6153222631","CSE","1674|8004","9150|1674|8004","$0.00 ","This project is focused on developing, deploying and distributing the Integrated Molecular Design Environment for Lubrication Systems (iMoDELS), an open-source simulation and design environment (SDE) that encapsulates the expertise of specialists in first principles, forcefields and molecular simulation related to nanoscale lubrication in a simple web-based interface. The iMoDELS SDE is being developed using model-integrated computing (MIC), a state-of-the-art powerful, well-established, extensible, community-supported, and application-hardened software engineering framework that supports scientific and engineering workflows. Making iMoDELS broadly accessible is motivated by the high cost (over $800B/yr in the US) of friction and wear, which, along with the methodology to overcome them, lubrication, are collectively known as tribology. Tribology involves molecular mechanisms occurring on a nanometer scale, and hence understanding tribological behavior on this scale is critical to developing new technologies for reducing wear due to friction. Deployment of iMoDELS will enable non-computational specialists to be able to evaluate, design and optimize nanoscale lubrication systems, such as hard disk drives, NEMS (nanoelectromechanical systems) and MEMS (microelectromechanical systems), and experiments involving rheological measurements via atomic force microscopes (AFMs) and surface force apparatuses (SFAs).<br/><br/>The iMoDELS SDE brings together a unique combination of materials and computer scientists who will combine their skills to abstract the deep human expertise currently required for the development of simulation-based experiments, thus making broadly available easy-to-use tools to an empirically driven area of science and engineering (nanotribology) of rapidly growing technological importance. iMoDELS includes the creation and open dissemination of forcefield and simulation results databases that will benefit the simulation community worldwide and catalyze broad-based activity in this area. The proposed research also includes the interdisciplinary training of undergraduate and graduate students, as well as postdoctoral researchers at the interface of tribology, computational materials sciences, and computer science. The PIs will use the iMoDELS SDE, and results from it, in presentations used in outreach to local area high school and in classes given to undergraduate students. The iMoDELS SDE will be vigorously promoted through workshops and presentations at national conferences, and via the dedicated website for development and dissemination.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047828","fuchsia_tree","Collaborative Research: SI2-SSI: Development of an Integrated Molecular Design Environment for Lubrication Systems (iMoDELS)"
"1047932","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|COFFES|Software Institutes","9/15/2010","9/7/2010","Denis Zorin","NY","New York University","Standard Grant","Daniel Katz","12/31/2014","$250,000.00 ","","dzorin@mrl.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","70 WASHINGTON SQUARE S, NEW YORK, NY","100121019","2129982121","CSE","1253|7478|7552|8004","1253|7478|7552","$0.00 ","We propose to develop and deploy mathematical software for boundary-value problems in three-dimensional complex geometries. The algorithms in the library will be based on integral equation formulations. The library will be designed to scale on novel computing platforms that comprise special accelerators and manycore architectures. <br/><br/>Integral equations can be used to conduct simulations on many problems in science and engineering with significant societal impact. Three example applications on which the proposed simulation technologies will have an impact in this project are microfluidic chips, biomolecular electrostatics, and plasma physics. First, microfluidic chips are submillimeter-sized devices used for medical diagnosis and drug design. Optimizing the function of such devices at low cost requires efficient computer simulation tools, such as the ones we propose to develop. Second, understanding the structure and function of biomolecules such as DNA and proteins is crucial in biotechnology. The proposed technologies can be used to resolve bimolecular electrostatic interactions. Third, plasma physics, which is related to fusion nuclear reactors, includes electrostatic interactions in complex geometries, and the proposed work will enable large-scale three-dimensional simulations. <br/><br/>The key features of the proposed software are: (1) parallel fast multipole methods, (2) efficient geometric modeling techniques for complex geometries, (3) simple library interfaces that allow use of the proposed software by non-experts, and (4) scalability on heterogeneous architectures.<br/><br/>Along with our research activities, an educational and dissemination program will be designed to communicate the results of this work to students and researchers. Several postdoctoral, graduate, and undergraduate students will be involved with the project. Additional educational activities will include research experiences for undergraduates, leveraging ongoing programs such as NSF REUs. We will encourage participation by women, minorities, and underrepresented groups.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047932","fuchsia_atom","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures"
"1339737","SI2-SSE: Peer-to-Peer Overlay Virtual Network for Cloud Computing Research","ACI","SPECIAL PROJECTS - CCF|Software Institutes","9/1/2013","5/26/2015","Renato Figueiredo","FL","University of Florida","Standard Grant","Rajiv Ramnath","8/31/2017","$494,107.00 ","","renato@acis.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","1 UNIVERSITY OF FLORIDA, GAINESVILLE, FL","326112002","3523923516","CSE","2878|8004","7433|8005|9251","$0.00 ","Modern cloud computing systems use virtual machine technologies to deliver unprecedented flexibility to users, enabling businesses and individuals to cost-effectively deploy computing and storage capacity on-demand, at scale, across multiple infrastructures distributed geographically. While the ability to deploy virtual machines for cloud computing is widely supported, researchers face increasing challenges in prototyping and deploying experimental research systems that span across multiple cloud providers. In particular, providing end-to-end network connectivity among distributed virtual machines in today's Internet environment (where nodes are often constrained by firewalls and network address translators) requires significant investment of time in development, testing and maintenance of code needed solely to provide connectivity. This project addresses these connectivity challenges in cloud computing by developing an open-source scientific software element that allows researchers and users of clouds to seamlessly create virtual networks on demand for distributed virtual machines. To this end, the project creates software-defined virtual networks that support the standard Internet Protocol (IP) and use tunneling of virtual network packets over Peer-to-Peer (P2P) links among virtual machines for scalable and resilient messaging. In addition to the core IP-over-P2P virtual networking, the software provides a framework for configuration, management and monitoring that enables easy deployment of user-defined overlays for inter-cloud research experiments. <br/>The open-source software developed in this project enables advances in the state-of-the-art of research of cloud computing systems and applications. Complementary to research and development activities, this project delivers educational modules, tutorials, software packages, and pre-configured virtual machine images that allow non-expert users to deploy their own virtual networks over private, commercial and public clouds. Because cloud computing technologies are increasingly pervasive and of growing importance to the economy and society, the broader impacts of this project can reach Internet users at large who benefit from the ability to seamlessly interconnect cloud virtual machines across multiple providers. In particular, leveraging online social networking technologies, the virtual network software software enables individuals and small groups to easily create social virtual private networks connecting personal computers and multiple cloud resources.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339737","fuchsia_atom","SI2-SSE: Peer-to-Peer Overlay Virtual Network for Cloud Computing Research"
"1339745","SI2-SSI: Collaborative: The XScala Project: A Community Repository for Model-Driven Design and Tuning of Data-Intensive Applications for Extreme-Scale Accelerator-Based Systems","ACI","CYBERINFRASTRUCTURE","10/1/2013","9/16/2013","David Bader","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","9/30/2016","$1,188,710.00 ","Edward Riedy, Richard Vuduc","bader@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","7231","7433|8009|9145","$0.00 ","The increasing gap between processor and memory performance -- referred to as the memory wall -- has led high-performance computing vendors to design and incorporate new accelerators into their next-generation systems. Representative accelerators include recon&#64257;gurable hardware such as FPGAs, heterogeneous processors such as CPU+GPU processors, highly multicore and multithreaded processors, and manycore co-processors and general-purpose graphics processing units, among others. These accelerators contain myriad innovative architectural features, including explicit control of data motion, large-scale SIMD/vector processing, and multithreaded stream processing. Such features provide abundant opportunities for developers to achieve high-performance for applications that were previously deemed hard to optimize. This project aims to develop tools that will assist developers in using hardware accelerators (co-processors) productively and effectively.<br/><br/>This project's specific technical focus is on data-intensive kernels including large dictionary string matching, dynamic programming, graph theory, and sparse matrix computations that arise in the domains of biology, network security, and the social sciences. The project is developing XScala, a software framework for designing efficient accelerator kernels. The framework contains a variety of design time and run-time performance optimization tools. The project concentrates on data-intensive kernels, bound by data movement. It proposes optimization techniques including (a) enhancing and exploiting maximal concurrency to hide data movement; (b) algorithmic reorganization to improve spatial and/or temporal locality; (c) data structure transformations to improve locality or reduce the size of the data (compressed structures); and (d) prefetching, among others. The project is also developing a public software repository and forum, called the XBazaar, for community-developed accelerator kernels. This project includes workshops, tutorials, and the PIs class and summer projects as various means by which to increase community involvement. The broader impacts include productive use of emerging classes of accelerator-augmented computer systems; creation of an open and accessible community repository, the XBazaar, for distributing accelerator-tuned computational kernels, software, and models; training of graduate and undergraduate students; and dissemination through publications, presentations at scientific meetings, lectures, workshops, and tutorials. The framework itself will be released as open-source code and as precompiled binaries for several common platforms, through the XBazaar, as an initial step toward building a community around accelerator kernels.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339745","fuchsia_atom","SI2-SSI: Collaborative: The XScala Project: A Community Repository for Model-Driven Design and Tuning of Data-Intensive Applications for Extreme-Scale Accelerator-Based Systems"
"1339676","SI2 SSI: Collaborative Research: Sustained Innovations for Linear Algebra Software (SILAS)","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS","10/1/2013","9/2/2015","James Demmel","CA","University of California-Berkeley","Continuing grant","Rajiv Ramnath","9/30/2016","$611,954.00 ","","demmel@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","CSE","1253|8004|8069","7433|8009","$0.00 ","As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs' own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.<br/><br/>The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for ""big data"". No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339676","fuchsia_tree","SI2 SSI: Collaborative Research: Sustained Innovations for Linear Algebra Software (SILAS)"
"1339600","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics","ACI","STELLAR ASTRONOMY & ASTROPHYSC|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","1/1/2014","8/29/2013","Francis Timmes","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","12/31/2017","$306,840.00 ","","fxt44@mac.com","ORSPA","TEMPE","AZ","ORSPA, TEMPE, AZ","852816011","4809655479","CSE","1215|1253|8004","1206|7433|8005","$0.00 ","As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of ""time domain astronomy"". The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011. MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.<br/><br/>This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community. MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation. This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339600","fuchsia_atom","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics"
"1440620","SI2-SSE: The Next Generation of the Montage Mosaic Engine","ACI","OFFICE OF MULTIDISCIPLINARY AC||Software Institutes","10/1/2014","8/14/2014","Graham Berriman","CA","California Institute of Technology","Standard Grant","Rajiv Ramnath","9/30/2017","$499,902.00 ","","gbb@ipac.caltech.edu","1200 E California Blvd","PASADENA","CA","1200 E California Blvd, PASADENA, CA","911250600","6263956219","CSE","1253|1798|8004","1206|7433|8005","$0.00 ","Images produced by the new generation of astronomical instruments are addressing fundamental questions about the Universe, such as the formation of the very first galaxies after the Big Bang, and the very first stages of the formation of stars in massive dust clouds in our Galaxy. Exploiting this new generation of data is difficult because the data sets they produce are sufficiently complex and large as to demand new approaches to data processing that lag far behind developments in instrumentation. A growing community is working to rectify this state-of-affairs. This project will deliver software tools that will aggregate data from the new instruments into images of large scale regions of the sky so that astronomers can fully study scientific questions such as those identified above. This approach of studying aggregated images, or mosaics, is a powerful tool in astronomy. The project will deliver the next generation of an existing mosaic-building engine, Montage, which is in wide use in astronomy and in educational activities. It will support processing of the new data sets such that they can be visualized in immersive tools such as the World Wide Telescope, widely used in developing innovative approaches to education, and such that that they can generate data used by Citizen Science services such as Zooniverse. Montage will come be bundled with a set of tools that will enable astronomers to process massive collections images on powerful ""cloud computing"" platforms. These tools will be applicable to many data-intensive problems in fields such as earthquake prediction, DNA sequencing, and climate modeling. Finally, Montage is in wide use in developing and testing national cyberinfrastructure to benefit the U.S. science community. We anticipate that the next-generation Montage will be used in the same way to develop ever more powerful cyberinfrastructure as data volumes grow rapidly in all fields.<br/><br/>In greater detail, the project will deliver the next generation of the Montage image mosaic engine, which will offer new capabilities that respond to the changing astronomy data and computing landscapes. These capabilities, requested by the user community, are: 1. Support for mosaicking of data cubes, now routinely generated by modern instrumentation; 2. Support for two widely used sky-partitioning schemes, HEALPix and TOAST; 3. An API to enable users to call Montage directly in Python and other languages. The work to develop memory management and subsetting techniques to support mosaicking will be available for others to use and extend. Support for HEALPix will enable integration and analysis of far-infrared, cosmic background data sets with other image data sets. TOAST will enable essentially any image data set to be incorporated into the WWT. Montage will be bundled with a turnkey package of open source tools that provision resources and run applications on cloud platforms. This package will build on knowledge gained in creating data products at scale with cloud platforms. These tools will bring cloud computing to scientists who have little system configuration knowledge, one of the biggest barriers to entry; these tools are general purpose and will be applicable to data intensive applications in may fields. Thus Montage will provide powerful new capabilities to astronomers, to projects analyzing data at scale to create new data products, and to scientists in data-intensive fields outside astronomy. The next-generation toolkit will inherit the sustainable Montage architecture, which has attracted a large user base among astronomers, E/PO specialists, and computer technologists. Montage is written in C, is portable across all common Unix platforms, highly scalable and delivered as components that are easy to incorporate into pipelines and processing environments. Montage is the only mosaic engine with all these characteristics. The project will use the evolutionary delivery lifecycle model. The code will be a made accessible on the GitHub repository, and released as Open Source code with a BSD 3-clause license. A Users' Panel will advise on detailed specifications.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440620","fuchsia_atom","SI2-SSE: The Next Generation of the Montage Mosaic Engine"
"1441963","SI2-SSI: SAGEnext: Next Generation Integrated Persistent Visualization and Collaboration Services for Global Cyberinfrastructure","ACI","Software Institutes","5/16/2014","6/20/2016","Jason Leigh","HI","University of Hawaii","Continuing grant","Rajiv Ramnath","9/30/2018","$4,661,450.00 ","","leighj@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","2440 Campus Road, Box 368, HONOLULU, HI","968222234","8089567800","CSE","8004","7433|8004|8009|9150|9251|8211","$0.00 ","Cyberinfrastructure runs the gamut - from computing, networks, data stores, instruments, observatories, and sensors, to software and application codes - and promises to enable research at unprecedented scales, complexity, resolution, and accuracy. However, it is the research community that must make sense of all the data being amassed, so the SAGEnext (Scalable Adaptive Graphics Environment) framework is an innovative user-centered platform that connects people to their data and colleagues, locally and remotely, via tiled display walls, creating a portal, or wide-aperture ""digital lens,"" with which to view their data and one another. It enables Cyber-Mashups, or the ability to juxtapose and integrate information from multiple sources in a variety of resolutions, as easily as the Web makes access to lower-resolution images today. <br/><br/>SAGEnext expands on a vibrant partnership among national and international universities, supercomputer centers, government laboratories and industry; 100 institutions worldwide use the current version of SAGE. For computational scientists, from such diverse fields as biology, earth science, genomics, or physics, SAGEnext will transform the way they manage the scale and complexity of their data. For computer scientists, SAGEnext is a platform for conducting research in human-computer interaction, cloud computing, and advanced networking. SAGEnext capabilities, integrating visualization application codes, cloud documents, stereo 3D, and new user-interaction paradigms, is unprecedented and heretofore not available, and will have a transformative effect on data exploration and collaboration, making cyberinfrastructure more accessible to end users, in both the laboratory and in the classroom. SAGEnext, integrated with advanced cyberinfrastructure tools, will transform the way today's scientists and future scientists manage the scale and complexity of their data, enabling them to more rapidly address problems of national priority - such as global climate change or homeland security - which benefits all mankind. These same tools can better communicate scientific concepts to public policy and government officials, and via museum exhibitions, to the general public.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1441963","fuchsia_tree","SI2-SSI: SAGEnext: Next Generation Integrated Persistent Visualization and Collaboration Services for Global Cyberinfrastructure"
"1440571","Collaborative Research: SI2-SSE: Pythia Network Diagnosis Infrastructure (PuNDIT)","ACI","Software Institutes|Campus Cyberinfrastrc (CC-NIE)","9/1/2014","8/14/2014","Shawn McKee","MI","University of Michigan Ann Arbor","Standard Grant","Rajiv Ramnath","8/31/2017","$231,187.00 ","","smckee@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","3003 South State St. Room 1062, Ann Arbor, MI","481091274","7347636438","CSE","8004|8080","7433|8005","$0.00 ","In today's world of distributed collaborations of scientists, there are many challenges to providing effective infrastructures to couple these groups of scientists with their shared computing and storage resources. The Pythia Network Diagnostic InfrasTructure (PuNDIT) project will integrate and scale research tools and create robust code suitable for operational needs to address the difficult challenge of automating the detection and location of network problems. <br/><br/>PuNDIT will build upon the de-facto standard perfSONAR network measurement infrastructure to gather and analyze complex real-world network topologies coupled with their corresponding network metrics to identify possible signatures of network problems from a set of symptoms. For example if the symptoms suggest a router along the path has buffers configured too small for high performance, Pythia will return a diagnosis of ""Small Buffer"". If symptoms indicate non-congestive packet-loss for a particular network segment, the user can be notified of a possible ""Bad Network Segment"". A primary goal for PuNDIT is to convert complex network metrics into easily understood diagnoses in an automated way.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440571","fuchsia_atom","Collaborative Research: SI2-SSE: Pythia Network Diagnosis Infrastructure (PuNDIT)"
"1450339","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS|CDS&E","8/1/2015","8/11/2015","Matthew Knepley","IL","University of Chicago","Standard Grant","Rajiv Ramnath","2/29/2016","$262,655.00 ","","knepley@rice.edu","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","1253|8004|8069|8084","7433|8004|8009|8084","$0.00 ","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450339","fuchsia_atom","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction"
"1460334","SI2-SSE: Parallel and Adaptive Simulation Infrastructure for Biological Fluid-Structure Interaction","ACI","INTERFAC PROCESSES & THERMODYN|Mechanics of Materials and Str|DYNAMICAL SYSTEMS|Software Institutes","7/1/2014","9/11/2014","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Standard Grant","Rajiv Ramnath","12/31/2015","$75,361.00 ","","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","104 AIRPORT DR STE 2200, CHAPEL HILL, NC","275991350","9199663411","CSE","1414|1630|7478|8004","","$0.00 ","The immersed boundary (IB) method is both a mathematical formulation and a numerical approach to problems of fluid-structure interaction, treating the specific case in which an elastic structure is immersed in a viscous incompressible fluid. The IB method was introduced to describe the fluid dynamics of heart valves, but this methodology has also been applied to a wide range of problems in biological and non-biological fluid dynamics. The IB method typically requires high spatial resolution to resolve the viscous boundary layers at fluid-structure interfaces and, at higher Reynolds numbers, to resolve vortices shed from such interfaces. To improve the efficiency of the IB method, the principal investigator has developed an adaptive version of the IB method that employs block-structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where it is needed. IBAMR software is a distributed-memory parallel implementation of this adaptive scheme. The key goal of this project is to make IBAMR the unifying software framework for users of the IB method, thereby establishing a community of researchers who employ a common software infrastructure for biofluids model development and simulation. The project aims to enhance IBAMR substantially by (1) developing and implementing implicit IB schemes that will allow for the efficient use of large numerical timesteps; (2) developing and implementing extensions of the basic IB methodology, including a new variable-viscosity version of the IB method, and an existing stochastic version for microscale and nanoscale problems in which Brownian motion is important; (3) optimizing IBAMR for use with modern as well as projected-future high performance computing systems comprised of multi-core compute nodes interconnected by a high-speed network; and (4) developing front-end tools for model construction, validation, and execution, thereby facilitating the adoption and use of IBAMR, especially by students and researchers with limited computational experience.<br/><br/>From the writhing and coiling of DNA, to the beating and pumping motions of cilia and flagella, to the flow of blood in the heart and throughout the circulation, coupled fluid-structure systems are ubiquitous in biology and physiology. This project aims to enhance significantly the IBAMR software developed by the principal investigator. IBAMR is a framework for performing computer simulations of biological fluid mechanics, and this project seeks to establish IBAMR as a unifying software infrastructure that will serve as a common ""language"" for developing and exchanging such models. IBAMR is already being actively used within several independent research projects that aim to model different aspects of cardiovascular dynamics, such as platelet aggregation and the fluid dynamics of natural and prosthetic heart valves. Such simulations promise ultimately to improve the efficacy of devices and procedures for treating cardiovascular disease. This software also is being used within projects that study other problems in biofluid mechanics, including insect flight, aquatic locomotion, and the dynamics of phytoplankton. By enhancing IBAMR, this project will also enhance significantly the ability of these and other research groups to construct detailed biofluids models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will enhance the IBAMR software substantially, extending the range of problems to which it may be applied, and improving the methods implemented within the software as well as the efficiency of the implementation. The work of this project will extend greatly the community of students and researchers who are able to use IBAMR to model biological fluid-structure interaction, in part by implementing graphical software tools for building IB models and running IB simulations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1460334","fuchsia_atom","SI2-SSE: Parallel and Adaptive Simulation Infrastructure for Biological Fluid-Structure Interaction"
"1148473","SI2-SSE: Fingerprinting Scientific Codes to Verify and Create Compatible System Software Environments","ACI","Software Institutes","4/1/2012","4/11/2012","Philip Papadopoulos","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","3/31/2017","$500,000.00 ","","phil@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8004","8005|8004","$0.00 ","Every computational science application requires supporting software (code libraries, data files, other programs) to run properly. However, the computer systems that complex scientific applications run on are continuously updated by administrators to add new capabilities, fix bugs, and plug security holes. These are necessary changes, but a user needs to know if ""something changed in the system that will break his/her code"". A similar question arises when running a code on a different computer from which it was compiled or developed: ""does the new system have all the necessary support software to run my applications?"" This proposal describes a plan for developing the software capability to both statically and dynamically ""fingerprint"" a scientific application to answer those two questions and therefore test for compatibility of new system or determine if a new update could have deleterious effects. Our proposed software system will have five major components: fingerprint format, static analysis of an application to create one type of fingerprint, a system verifier that checks if a system satisfies the needs defined in a fingerprint, dynamic analysis to find applications dependencies only detectable at run time, and a composer capability to automatically define a Rocks cluster managed system that will fulfill the requirements of a specific fingerprint. All software developed will be open-source and freely available.<br/><br/>Nearly every person who uses a computer has seen messages similar to ""new updates have been installed, you must restart your computer."" Sometimes, applications break because of these updates. This proposal's broader impact is that, for a wide variety of scientific applications, we will be able to determine if a new system or an existing system is compatible by verifying the code's fingerprint. Since, scientific applications can be very sensitive to underlying software changes, it will be possible to detect incompatibility before wasting time and energy to run a large-scale application on an production systems. When these applications are used as the basis of scientific discovery, it becomes even more imperative that we view the computing environment and application together as an experimental apparatus whose configuration we need to better understand. We believe that the composer capability of this proposal can have the transformational impact by developing fully- descriptive catalogs of what an application needs to function properly. In the era in which simulation output is being used to drive policy, this kind of scientific reproducibility has impact well beyond the notions of academic completeness.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148473","fuchsia_atom","SI2-SSE: Fingerprinting Scientific Codes to Verify and Create Compatible System Software Environments"
"1148359","Collaborative Research SI2 SSE: Pipeline Framework for Ensemble Runs on Clouds","ACI","Software Institutes","5/1/2012","5/7/2012","Beth Plale","IN","Indiana University","Standard Grant","Daniel Katz","4/30/2014","$292,863.00 ","","plale@cs.indiana.edu","509 E 3RD ST","Bloomington","IN","509 E 3RD ST, Bloomington, IN","474013654","8128550516","CSE","8004","8005|8004","$0.00 ","Cloud computing is an attractive computational resource for e-Science because of the ease with which cores can be accessed on demand, and because the virtual machine implementation that underlies cloud computing reduces the cost of porting a numeric or analysis code to a new platform. It is difficult to use cloud computing resources for large-scale, high throughput ensemble jobs however. Additionally, the computationally oriented researcher is increasingly encouraged to make data sets available to the broader community. For the latter to be achieved, using capture tools during experimentation to harvest metadata and provenance reduces the manual burden of marking up results. Better automatic capture of metadata and provenance is the only means by which sharing of scientific data can scale to meet the burgeoning explosion of data.<br/><br/>This project develops a pipeline framework for running ensemble simulations on the cloud; the framework has two key components: ensemble deployment and metadata harvest. Regarding the former, on commercial cloud platforms typically a much smaller number of jobs than desired can be started at any one time. An ensemble run will need to be pipelined to a cloud resource, that is, executed in well-controlled batches over a period of time. We will use platform features of Azure, and employ machine learning techniques to continuously refine the pipeline submission strategy and workflow strategies for ensemble parameter specification, pipelined deployment, and metadata capture. Regarding the latter key component, we expect to reduce the burden of sharing scientific datasets resulting from the use of cloud resources through automatic metadata and provenance capture and representation that aligns the metadata with emerging best practices in data sharing and discovery. Ensemble simulations result in complex data sets, whose reuse could be increased by expressive, granule and collection level metadata, including the lineage of the resulting products, to contribute towards trust.<br/><br/>In this project we focus on a compelling and timely application from climate research: One of the more immediate and dangerous impacts of climate change could be a change in the strength of storms that form over the oceans. In addition, as sea level rises due to global warming and melting of the polar ice caps, coastal communities will become increasingly vulnerable to storm surge. There have already been indications that even modest changes in ocean surface temperature can have a disproportionate effect on hurricane strength and the damage inflicted by these storms. In an effort to understand these impacts, modelers turn to predictions generated by hydrodynamic coastal ocean models such as the Sea, Lake and Overland Surges from Hurricanes (SLOSH) model. The proposed research advances the knowledge and understanding of probabilistic storm surge products by enhancements to the SLOSH model itself and through mechanisms that take advantage of commercial cloud resources. This knowledge is expected to have application in research, the classroom, and in operational settings.<br/><br/>The broader significance of the project is several-fold. Cloud computing is an important economic driver but it remains difficult for use in computationally driven scientific research. This project lowers the barriers to conducting e-Science research that utilizes cloud resources, specifically Azure. It will contribute tools to help researchers share, preserve, and publicize the scientific data sets that result from their research. Because we focus on and improve an application that predicts storm surge in response to sea level changes and severe storms, our work contributes to societal responses and adaptations to climate change, including planning and building the sustainable, hazard-resilient coastal communities of the future.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148359","fuchsia_atom","Collaborative Research SI2 SSE: Pipeline Framework for Ensemble Runs on Clouds"
"1550221","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","ACI","Software Institutes","7/1/2016","7/6/2016","Rainer Fries","TX","Texas A&M University Main Campus","Continuing grant","Bogdan Mihaila","6/30/2020","$119,351.00 ","","rjfries@comp.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","400 Harvey Mitchell Pkwy South, College Station, TX","778454375","9798626777","CSE","8004","026Z|7433|7569|8009|8084","$0.00 ","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550221","fuchsia_tree","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)"
"1048018","SI2-SSE: Software Infrastructure For Partitioning Sparse Graphs on Existing and Emerging Computer Architectures","ACI","COMPUTER SYSTEMS|Software Institutes","9/15/2010","9/7/2010","George Karypis","MN","University of Minnesota-Twin Cities","Standard Grant","Sol J. Greenspan","8/31/2015","$499,784.00 ","Michael Whalen","karypis@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","200 OAK ST SE, Minneapolis, MN","554552070","6126245599","CSE","7354|8004","7354","$0.00 ","Algorithms that find good partitionings of large, sparse, and unstructured graphs represent an important technique for developing effective and computationally efficient approaches for problems that need to process and analyze such graphs. As a result, they have found extensive applications in many diverse areas such as high-performance computing, scientific computing, VLSI design, data mining, pattern recognition, computer graphics, network analysis, database and geographical information systems, operations research, optimization, and scheduling. This project will develop and make available a software infrastructure that provides a broad range of graph partitioning tools for large, sparse, and unstructured graphs. This infrastructure will be built using modern object-oriented software engineering principles that will facilitate their modularity, user-extensibility, maintainability, and community development; and incorporate novel graph partitioning algorithms that can scale to graphs containing billions of nodes and facilitate the partitioning of different types of graphs on different computing architectures. This software infrastructure will enable the efficient execution of scientific numerical simulations on parallel systems containing tens of thousands of processing nodes and billions of mesh elements, the development of divide-and-conquer approaches for synthesizing very large VLSI circuits on different chip architectures, the clustering and analysis of very large graphs and networks, and the solution of a wide-range of partitioning problem instances involving different objectives and constraints.<br/><br/>This will positively impact numerous science & engineering disciplines, commercial companies, non-profit organizations, and individuals that benefit from the results of the computations that are enabled and facilitated by the various application domains that rely on graph partitioning. Finally, the project integrates the research with an educational plan focused on undergraduate and graduate education and mentoring through courses, software engineering projects, summer institutes, and research opportunities; and a community development and an outreach plan designed to promote broad adoption of the resulting software infrastructure by providing extensive documentation, online tutorials, and organizing meetings at relevant conferences and workshops.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1048018","fuchsia_atom","SI2-SSE: Software Infrastructure For Partitioning Sparse Graphs on Existing and Emerging Computer Architectures"
"1047955","SI2-SSE: SciDB - A Scientific DataManagement System","ACI","COMPUTER SYSTEMS|Software Institutes","9/15/2010","9/14/2010","Michael Stonebraker","MA","Massachusetts Institute of Technology","Standard Grant","Daniel Katz","8/31/2014","$500,000.00 ","Samuel Madden","stonebraker@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","77 MASSACHUSETTS AVE, Cambridge, MA","21394301","6172531000","CSE","7354|8004","7354","$0.00 ","Current relational database systems (RDBMSs) were engineered for the business data processing market, and not for scientific users (e.g, astronomers, physicists, chemists, oceanographers, or earth scientists). By and large, science users either grumble and use RDBMSs or, more often, ""roll their own"" data management software. Significant projects, such as the Large Hadron Collider (LHC) and the NASA Mission to Planet Earth, have spent millions of dollars on custom software systems, with limited applicability to other projects. As such, after a generation of science applications, there is limited shared data management infrastructure. <br/><br/>SciDB is a project focused on building an open-source DBMS focused on the needs of science users. We have developed the requirements of SciDB based on a close collaboration with a number of scientists. Data management features include a nested array data model (rather than the tabular model of RDBMSs) with operations attuned to scientific data, a no-overwrite storage model (allowing interaction with historical results), and support for uncertainty, named versions and provenance information. <br/><br/>At this point, there is a distributed team of 17 programmers and scientists working actively on the design and implementation of SciDB, assisted by an advisory committee of 15 scientists. This team is primarily focused on the research issues surrounding the design of SciDB, and most contributors are involved in the project as volunteers or are paid by their individual organizations. <br/><br/>A demo of the first working proof-of-concept SciDB prototype was given at the VLDB conference in August 2009, and a first public release of SciDB is planned for September 2010. The purpose of this NSF grant is to enhance SciDB with additional science-oriented features, including time travel, versions, uncertainty and provenance. With NSF?s help, we expect to develop a full-function system by the end of the grant period.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047955","fuchsia_atom","SI2-SSE: SciDB - A Scientific DataManagement System"
"1047764","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|COFFES|Software Institutes","9/15/2010","9/7/2010","John Hart","IL","University of Illinois at Urbana-Champaign","Standard Grant","Daniel Katz","8/31/2014","$251,643.00 ","","jch@cs.uiuc.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","1253|7478|7552|8004","1253|1630|7478|7552","$0.00 ","The Lagrangian Coherent Structures (LCS) software elements developed by this project will provide a valuable tool set for fluid mechanics research to extract new discoveries from the vast and growing body of computational and experimental fluid mechanics data. The computation of LCS enables a systematic approach to accurately characterize transport phenomena in complex systems that pose insurmountable challenges to traditional Eulerian approaches. Prior, ah hoc implementations of LCS have already helped in important, real-world challenges including, tracking pollutants in the ocean, developing novel diagnoses and therapies for cardiovascular disease, and helping airplanes to avoid turbulence. We will produce an open LCS software system to provide a modular, extensible and flexible infrastructure to broaden the community of scientists and engineers that benefit from LCS, in problems ranging from fluid dynamics to general dynamical systems. Prof. Shadden will lead the LCS algorithm design and numerical analysis, and Prof. Hart will oversee the package's architectural design and the efficient parallel implementation of its elements.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047764","fuchsia_atom","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis"
"1047962","SI2-SSE: A Tracing Virtual Machine for Statistical Computing","ACI","OFFICE OF MULTIDISCIPLINARY AC|SPECIAL PROJECTS - CCF|COMPUTER SYSTEMS|COFFES|Software Institutes","9/15/2010","9/8/2010","Jan Vitek","IN","Purdue University","Standard Grant","Sol J. Greenspan","8/31/2013","$489,084.00 ","Olga Vitek","j.vitek@neu.edu","Young Hall","West Lafayette","IN","Young Hall, West Lafayette, IN","479072114","7654941055","CSE","1253|2878|7354|7552|8004","1253|1640|2878|7552","$0.00 ","Scripting languages are lightweight, dynamic computer programming languages designed to maximize productivity by offering high-level abstractions and reducing syntactic overhead. Scripting languages strive to optimize programmer time, rather than machine time, which is desirable early in the software life cycle. However, they lose their appeal when requirements stabilize and projects enter their deployment phase. The compromises made to reduce development time make it hard to scale to large data sets or to get the needed speed to perform computationally intensive tasks. This is certainly the case for the R scripting language and its development environment. R is an extremely powerful tool for Biostatistics practitioners. Over the years, it has grown to become a mainstay for statistical computing in the life sciences. This project's goal is to build an execution environment for the R programming language, which we call ReactoR, with particular emphasis on improving scalability for real-world applications in the life sciences. The proposed work will yield an implementation of R which delivers performance comparable to compiled native code and lets developers write code without having to resort to low-level intrinsics. This virtual machine will include a trace-based compiler for generating native code for the most frequently executed routines and a concurrent garbage collector to decrease footprint.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047962","fuchsia_atom","SI2-SSE: A Tracing Virtual Machine for Statistical Computing"
"1339884","Collaborative Research: SI2-SSE: A Petascale Numerical Library for Multiscale Phenomena Simulations","ACI","Software Institutes|CDS&E","10/1/2013","9/11/2013","Dmitry Pekurovsky","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","9/30/2016","$358,917.00 ","","dmitry@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8004|8084","7433|8005|8084","$0.00 ","Multiscale phenomena are a grand challenge for theory, simulations and experiments. It is difficult to understand phenomena at both highest and lowest scales, as well as all those in between. This challenge shows up in diverse fields. One of the long-standing problems in cosmology is understanding cosmological structure formation. In computer simulations of this process the challenge is increasing grid resolution while retaining the essential physics. In all-atom molecular dynamics simulations of enzymes the challenge is simulating systems with a large number of atoms while resolving long-range interactions and having sufficiently high throughput. Such simulations are critical in understanding important biological processes and eventually designing new drugs. Another prime example of multiscale phenomena is turbulent flows, a rich and complex subject of great relevance to many of the main technological issues of the day, including climate, energy, and the management of oil and biohazards. Understanding turbulent flows is critical for design of new transportation vehicles, improving efficiency of combustion processes and managing their environment pollution. Here simulations have been historically limited, and remain so, due to extremely high computational cost, even using the high-end computational systems available to researchers today. Reducing this cost, and efficiently using the computational resources, often requires specialized expertise, as well as significant development time and cost many research groups cannot afford. This project will develop a powerful suite of critical software components to provide tools for performing simulations of multiscale phenomena. <br/><br/>The suite will implement state-of-the-art techniques for reducing communication cost, which has become the most important contributing factor to the total simulation cost, especially at larger scales. It will provide a flexible set of features that will make it usable in a great number of codes across the disciplines. In particular, the library will include user-friendly interfaces for Fourier transforms, spectral and compact differentiation in three dimensions, in addition to widely used communication routines (transposes, halo exchanges). This combination of emphasis on scalable performance and richness of features makes this suite unique among other libraries in existence today. Given the extraordinary challenge of simulations of multiscale phenomena, this library will provide a realistic path towards the Exascale. The suite will be available under an open source license. User outreach will be undertaken through the project website, mailing list, user surveys and presentations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339884","fuchsia_atom","Collaborative Research: SI2-SSE: A Petascale Numerical Library for Multiscale Phenomena Simulations"
"1339727","SI2-SSE: Software Infrastructure for Revealing Gene and Genome Evolution, Anchored by Enhancement of Multiple Genome Alignment Software MCSCAN","ACI","Software Institutes|CROSS-EF ACTIVITIES","2/1/2014","9/16/2013","Xiyin Wang","GA","University of Georgia Research Foundation Inc","Standard Grant","Rajiv Ramnath","1/31/2017","$493,692.00 ","Andrew Paterson","wangxy@uga.edu","310 East Campus Rd","ATHENS","GA","310 East Campus Rd, ATHENS, GA","306021589","7065425939","CSE","8004|7275","7433|8005","$0.00 ","The software MCSCAN, used to align multiple genomes, will be enhanced to contribute to deciphering the structure and evolutionary trajectories of eukaryotic genomes and genes, in particular addressing consequences of recursive whole-genome duplications. Burgeoning sets of eukaryotic genome sequences provide the foundation for a new spectrum of investigations into the functional and evolutionary consequences of gene and genome duplication, as well as the means to clarify knowledge of relationships among particular genes. The current software can only align small numbers of genomes; and layers of duplicated blocks produced by different genome duplication events are not readily deconvoluted, thus failing to provide crucial information toward understanding evolutionary trajectories of genomes and gene families. The enhanced software will mitigate these limitations.<br/><br/>The enhanced software will greatly help researchers to reconstruct the evolutionary trajectories of genomes and gene families, including the singularly challenging genomes of angiosperms and other taxa that have experienced polyploidization events. In particular, multiple alignment (of an expanded number of genomes) will be preceded by a multiple-way comparison of homologous regions at the DNA level, which will provide a holographic grasp of layers of homology produced by different duplication events. To reflect the evolutionary trajectories of structural changes, genomes will be input in a stepwise manner, with those of simple structures first. The resulting multiple alignment will much more accurately depict evolutionary relationships between chromosomal regions from diverse genomes, and easily be visualized and understood by users. The core part of the software will be implemented using the C++ programming language while the visualization module will be developed in Python language. The multiple and pairwise alignment information will be stored in MySQL or SQLite databases. The software will be tamed to work under multiple operating systems, including MS Windows, UNIX and Linux. Online service will be developed using the Django Web framework and jQuery (a concise JavaScript Library), and added to our NSF-supported PGDD. The software will be formed by several independent modules, which can be freely used by other researchers. A to-be-constructed web server accompanying the software will show figures illustrating genome structures, comparison between different plants, and evolutionary changes inferred to have occurred over millions of years. These intuitive visual resources will benefit researchers seeking to understand the evolution of plants, as well as elementary and middle school students, and readers at local libraries. The program will regularly host visitors from other institutions, countries, and the public. The enhanced software and related results in genomic analysis will be reported in academic conferences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339727","fuchsia_atom","SI2-SSE: Software Infrastructure for Revealing Gene and Genome Evolution, Anchored by Enhancement of Multiple Genome Alignment Software MCSCAN"
"1339822","SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS)","ACI","Software Institutes|SPECIAL PROJECTS - CCF|CDS&E-MSS|OFFICE OF MULTIDISCIPLINARY AC","10/1/2013","8/25/2015","Jack Dongarra","TN","University of Tennessee Knoxville","Continuing grant","Rajiv Ramnath","9/30/2016","$1,427,955.00 ","","dongarra@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","1 CIRCLE PARK, KNOXVILLE, TN","379960003","8659743466","CSE","8004|2878|8069|1253","7433|8009|9150","$0.00 ","As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs' own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.<br/><br/>The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for ""big data"". No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339822","fuchsia_tree","SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS)"
"1339757","SI2-SSE: Collaborative Research: Software Elements for Transfer and Analysis of Large-Scale Scientific Data","ACI","Software Institutes|SPECIAL PROJECTS - CCF","9/1/2013","8/29/2013","Gagan Agrawal","OH","Ohio State University","Standard Grant","Rudolf Eigenmann","8/31/2017","$400,000.00 ","","agrawal@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","Office of Sponsored Programs, Columbus, OH","432101016","6146888735","CSE","8004|2878","7433|8005","$0.00 ","As science has become increasingly data-driven, and as data volumes and velocity are increasing, scientific advance in many areas will only be feasible if critical `big-data' problems are addressed - and even more importantly, software tools embedding these solutions are readily available to the scientists. Particularly, the major challenge being faced by current data-intensive scientific research efforts is that while the dataset sizes continue to grow rapidly, neither among network bandwidths, memory capacity of parallel machines, memory access speeds, and disk bandwidths are increasing at the same rate.<br/><br/>Building on top of recent research at Ohio State University, which includes work on automatic data virtualization, indexing methods for scientific data, and a novel bit-vectors based sampling method, the goal of this project is to fully develop, disseminate, deploy, and support robust software elements addressing challenges in data transfers and analysis. The prototypes that have been already developed at Ohio State are being extended into two robust software elements: <br/><br/>Software Element 1: GridPFTP (Grid Partial-File Transport Protocol): An Extention to GridFTP: an extention of GridFTP that allows users to specify a subset of the file to be transferred, avoiding unnecessary transfer of the entire file.<br/>Software Element 2: Parallel Readers for NetCDF and HDF5 for Paraview and VTK: Data subsetting and sampling tools for NetCDF and HDF5 that perform data selection and sampling at the I/O level, and in parallel.<br/><br/>This project impacts a number of scientific areas, i.e., any area that involves big (and growing) dataset sizes and need for data transfers and/or visualization. This project also contributes to computer science research in `big data', including scientific (array-based) databases, and visualization. Another contribution will be towards preparation of the broad science and engineering research community for big data handling and analytics.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339757","fuchsia_atom","SI2-SSE: Collaborative Research: Software Elements for Transfer and Analysis of Large-Scale Scientific Data"
"1440727","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud","ACI","CDS&E|DMR SHORT TERM SUPPORT|Software Institutes","10/1/2014","7/31/2014","Alejandro Strachan","IN","Purdue University","Standard Grant","Rajiv Ramnath","9/30/2017","$349,804.00 ","Benjamin Haley, Chunyu Li","strachan@purdue.edu","Young Hall","West Lafayette","IN","Young Hall, West Lafayette, IN","479072114","7654941055","CSE","8084|1712|8004","7433|8005|8400|7237|9216|024E|085E","$0.00 ","The use of polymers, their composites, and nanostructures is growing at a fast pace, both by displacing traditional materials and by enabling emerging technologies. Examples range from the all-composite airframe of the Boeing 787 and the new Airbus 350 to wearable electronics. This project aims to develop a software infrastructure to simulate these materials with atomic resolution and make these tools universally accessible and useful via on-line computing. These simulations have the potential to accelerate the development of optimized material formulations that can benefit society and reduce the associated costs by combining physical with computational experiments. Making these advanced tools available for free online simulations and complementing them with tutorials and educational material will encourage their use in the classroom and will impact the education of new generations of engineers and scientists familiar with these powerful tools that will be required to address tomorrow's challenges.<br/><br/>The objective of this effort is to enable pervasive, reproducible molecular simulations of polymeric materials using state-of-the-art tools and with quantified uncertainties building on recent breakthroughs in molecular simulations, cyber-infrastructure and uncertainty quantification. The framework will consist of three main components: i) powerful simulation tools for polymer nano structures including: state-of-the-art molecular builders, a parallel MD engine with stencils that enable efficient structure relaxation and property characterization and post-processing codes; ii) a UQ framework to orchestrate the molecular simulations and propagate uncertainties in input parameters to predictions and compare the predictions to experimental values; iii) databases of force fields and molecular structures as well as predicted and experimental properties. The simulation framework will be deployed via NSF's nanoHUB where users will be able to run online simulations without downloading or installing any software while expert users will have the option to download, modify and contribute to the infrastructure. Usage of and contributions to the software framework will be facilitated and encouraged via online and in-person user guides, learning modules and research tutorials.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440727","fuchsia_atom","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud"
"1440638","SI2-SSE: GEM3D: Open-Source Cartesian Adaptive Complex Terrain Atmospheric Flow Solver for GPU Clusters","ACI","Software Institutes|EarthCube|PHYSICAL & DYNAMIC METEOROLOGY","10/1/2014","8/13/2014","Inanc Senocak","ID","Boise State University","Standard Grant","Rajiv Ramnath","9/30/2017","$500,000.00 ","Grady Wright, Donna Calhoun, Elena Sherman","senocak@boisestate.edu","1910 University Drive","boise","ID","1910 University Drive, boise, ID","837251135","2084261574","CSE","8004|8074|1525","7433|8005|9150","$0.00 ","The U.S. Government invests in leadership supercomputing facilities through several agencies to advance scientific discovery in many fronts. This project is motivated by this national commitment to supercomputing research and the increasing availability of many-core computing hardware from workstations to supercomputers. Today scientists and engineers have access to extreme-scale computing resources. However, many legacy codes do not take advantage of recent innovations in computing hardware, and there is a lack of open-source simulation science software that can effectively leverage the many-core computing paradigm. Computational fluid dynamics (CFD) solvers have advanced many fields such as aerospace engineering and atmospheric sciences. Many current open-source CFD models and numerical weather prediction models do not take full advantage of the superior compute performance of graphics processing units (GPUs). By creating an open-source community model that can execute on multi-GPU workstations and large GPU clusters, the project team expects to broaden the use of high-performance computing in fluid dynamics applications. The immediate target application is wind modeling over complex terrain, to support research and development in wind resource assessment, power forecasting, atmospheric research, and air pollution. Through this project, the PIs will continue to transfer and expand the knowledge bases in GPU computing, computational mathematics, and software engineering to new students. Skill sets that transcend traditional disciplines are highly prized by national laboratories as there is a critical shortage of workforce who can conduct scientific research using supercomputers. Students and postdoctoral researchers who are involved in this project will contribute toward this critical workforce. <br/><br/>This project brings together engineers, applied mathematicians, and computer scientists. The entire suite of software elements will be designed for GPU clusters with an MPI-CUDA implementation that overlaps computation with communications using a three-dimensional decomposition for enhanced scalability. The implementation will balance performance and further development and ownership by a broader community of academic researchers. The team will follow modern software engineering practices for concurrent applications. An adaptive mesh refinement strategy that can scale on GPU clusters will be developed. A novel projection method based on radial basis functions will impose the divergence-free constraint on a hierarchy of adaptively refined grids. Software elements will be tested using unit testing and verification techniques for concurrent programs, and against data available from benchmark numerical problems. The flow solver will include modules for the immersed boundary approach for arbitrarily complex terrain and the dynamic large-eddy simulation technique. The software implementation and syntax will be intuitive to allow contributions from a larger community. The project team expects the proposed software to help reduce modeling errors with very high resolution simulations and contribute toward a fundamental understanding of turbulent winds over complex terrain. The PIs of this project will continue their teaching efforts in Parallel Scientific Computing, Computational Mathematics, and Software Engineering. The results will be disseminated through conference presentations and via a wiki site for the open-source project. Software elements will be released under an open-source GNU General Public License.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440638","fuchsia_atom","SI2-SSE: GEM3D: Open-Source Cartesian Adaptive Complex Terrain Atmospheric Flow Solver for GPU Clusters"
"1450372","Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory","ACI","Software Institutes|DMR SHORT TERM SUPPORT|OFFICE OF MULTIDISCIPLINARY AC","6/15/2015","6/11/2015","Lin Lin","CA","University of California-Berkeley","Standard Grant","Rajiv Ramnath","5/31/2019","$504,016.00 ","","linlin@math.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","CSE","8004|1712|1253","7433|8009|9216|8084","$0.00 ","Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (""in silico"") design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the ""cubic scaling wall"" of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure ""ELSI"" that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.<br/><br/>ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the ""scaling wall"" of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450372","fuchsia_atom","Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory"
"1440530","SI2-SSE: LC/DC: Lockless Containers and Data Concurrency","ACI","SPECIAL PROJECTS - CCF|Software Institutes","1/1/2015","8/13/2014","Damian Dechev","FL","University of Central Florida","Standard Grant","Rajiv Ramnath","12/31/2017","$500,000.00 ","","dechev@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","ORLANDO","FL","4000 CNTRL FLORIDA BLVD, ORLANDO, FL","328168005","4078821120","CSE","2878|8004","7433|8005|8004","$0.00 ","Multicore programming demands a change in the way we design and use fundamental algorithms and data structures. This research represents a forward-looking and pragmatic approach that will lead to the discovery of the key principles for effective data and resource management for multiprocessor application development. In the course of this project the PI will create new methodologies and tools for the design, verification, and effective use of lock-free and wait-free data structures and algorithms. The proposed methodology will allow for the construction and use of lightweight multiprocessor algorithms with minimal overhead by supporting only the minimal set of operations and guarantees required by the user's application. The ideas advanced in this work will allow first-of-a-kind technology that will deliver immense boost in performance and software reuse; thus productivity will increase for developers of commercial and scientific applications. This research will pave the way for tool-based specification and verification of nonblocking algorithms, which will help reliability of multiprocessor programs. <br/><br/>This work will create novel multiprocessor data structures that provide wait-free and lock-free progress. A concurrent object is lock-free if it guarantees that some thread makes progress. A wait-free algorithm guarantees that all threads make progress, thus eliminating performance bottlenecks and entire classes of safety hazards such as starvation, deadlock, and order violations. Unlike a sequential data structure, a concurrent container must maintain correctness when multiple threads are performing its operations. Achieving this correctness adversely affects the complexity and performance of the operations. As a result, users of concurrent containers are often forced to sacrifice functionality or safety guarantees to achieve desired performance. Here, the PI will introduce the use of alternative function models that will deliver high performance in parts of the program that require less functionality and more functionality in other fragments of the program that need it. The deliverables of this research include: a collection of formally verified multiprocessor data structure designs including queues, vectors, ring buffers, sets, and hash maps; a wait-free database; a multiple resource lock manager; a set of unified concurrent APIs to assist the end users of the data structures; and a technique for specification of the key progress and correctness properties of these containers. All software developed under this project will be released under BSD License and will be made available to the broad research and development community.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440530","fuchsia_atom","SI2-SSE: LC/DC: Lockless Containers and Data Concurrency"
"1450468","SI2-SSI Integration of Synchrotron X-Ray Analysis Software Methods into the Larch Framework","ACI","OFFICE OF MULTIDISCIPLINARY AC|GEOPHYSICS|DMR SHORT TERM SUPPORT|Software Institutes|EarthCube","10/1/2015","9/16/2015","Matthew Newville","IL","University of Chicago","Standard Grant","Rajiv Ramnath","9/30/2018","$540,969.00 ","","newville@cars.uchicago.edu","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","1253|1574|1712|8004|8074","7433|7574|8009|9216|8004","$0.00 ","The solutions to many of the outstanding problems in geology, environmental science, material science, and biology require understanding the chemical state and detailed atomic structure of the molecules and solids that make up our world. Such problems range from understanding the molecular forms of arsenic in rice, determining the chemical composition of the earths interior, and improving the performance and reducing the environmental impact of batteries that are in our laptops, cellphones, and cars. The nation's synchrotron facilities provide powerful X-ray facilities that allow researchers to study these questions by investigating the chemical makeup and crystal structure of complex, real-world materials such as plant seeds, contaminated soils, human and animal tissue, minerals and meteorites, and working batteries and catalysts. Synchrotron measurement techniques have developed very rapidly over the past few decades, and are being used by many more researchers. The ability to handle and interpret the large and complex datasets now being routinely generated at these facilities is often a significant challenge, even for experts. The work here will develop the Larch X-ray analysis framework to provide open-source software that is easy to use and specific enough to correctly interpret several categories of synchrotron X-ray data. The approach will provide tools that are flexible enough to enable researchers to explore and interpret new combinations of data easily enough to make new connections and discoveries in a wide variety of scientific areas.<br/><br/><br/>This project will integrate visualization and analysis software for multiple synchrotron X-ray techniques into the open source and extensible Larch X-ray Analysis framework. The immediate focus of the work is to support visualization and quantitative analysis of the rich and complex data from X-ray microprobes, including X-ray fluorescence imaging, fluorescence and absorption spectroscopies, and X-ray diffraction. The Larch framework already provides a suite of analysis procedures for X-ray absorption spectroscopy and fluorescence imaging, and has been designed to be readily extensible by adding plug-ins in Python, widely used in scientific computing and being embraced in the synchrotron user communities. Existing state-of-the-art analysis procedures for X-ray fluorescence, X-ray absorption, and X-ray diffraction have been identified to be integrated into the Larch framework, adapting and translating the software as needed to be compatible with the open-source Python framework. With the combination of state-of-the-art analysis methods for multiple data types, Larch will provide a single well-supported and -documented analysis package with robust, easy to use analytic methods for a range of synchrotron X-ray data. By being easily extensible, the Larch package can also accommodate methods for other synchrotron X-ray techniques.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450468","fuchsia_atom","SI2-SSI Integration of Synchrotron X-Ray Analysis Software Methods into the Larch Framework"
"1440677","SI2-SSE: RADICAL Cybertools: Scalable, Interoperable and Sustainable Tools for Science","ACI","Software Institutes","1/1/2015","7/24/2014","Shantenu Jha","NJ","Rutgers University New Brunswick","Standard Grant","Rajiv Ramnath","12/31/2017","$499,916.00 ","","shantenu.jha@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","33 Knightsbridge Road, Piscataway, NJ","88543925","8489320150","CSE","8004","7433|8005","$0.00 ","To support science and engineering applications that are the basis of many societal and intellectual challenges in the 21st century, there is a need for comprehensive, balanced, and flexible distributed cyberinfrastructure (DCI). The process of designing and deploying such large-scale DCI, however, presents a critical and challenging research agenda. One specific challenge is to produce tools that provide a step change in the sophistication of problems that can be investigated using DCI, while being extensible, easy to deploy and use, as well as being compatible with a variety of other established tools. RADICAL Cybertools will meet these requirements by providing an abstractions-based suite of well-defined capabilities that are architected to support scalable, interoperable and sustainable science on a range of high-performance and distributed computing infrastructure. RADICAL Cybertools builds upon important theoretical advances, production-software-development best practices, and carefully-analyzed usage and programming models. RADICAL Cybertools is posed to play a role in grand-challenge problems, ranging from personalized medicine and health to understanding long-term global and regional climate. All software developed through the project will be open source and will be licensed under the MIT License (MIT). Version control on the SVN repository will be accessible via http://radical.rutgers.edu.<br/><br/>Existing and current utilization of RADICAL Cybertools is built upon preliminary research prototypes of RADICAL Cybertools. There is a significant difference, however, in the quality and capability required to support scalable end-usage science, compared to that of a research prototype. It is the aim of this project to bridge this gap between the ability to serve as a research prototype versus the challenges of supporting scalable end-usage science. This will be achieved by addressing existing limitations of usability, functionality, and scalability. We will do so by utilizing conceptual and theoretical advances in the understanding of distributed systems and middleware, resulting in a scalable architecture and robust design. We will employ advances in performance engineering, data-intensive methods, and cyberinfrastructure to deliver the next generation of RADICAL Cybertools. This project will take the existing research prototypes of RADICAL Cybertools to the next level towards becoming a hardened, extensible, and sustainable tool that will support a greater number of users, application types, and resource types.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440677","fuchsia_atom","SI2-SSE: RADICAL Cybertools: Scalable, Interoperable and Sustainable Tools for Science"
"1450273","SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Perfomance, Security, and Correctness Tools","ACI","Software Institutes","8/1/2015","7/20/2015","John Mellor-Crummey","TX","William Marsh Rice University","Standard Grant","Rajiv Ramnath","7/31/2020","$1,500,000.00 ","","johnmc@rice.edu","6100 MAIN ST","Houston","TX","6100 MAIN ST, Houston, TX","770051827","7133484820","CSE","8004","7433|8009","$0.00 ","Software has become indispensable to society, used by computational scientists for science and engineering, by analysts mining big data for value, and to connect society over the Internet. However, the properties of software systems for any of these purposes cannot be understood without accounting for code transformations applied by optimizing compilers used to compose algorithm and data structure templates, and libraries available only in binary form. To address this need, this project will overhaul, integrate, and enhance static binary analysis and runtime technologies to produce components that provide a foundation for performance, correctness, and security tools. The project will build upon three successful and widely adopted open source software packages: the DynInst library for analysis and transformation of application binaries, the MRNet infrastructure for control of large-scale parallel executions and data analysis of their results, and the HPCToolkit performance analysis tools. The project team will engage the community to participate in the design and evaluation of the emerging components, as well as to adopt its components. <br/><br/>This project will have a wide range of impacts. First, software components built by the project will enable the development of sophisticated, high-quality, end-user performance, correctness, and security tools built by the project team, as well as others in academia, government, and industry. Software developed by the project team will help researchers and developers tackle testing, debugging, monitoring, analysis, and tuning of applications for systems at all scales. Second, end-user tools produced by the project have a natural place in the classroom to help students write efficient, correct, and secure programs. Third, components produced by the project will lower the barrier for new researchers to enter the field and build tools that have impact on production applications without years of investment. Fourth, the project will provide training for graduate students and interns in the area of software for performance, correctness, and security. Finally, through workshops and tutorials, the project will disseminate project results, provide training to enable others to leverage project software, and grow a community of tool researchers who depend on project components and thus have a strong motivation to help sustain project software into the future.<br/><br/>Modernizing open-source software components and tools for binary analysis will enable static analysis of application characteristics at the level of executable machine code, transformation of binaries to inject monitoring code, measurement to capture a detailed record of application?s interactions with all facets of a target platform, analysis of recorded data in parallel, and attribution of analysis results back to application source code in meaningful ways. Providing innovative, software components that support development of robust performance, correctness, and security tools will accelerate innovation by tools researchers and help them grapple with the increasing complexity of modern software. Of particular note, helping tools researchers and computational scientists grapple with the challenges of software for modern parallel systems and producing training materials that help people use this software, addresses several of the needs identified in the NSF Vision for Cyberinfrastructure for the 21st Century.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450273","fuchsia_atom","SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Perfomance, Security, and Correctness Tools"
"1450195","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","PHYSICAL & DYNAMIC METEOROLOGY|Software Institutes|EarthCube","8/1/2015","8/4/2015","Robert Fovell","NY","SUNY at Albany","Standard Grant","Rajiv Ramnath","7/31/2018","$246,111.00 ","","rfovell@albany.edu","1400 WASHINGTON AVE","Albany","NY","1400 WASHINGTON AVE, Albany, NY","122220100","5184374550","CSE","1525|8004|8074","4444|7433|8009","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450195","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1440800","SI2-SSE: Synthesizing Self-Contained Scientific Software","ACI","Software Institutes|SPECIAL PROJECTS - CCF","10/1/2014","8/8/2014","Ashish Gehani","CA","SRI International","Standard Grant","Rajiv Ramnath","9/30/2017","$499,919.00 ","","ashish.gehani@sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","333 RAVENSWOOD AVE, Menlo Park, CA","940253493","6508592651","CSE","8004|2878","7433|8005|8004|2878","$0.00 ","Computational aspects of scientific experiments have been growing steadily. This creates an increasing need to be able to reproduce the results. Science is also increasingly performed by exploring diverse sets of data. Unsurprisingly, there is a demand for being able to easily repeat the numerous transformations performed. Software packaged with tools from this project will allow scientists to publish their code in a form that can be utilized by others with minimal effort. By eliminating many of the challenges of building, configuring, and running software, it will allow members of the scientific community to more easily reproduce each others' computational results.<br/><br/>Increasingly, entire virtual machines are published to ensure that a recipient does not have to replicate the compute environment, retrieve data and code dependencies, or invest effort into configuring the system. However, this approach scales poorly with the growth in size of the included data sets, the extraneous functionality in applications that utilize versatile software libraries, and the irrelevant code in stock operating system distributions. This project will design, develop, and evaluate a toolchain that allows scientists to transform their software into specialized applications with all the necessary environmental conditions and portions of required data sets built directly into the code. The resulting scientific appliances can be distributed for others to explore and verify results without the overhead of shipping extraneous data and code.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440800","fuchsia_atom","SI2-SSE: Synthesizing Self-Contained Scientific Software"
"1450440","SI2-SSI: Collaborative Research: A Software Infrastructure for MPI Performance Engineering: Integrating MVAPICH and TAU via the MPI Tools Interface","ACI","Software Institutes","9/1/2015","8/31/2015","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Patricia Knezek","8/31/2019","$1,200,000.00 ","","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","Office of Sponsored Programs, Columbus, OH","432101016","6146888735","CSE","8004","7433|8009","$0.00 ","Message-Passing Interface (MPI) continues to dominate the supercomputing landscape, being the primary parallel programming model of choice. A large variety of scientific applications in use today are based on MPI. On the current and next-generation High-End Computing (HEC) systems, it is essential to understand the interaction between time-critical applications and the underlying MPI implementations in order to better optimize them for both scalability and performance. Current users of HEC systems develop their applications with high-performance MPI implementations, but analyze and fine tune the behavior using standalone performance tools. Essentially, each software component views the other as a blackbox, with little sharing of information or access to capabilities that might be useful in optimization strategies. Lack of a standardized interface that allows interaction between the profiling tool and the MPI library has been a big impediment. The newly introduced MPI_T interface in the MPI-3 standard provides a simple mechanism that allows MPI implementers to expose variables representing configuration parameters or performance measurements from within the implementation for the benefit of tools, tuning frameworks, and other support libraries. However, few performance analysis and tuning tools take advantage of the MPI_T interface and none do so to dynamically optimize at execution time. This research and development effort aims to build a software infrastructure for MPI performance engineering using the new MPI_T interface.<br/><br/>With the adoption of MPI_T in the MPI standard, it is now possible to take positive steps to realize close interaction between and integration of MPI libraries and performance tools. This research, undertaken by a team of computer scientists from OSU and UO representing the open source MVAPICH and TAU projects, aims to create an open source integrated software infrastructure built on the MPI_T interface which defines the API for interaction and information interchange to enable fine grained performance optimizations for HPC applications. The challenges addressed by the project include: 1) enhancing existing support for MPI_T in MVAPICH to expose a richer set of performance and control variables; 2) redesigning TAU to take advantage of the new MPI_T variables exposed by MVAPICH; 3) extending and enhancing TAU and MVAPICH with the ability to generate recommendations and performance engineering reports; 4) proposing fundamental design changes to make MPI libraries like MVAPICH ``reconfigurable'' at runtime; and 5) adding support to MVAPICH and TAU for interactive performance engineering sessions. The framework will be validated on a variety of HPC benchmarks and applications. The integrated middleware and tools will be made publicly available to the community. The research will have a significant impact on enabling optimizations of HPC applications that have previously been difficult to provide. As a result, it will contribute to deriving ""best practice"" guidelines for running on next-generation Multi-Petaflop and Exascale systems. The research directions and their solutions will be used in the curriculum of the PIs to train undergraduate and graduate students.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450440","fuchsia_atom","SI2-SSI: Collaborative Research: A Software Infrastructure for MPI Performance Engineering: Integrating MVAPICH and TAU via the MPI Tools Interface"
"1450170","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","Software Institutes|PHYSICAL & DYNAMIC METEOROLOGY|EarthCube","8/1/2015","8/4/2015","William Capehart","SD","South Dakota School of Mines and Technology","Standard Grant","Rajiv Ramnath","7/31/2018","$183,956.00 ","","William.Capehart@sdsmt.edu","501 East Saint Joseph Street","Rapid City","SD","501 East Saint Joseph Street, Rapid City, SD","577013995","6053941218","CSE","8004|1525|8074","7433|8009|9150|4444","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450170","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1636501","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users","ACI","Software Institutes","10/1/2015","3/15/2016","Mark Fahey","IL","University of Chicago","Standard Grant","Rajiv Ramnath","9/30/2016","$46,194.00 ","","markrfahey@uchicago.edu","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004","7433|8005|9150","$0.00 ","This research addresses two important questions: what software do researchers actually use on high-end computers, and how successful are they in their efforts to use it? It is a plan to improve our understanding of individual users' software needs, then leverage that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The signature product, UTWrangler, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe. For the first time, complete, accurate, detailed, and continuous ground truth information about software needs, trends, and issues at the level of the individual job are being delivered.<br/><br/>UTWrangler will instrument, monitor, and analyze individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. UTWrangler's prototypes prove its value and future impact: simplifying end users' workflows; improving support, training and documentation; saving money; and helping administrators prioritize maintenance of their large base of installed software. UTWrangler will build on the capabilities of its prototypes, providing a robust, sustainable, second generation mechanism that will help the computational research community make the most effective use of limited computing cycles and labor hours. And UTWrangler will mitigate the difficulties new users encounter, reporting configuration problems as soon as jobs begin, and identifying opportunities to improve documentation, education and outreach programs.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1636501","fuchsia_atom","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users"
"1147522","SI2-SSE: A Unified Software Environment to Best Utilize Cache and Memory Systems on Multicores","ACI","INFORMATION TECHNOLOGY RESEARC|Software Institutes","6/1/2012","6/22/2012","Xiaodong Zhang","OH","Ohio State University","Standard Grant","Patricia Knezek","5/31/2017","$500,000.00 ","","zhang@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","Office of Sponsored Programs, Columbus, OH","432101016","6146888735","CSE","1640|8004","7942|8005|1640|8004","$0.00 ","This project further develops and maintains a set of system and application software to benefit application users of multicores by providing a unified software environment where multicore system utilities can be easily used as common functions in various applications. The project consists of the following three tasks. First, we further improve our cache-partitioning OS utility, and make efforts to add two other critical system utilities: (1) Multicore buffer cache management to prevent the shared cache from thrashing and pollution; (2) multicore-aware synchronization lock management to effectively make process assignments such that the co-running processes would minimize bandwidth consumption within a multicore chip and cross multiple multicore chips. Second, we continue our efforts to develop a software runtime library that enables programmers to explicitly manage and optimize the shared cache usage and memory accesses by allocating proper cache space and memory modules for different data sets of different processes. Finally, we provide a unified software environment for application users. With a set of easy interface functions, the users can access both middleware runtime library and the system utilities without a requirement of knowing architectural and system details. <br/><br/>The broader and transformative impact of the project can be significant: (1) Our software will provide effective and accessible solutions for significant performance improvement in multicores for a large scope of application community. (2) Gaining the insights into system interactions among applications, OS, and multicore architecture, we will provide valuable guidance for designs and implementations of application software. (3) The software is online with a maintenance for a public, wide, and sustained usage, which will directly impact open source software, and contribute to application users. (4) The research and software development of the project will train both undergraduate and graduate students for their future technical innovations in academia and industries.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147522","fuchsia_atom","SI2-SSE: A Unified Software Environment to Best Utilize Cache and Memory Systems on Multicores"
"1148144","SI2-SSE: Software Tools for Biomolecular Free Energy Calculations","ACI","Software Institutes|DMR SHORT TERM SUPPORT|MATERIALS AND SURFACE ENG|CHEMISTRY PROJECTS|OFFICE OF MULTIDISCIPLINARY AC","7/1/2012","7/6/2012","Celeste Sagui","NC","North Carolina State University","Standard Grant","Rajiv Ramnath","6/30/2016","$497,797.00 ","Christopher Roland","sagui@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","CAMPUS BOX 7514, RALEIGH, NC","276957514","9195152444","CSE","8004|1712|1633|1991|1253","8005|7237|7433|7569|7573|9215|9216|9263|1982|HPCC|1253|1633|1712|1991|8004","$0.00 ","TECHNICAL SUMMARY<br/><br/>The Office of Cyberinfrastructure, Division of Materials Research, Chemistry Division, and Division of Civil, Mechanical, and Manufacturing Innovation contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports the development of a new set of software tools for calculating biomolecular free energies, transition paths, and reaction rates that will be based on and will augment the PIs' ABMD/REMD/SMD suite of codes. New capabilities are to be added to the existing codes, which should greatly enhance the applicability of the software to challenging biomolecular problems. In particular, the PIs aim to implement and develop: (i) the capability for dealing with dynamically coupled multiple walkers or replicas for enhanced sampling; (ii) the introduction of Transition Path Theory and Sampling methodology for calculating reaction rates and related physical quantities; and (iii) the introduction of the ""string method"" for multi-dimensional free energy calculations and the calculation of the important minimum free energy path. These software tools will be released under the open source GPL, and released as part of the AMBER software package. The codes will also be released as stand-alone modules via the web for other users to integrate into their own programs.<br/><br/>To test and showcase the workings of the software package, the PIs will apply these methods to biomolecular systems where changes in handedness play an important role, including: proline-rich systems, mainly collagen and a new class of cell-penetrating peptides, and DNA and RNA in B and Z double helices. <br/><br/>This award will also enhance education through: the mentoring and recruitment of minority students, the development of a Biophysics option, the integration of research topics into current educational forums, working with local schools, and the development of a new Institute for Computational Science and Engineering.<br/><br/>NON-TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, Chemistry Division, and Division of Civil, Mechanical, and Manufacturing Innovation contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports the development of a new set of software tools that will enable the calculation of rates of chemical reactions involving biomolecules. The codes will also be able to calculate the energy that can be converted into useful work in a biochemical system. The codes will be distributed through a widely used software package called AMBER which contributes to the software cyberinfrastructure of computational chemistry, biochemistry, chemical engineering, materials, and biological physics communities. It will enable a wide range of computational research involving biological molecules. Among the possible applications are the study of the structure and transformations of biomolecules which provides information on their possible functions, the simulation of antibiotics, and simulating nanoscale technology applications. <br/><br/>This award will also enhance education through: the mentoring and recruitment of minority students, the development of a Biophysics option, the integration of research topics into current educational forums, working with local schools, and the development of a new Institute for Computational Science and Engineering.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148144","fuchsia_atom","SI2-SSE: Software Tools for Biomolecular Free Energy Calculations"
"1148424","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack","ACI","Software Institutes|INFORMATION TECHNOLOGY RESEARC","6/1/2012","6/4/2012","William Barth","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","5/31/2016","$449,995.00 ","Tommy Minyard","bbarth@tacc.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","101 E. 27th Street, Suite 5.300, Austin, TX","787121532","5124716424","CSE","8004|1640","8009|7433|1640|8004","$0.00 ","The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc. Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by<br/>system administrators, or by end-users. These default parameters may or may not be optimal for all system configurations and applications.<br/><br/>The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications. Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: ""Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?"" The investigators, involving computer<br/>scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.<br/><br/>The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time? 2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs? 3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface? 4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications? and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework? The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU. The proposed designs will be integrated into the open-source MVAPICH2 library.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148424","fuchsia_atom","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack"
"1148276","Collaborative Research SI2-SSE:Sustained Innovation in Acceleration of Molecular Dynamics on Future Computational Environments: Power to the People in the Cloud and on Accelerators","ACI","CHEMISTRY PROJECTS|DMR SHORT TERM SUPPORT|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","6/1/2012","6/24/2014","Ross Walker","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","5/31/2016","$320,833.00 ","","rcw@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","1991|1712|1253|8004","8005|7433|9216|9263|7237|7569|7573|7683","$0.00 ","This collaborative project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA, Intel and Amazon is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using next generation accelerator hardware including Intel's MIC system and Graphics Processing Units (GPU). It will extend support to include all major MD techniques and develop open source accelerated analysis libraries. A priority is enhanced sampling techniques including Thermodynamic Integration, constant pH algorithms, Multi-Dimensional Hamiltonian Replica Exchange and Metadynamics. These elements will then be combined, in collaboration with Amazon to support MD as-a-service through easily accessible web front ends to cloud services, including Amazon's EC2 GPU hardware. Transitioning large scale MD workflows from requiring access to large supercomputer hardware to being accessible to all on desktop and cloud resources provides the critical software infrastructure to support transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy.<br/><br/>The software elements created through this project have an extremely broad impact. The integration of comprehensive support for next generation hardware acceleration into the AMBER software alone benefits a very large user base. With over 10,000 downloads of the latest AMBER Tools package from unique IPs and >800 sites using the AMBER MD engines testify to the scope of the community of researchers this work impacts. The development of simple web based front ends for use of elastically scalable cloud resources makes simulations routine for all researchers. Meanwhile education and outreach efforts train the next generation of scientists not just in how to use the MD acceleration libraries and advanced MD simulation techniques developed here but also gets them thinking about how their approach can be transformed given that performance that was previously restricted to large scale supercomputers is now available on individual desktops.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148276","fuchsia_atom","Collaborative Research SI2-SSE:Sustained Innovation in Acceleration of Molecular Dynamics on Future Computational Environments: Power to the People in the Cloud and on Accelerators"
"1148523","SI2-SSI: Next-Generation Volunteer Computing","ACI","CI-TEAM|Software Institutes","4/15/2012","4/22/2015","David Anderson","CA","University of California-Berkeley","Standard Grant","Daniel Katz","9/30/2015","$1,100,000.00 ","","davea@ssl.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","CSE","7477|8004","7433|8004|8009","$0.00 ","By 2015, the global consumer digital infrastructure will include two resource pools of importance to scientific computing: a billion GPU-equipped desktop and laptop computers with a capability of roughly 100 ExaFLOPS, and 10 billion mobile devices with a capacity of about 20 ExaFLOPS and energy efficiency about 20 times better than larger devices. Volunteer computing is a proven approach by which these resources can be used for scientific computing. BOINC (Berkeley Open Infrastructure for Network Computing) is the dominant middleware for volunteer computing. This project will extend BOINC to create the technology for the next generation of volunteer computing, focusing on the following areas:<br/><br/>1) Computing on mobile devices: support for scientific computing on smart phones and other mobile devices based on Android;<br/><br/>2) Virtual-machine applications: support for applications that run in virtual machines. This reduces heterogeneity issues and, by providing a strong form of sandboxing, allows untrusted applications to be run on volunteer computers;<br/><br/>3) Multi-user projects: server-side features that allow the resources of a BOINC-based project to be shared fairly among many scientists. This will allow BOINC to be used effectively by science portals.<br/><br/>In addition we will add other new capabilities to BOINC, and will add support for new operating system versions, GPU types and models, and language systems such as OpenCL.<br/><br/>By providing scientists with access to huge and essentially free computing power, this will enable new research in fields such as nanotechnology, proteomics, genomics, climate modeling, epidemiology, cancer care, bio-fuels, battery technology for electric vehicles, earthquake engineering, volcanic activity, pharmaceutical engineering, microelectromechanical systems, and modeling of biological ecosystems, to name a few. In addition, by attracting millions of ?citizen scientists? who volunteer their computing resources, we will increase public awareness of and interest in science, and will expand the public outreach and education channel provided by volunteer computing",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148523","fuchsia_atom","SI2-SSI: Next-Generation Volunteer Computing"
"1148331","SI2-SSE: Interdisciplinary Software Infrastructure for Differential Geometry, Lie Theory and their Applications","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|Software Institutes|CDS&E-MSS","6/1/2012","5/29/2012","Ian Anderson","UT","Utah State University","Standard Grant","Rajiv Ramnath","5/31/2016","$360,845.00 ","Charles Torre","ian.anderson@usu.edu","Sponsored Programs Office","Logan","UT","Sponsored Programs Office, Logan, UT","843221415","4357971226","CSE","1253|7478|8004|8069","8005|9150|7433|7683|1253|7478|8004","$0.00 ","The goal of this proposal is to further develop symbolic software for the field of differential geometry and those areas of mathematics, physics and engineering where <br/>differential geometry plays an essential role. The proposed work will: provide new functionalities requested by the user community, complete packages currently under <br/>development, redesign critical components for improved computational efficiency, develop upgrades of existing algorithms and code whose performance does not support the demands of research. Specific objectives include [1] the development of a new coordinate-free computational environment for work with abstract differential forms and for tensor analysis on homogeneous spaces; [2] software for the structure theory of real/complex Lie algebras and their representations; [3] implementation of the theory of Young tableaux for tensors with symmetry to address resource and performance problems arising in large tensor computations; [4] new programs for symbolic computations for sub-manifold theory in Riemannian geometry, complex manifolds and Kahler geometry, and symplectic geometry; [5] a comprehensive new package for exterior differential systems; and [6] expansion of various data-bases of Lie algebras, differential equations, and exact solutions in general relativity.<br/><br/>Of all the core disciplines in mathematics, differential geometry is unique in that it interfaces with so many other subjects in pure mathematics, applied mathematics, physics, engineering, and even computer science. The PI's DifferentialGeometry (DG) software package has laid the foundation for a single, unified symbolic computational environment for research and teaching in differential geometry and its many application areas. The goal of this proposal is to add new computational environments to address specific application needs, to add basic functionalities that will bring various sub-packages to maturity, to upgrade routines with performance limitation, and significantly extend the DG data-bases of Lie algebras, group actions, integrable systems, and solutions of the Einstein equations. Earlier versions of this software have established a significant user community. Community feedback has dictated much of the specific program agenda in this proposal. A unique partnership between Utah State University and Maplesoft insures that the DG software meets the high standards of reliability, ease of use, documentation and support, and longevity that a extended user community (with diverse levels of symbolic computational experience) demands.<br/><br/>While originially designed as a research tool, DG also provides an innovative approach to teaching differential geometry and its applications in the classroom. All developments in DG are implemented with this in mind.<br/><br/>The PI will host a workshop at Utah State University entitled: Symbolic Methods in Differential Geometry, Lie Theory and Applications. This workshop will consist of <br/>hands-on training sessions, and lectures on applications of symbolic methods to problems in differential geometry. This workshop will also provide an ideal <br/>venue to survey participant research interests to drive future code development.<br/><br/>Student involvement at the undergraduate and graduate levels is an important component of this project. The experience gained in working with computer algebra systems in general, and differential geometry in particular, is valuable to the student for future educational activities and/or future employment.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148331","fuchsia_atom","SI2-SSE: Interdisciplinary Software Infrastructure for Differential Geometry, Lie Theory and their Applications"
"1148443","SI2-SSE: Enhancement and Support of Swift Parallel Scripting","ACI","Software Institutes","4/1/2012","4/16/2012","Michael Wilde","IL","University of Chicago","Standard Grant","Rajiv Ramnath","3/31/2015","$499,135.00 ","","wilde@mcs.anl.gov","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004","8005|8004","$0.00 ","The Swift parallel scripting language enables scientists, engineers, and data analysts to express and coordinate parallel invocations of application programs on distributed and parallel computing platforms: one of the dominant modes of performing computation in science and engineering. Swift runs on a variety of HPC clusters and clouds, and enables users to move their application scripts between computing environments with relative ease.<br/><br/>Swift comprises a programming model, scripting language, and runtime engine. Its implicitly parallel programming model allows users with minimal programming expertise to transparently utilize parallel and distributed systems. The scripting language is simple, minimal and standalone; the programming model has also been embedded into the Python and R languages.<br/><br/>Swift is employed in many diverse domains, including biochemistry, neuroscience, climate model analysis, earthquake simulation, hydrology, energy forecasting, economics modeling, mass media analysis, materials science, and astronomy.<br/><br/>This project makes usability and programmatic expressiveness enhancements to Swift, broadens its utility library, performs the testing and hardening needed to serve a large-scale national and global community, and extends existing documentation and training material in a manner that will create and serve a broad user base. It also develops enhancements in configuration, script debugging, exception handling, parallel collection processing, and data typing and mapping to make Swift increasingly productive.<br/><br/>Swift enables users with little or no experience in parallel programming to leverage parallel and distributed computing environments ranging in scale from desktops to petascale supercomputers. It opens up powerful cyberinfrastructure like XSEDE, Open Science Grid, FutureGrid, and Blue Waters to a wide range and scale of scientific user communities, thus broadening participation in high performance computing.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148443","fuchsia_atom","SI2-SSE: Enhancement and Support of Swift Parallel Scripting"
"1148188","Collaborative Research: SI2-SSI: Open Source Support for Massively Parallel, Generic Finite Element Methods","ACI","Software Institutes","8/1/2012","8/9/2012","Michael Heroux","MN","College of Saint Benedict","Standard Grant","Rajiv Ramnath","7/31/2017","$181,586.00 ","","mheroux@csbsju.edu","37 South College Avenue","Saint Joseph","MN","37 South College Avenue, Saint Joseph, MN","563742099","3203635690","CSE","8004","7433|8009|8004","$0.00 ","Partial differential equations are used in a wide variety of applications as<br/>mathematical models. Their numerical solution is, consequently, of prime<br/>importance for the accurate simulation and optimization of processes in the<br/>sciences, engineering, and beyond.<br/>The last decade saw the emergence of large and successful libraries that<br/>support such applications. While these libraries provide most of what such<br/>codes need for small-scale computations, many realistic applications yield<br/>problems of hundreds of millions or billions of unknowns and require clusters<br/>with thousands of processor cores, but there is currently little generic<br/>support for such problems, limiting access to the many large publicly<br/>supported computing facilities to experts in computational science and<br/>excluding scientists from many fields for whom computational simulation would<br/>be a useful tool. This project intends to build the software infrastructure that will allow a<br/>wide cross section of scientists to utilize these large resources.<br/><br/><br/>This project intends to support the software infrastructure for the<br/>large-scale solution of partial differential equations on massively parallel<br/>computational resources in a generic way. It will build on two of the most<br/>successful libraries for scientific computing, the finite element library<br/>deal.II, and Trilinos that provides the parallel linear algebra capabilities<br/>for the former. Specifically, we will: (i) Make support for massively parallel<br/>computations ubiquitous in deal.II; (ii) Research and develop seamless support<br/>for problems with billions of unknowns in both libraries and improve the<br/>interaction between the two; (iii) Exploit intra-node parallelism on today's<br/>clusters; (iv) Ensure the applicability of our work on a broad basis by<br/>implementing two real-world applications. <br/>Both deal.II and Trilinos have large, active and diverse developer and user<br/>communities, and this project will actively engage these communities through<br/>user meetings, short courses, regularly taught classes, mailing lists, and<br/>direct contact in focused projects.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148188","fuchsia_atom","Collaborative Research: SI2-SSI: Open Source Support for Massively Parallel, Generic Finite Element Methods"
"1102418","SI2-SSE: Adaptive Software for Quantum Chemistry","ACI","OFFICE OF MULTIDISCIPLINARY AC|PROJECTS|Software Institutes","10/1/2010","12/7/2010","So Hirata","IL","University of Illinois at Urbana-Champaign","Standard Grant","Daniel Katz","8/31/2014","$391,079.00 ","","sohirata@illinois.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","1253|1978|8004","","$0.00 ","The goal of this project is to establish a new paradigm of scientific software, electing quantum chemistry as the domain science. The new software does not have a static, compiled code, but instead consists of an expert system and code generator. At every execution, it analyzes the hardware and application parameters, determines(parallel) algorithms, and implements them for one-time use. This strategy not only allows unprecedented flexibility in algorithm optimization but can also realize ideas that are impossible otherwise. Since the approach makes no assumption about hardware or application, it is more extensible, maintainable, and portable. It is particularly well suited for chemistry, where a variety of molecules and reactions is infinite.<br/><br/>The expected long-term impact of this project is a change in the way scientific and engineering computing software is developed and defined. It promises novel software technology, which simultaneously achieves development efficiency, high product quality, and increased ability to optimize the code and enhance the methodological capabilities, by having no fixed source code. This project also offers unique, interdisciplinary education for chemistry graduate students, which places exceptionally large focus on computing, the field that has been a driving force of the 21st century economy.<br/><br/>This is an award within the solicitation of Software Infrastructure for Sustained Innovation. The award is co-funded by the Office of Cyberinfrastructure, the Division of Chemistry and the Office of Multidisciplinary Activities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1102418","fuchsia_atom","SI2-SSE: Adaptive Software for Quantum Chemistry"
"1147926","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack","ACI","INFORMATION TECHNOLOGY RESEARC|Software Institutes","6/1/2012","6/4/2012","Amitava Majumdar","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","5/31/2016","$450,772.00 ","","majumdar@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","1640|8004","7433|8009|1640|8004","$0.00 ","The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc. Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by<br/>system administrators, or by end-users. These default parameters may or may not be optimal for all system configurations and applications.<br/><br/>The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications. Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: ""Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?"" The investigators, involving computer<br/>scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.<br/><br/>The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time? 2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs? 3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface? 4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications? and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework? The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU. The proposed designs will be integrated into the open-source MVAPICH2 library.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147926","fuchsia_atom","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack"
"1147802","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|Software Institutes|CDS&E-MSS","6/1/2012","6/19/2012","William Stein","WA","University of Washington","Standard Grant","Daniel Katz","5/31/2015","$97,114.00 ","","wstein@math.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","4333 Brooklyn Ave NE, Seattle, WA","981950001","2065434043","CSE","1253|7478|8004|8069","7433|7683|8005|1253|5514|7478|8004","$0.00 ","Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is ""to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area"". There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s. The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thiery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.<br/><br/>The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147802","fuchsia_atom","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics"
"1148428","Collaborative Research: SI2-SSE: Correctness Verification Tools for Extreme Scale Hybrid Concurrency","ACI","Software Institutes","6/1/2012","6/19/2012","Rajeev Thakur","IL","University of Chicago","Standard Grant","Rudolf Eigenmann","5/31/2014","$44,346.00 ","","thakur@mcs.anl.gov","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004","8005|8004","$0.00 ","High Performance Computing is strategically important to national competitiveness. Advances in computational capabilities involve the use of unprecedented levels of parallelism: programming methods that involve billions of concurrent activities. Multiple styles of concurrency involving shared and distributed memory programming (?hybrid?) are necessary. Unfortunately, such programs are very difficult to debug using existing methods. This project develops formal (mathematically based) verification tools that can debug hybrid concurrent programs with very high certainty of bug elimination, while consuming only modest computational resources for verification. <br/><br/>The project develops execution-based tools that eliminate search over semantically equivalent alternative schedules as well as solver-based techniques that eliminate classes of bugs over single runs. Scalable methods based on non-determinism classification and heuristic execution-space reduction are also being developed. <br/><br/>Expected results include: (1) development of tools based on formal algorithmic techniques that verify large-scale hybrid programs; (2) amalgamation of incisive bug-hunting methods developed at other research organizations within formally based tools developed in our group; (3) incorporation of our verification tools and techniques within popular tool-integration frameworks; (4) large-scale case studies handled using our tools; and (5) training of undergraduate and graduate students on these advanced verification methods, building the talent pool vital to continued progress in high performance computing with applications to science and engineering, energy/sustainability, and homeland security.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148428","fuchsia_atom","Collaborative Research: SI2-SSE: Correctness Verification Tools for Extreme Scale Hybrid Concurrency"
"1535130","SI2-SSE: A Software Element for Neutrino Radiation Hydrodynamics in GenASiS","ACI","Software Institutes|COMPUTATIONAL PHYSICS|","9/1/2015","8/12/2015","Reuben Budiardja","TN","University of Tennessee Knoxville","Standard Grant","Rajiv Ramnath","8/31/2018","$432,488.00 ","Anthony Mezzacappa, Christian Cardall, Eirik Endeve","reubendb@utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","1 CIRCLE PARK, KNOXVILLE, TN","379960003","8659743466","CSE","8004|7244|1798","7433|8005|9150|8084|1206","$0.00 ","Multi-physics computational modeling is an integral part of the scientific study of many complex natural phenomena. These phenomena often involve the physics of radiation transport. For example, neutrino radiation hydrodynamics is a key element of the physics governing environments with hot and dense nuclear matter. Such extreme environments include the Early Universe, during primordial nucleosynthesis of light nuclei such as hydrogen through lithium just after the Big Bang; the merger through inspiraling of neutron star-neutron star or neutron star-black hole binaries; and the death throes of massive stars, more than ten times the mass of the Sun, in stellar explosions known as core-collapse supernovae, which are responsible for elements such as oxygen and calcium without which life as we know it would not exist. Radiation transport and kinetic theory of particles besides neutrinos---photons, electrons, or neutrons---are also relevant to many areas of astrophysics, as well as a broad range of other science applications, including materials science, plasma physics, neutron transport, multiphase flows, and high-energy-density physics. As such, the availability of a software element to solve radiation transport problems is highly valuable to researchers.<br/><br/>This project will create and deploy a software element to solve radiation hydrodynamics problems on modern supercomputers featuring ""hybrid"" architectures that include traditional CPUs plus ""accelerators"" or ""coprocessors,"" such as GPUs or Intel Many Integrated Core processors, respectively. This radiation hydrodynamics functionality will be developed within GenASiS (General Astrophysical Simulation System), a new framework being developed to facilitate the simulation of astrophysical phenomena on the world's leading capability supercomputers. In particular, the radiation transport solver will utilize the extant capabilities of GenASiS for adaptive computational ""mesh refinement,"" whereby the representation of the natural continuum is captured adaptively on a mesh of points foundational to any computational model in order to maximize the fidelity of the computational model for a given computational cost. We will use the so-called M1 approach, solving directly for the zeroth and first angular moments (energy density and momentum) of the radiation field, with higher-order moments given by ""closure relations,"" expressing them in terms of the zeroth and first moments. The energy dependence of the radiation field will be retained, with the zeroth and first angular moments discretized into ""energy bins."" Our computational approach to neutrino radiation transport will be an ""implicit-explicit"" (IMEX) scheme. Interactions between radiation and matter will be handled with a time-implicit subsolver, which will involve the inversion of dense matrices local to each node of the machine to exploit all available hardware in the node, including accelerators and coprocessors when available. Algorithms and software resulting from this project will be made available to the community. GenASiS, as an extensible, object-oriented simulation framework, will be valuable to researchers seeking to experiment with and implement different kinds of solvers for multi-physics problems. In particular, the neutrino hydrodynamics solver developed in this project is of high interest to astrophysics modelers.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535130","fuchsia_atom","SI2-SSE: A Software Element for Neutrino Radiation Hydrodynamics in GenASiS"
"1534836","SI2-SSE: TestRig 2.0","ACI","Software Institutes|Campus Cyberinfrastrc (CC-NIE)|SPECIAL PROJECTS - CCF","9/1/2015","6/29/2015","Christopher Rapier","PA","Carnegie-Mellon University","Standard Grant","Rajiv Ramnath","2/28/2017","$287,308.00 ","","rapier@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","5000 Forbes Avenue, PITTSBURGH, PA","152133815","4122689527","CSE","8004|8080|2878","7433|8005","$0.00 ","Advances in computation, modeling, and digital collection have led to an exponential growth in the size of scientific data sets. This 'Big Data' is transforming the process of scientific discovery. However, many scientific workflows require that the data be transferred, via networks like the Internet, to an optimal location for analysis and collaboration. Unfortunately, scientists often encounter performance problems on these networks. To resolve these problems, researchers depend on the expertise of engineers at Network Operations Centers (NOCs) for diagnosis and resolution. Resolving these problems often requires a cycle of interactions between the user and engineer that can last for days if not weeks, seriously hindering the scientific workflow. This project, known as TestRig 2.0, will short circuit this cycle by deploying a simple, easy to use system that will automatically collect a wide variety of important network diagnostic information. TestRig 2.0 will distribute dynamically generated test environments that are tuned to the specific needs of NOC engineers. TestRig 2.0 will automatically runs a series of tests against the infrastructure, collect the results, and transfer them back to the NOC engineer. TestRig 2.0 will also conducted a coarse analysis of the collected data so as to give the engineers some initial insight into the collected metrics.<br/><br/>In this project, researchers at Pittsburgh Supercomputing Center (PSC) propose to build a system, TestRig 2.0, that will quickly and easily gather a wide range of important network diagnostic information. This system includes a centralized management server that dynamically creates and configures client optical disk image files (as defined by ISO 9660). This server will be at PSC; it will provide TestRig 2.0 services to multiple geographically diverse NOCs via a free subscription model. These bootable images will provide a known good TCP stack configuration and system environment. TestRig 2.0 will use this baseline environment to perform tests against the perfSONAR monitoring infrastructure and also collect other pertinent data. The resulting data will undergo an initial automated coarse analysis by the client in order to provide engineers initial insight into the data. The resulting analysis and raw data will then be packaged and transferred back to the appropriate NOC for analysis. The client ISO creation process will include a unique dynamically generated public/private key pair. This key pair will be used for the authentication of the client ISO, distribution management, and user/test management. The rapid collection of pertinent data will enhance the process of scientific discovery by allowing researchers to focus on their research, instead of running tests for network engineers. By providing a framework for the use and development of post-collection data analysis, the system will lower the bar required for effective network diagnostics across a wide range of support personnel.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1534836","fuchsia_atom","SI2-SSE: TestRig 2.0"
"1535065","SI2-SSE: Collaborative Research: An Intelligent and Adaptive Parallel CPU/GPU Co-Processing Software Library for Aaccelerating Reactive-Flow Simulations","ACI","COMBUSTION|FIRE|& PLASMA SYS|Software Institutes","9/1/2015","5/10/2016","Kyle Niemeyer","OR","Oregon State University","Standard Grant","Rajiv Ramnath","8/31/2018","$285,487.00 ","","kyle.niemeyer@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","OREGON STATE UNIVERSITY, Corvallis, OR","973318507","5417374933","CSE","1407|8004","148E|7433|8004|8005|9251","$0.00 ","In order to develop the next generation of clean and efficient vehicle engines and power-generating combustors, engineers need the next generation of computational modeling tools. Accurately describing the chemistry of conventional and alternative liquid transportation fuels is vital to predict harmful emission levels and other important quantities, but the high computational cost of detailed models for chemistry poses a significant barrier to use by designers. In order to use such accurate models, software is needed that can efficiently handle chemistry in practical simulations. This collaborative project aims to develop such tools, employing the computational power of modern parallelized central processing units (CPUs) and graphics processing units (GPUs). In addition to helping designers create clean and efficient engine technology, the advances made in this project are widely applicable to other computational modeling problems including astrophysics, nuclear reactions, atmospheric chemistry, biochemical networks, and even cardiac electrophysiology.<br/><br/>The objective of the proposed effort is to develop software elements specifically targeted at co-processing on GPUs, CPUs, and other many-core accelerator devices to reduce the computational cost of using detailed chemistry and enable high-fidelity yet affordable reactive-flow simulations. This will be achieved by (1) developing and comparing chemical kinetics integration algorithms for parallel operation on CPUs and GPUs/accelerators, (2) developing a method for detecting local stiffness due to chemical kinetics and adaptively selecting the most efficient solver based on available hardware, (3) implementing a computational cell clustering strategy to group similar spatial locations, (4) demonstrating the improved performance offered by these software elements using commercial and open-source computational fluid dynamics codes for modeling reactive flows, and (5) designing a portable and sustainable software library based on the above software elements, including building a community of users. The result of this program will be an open source software library that significantly decreases the cost of using detailed, accurate chemistry in realistic combustion simulations; the success of the program will be determined based on achieving order-of-magnitude performance improvement or better.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535065","fuchsia_atom","SI2-SSE: Collaborative Research: An Intelligent and Adaptive Parallel CPU/GPU Co-Processing Software Library for Aaccelerating Reactive-Flow Simulations"
"1562450","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis","ACI","Software Institutes|CDS&E","7/1/2015","9/21/2015","Alison Marsden","CA","Stanford University","Standard Grant","Rajiv Ramnath","9/30/2017","$812,810.00 ","","amarsden@ucsd.edu","3160 Porter Drive","Palo Alto","CA","3160 Porter Drive, Palo Alto, CA","943041212","6507232300","CSE","8004|8084","7433|8009|8084","$0.00 ","The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. <br/><br/>This project is addressing the above roadblocks through the following aims: 1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community. Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1562450","fuchsia_atom","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis"
"1216747","Collaborative Research - SI2-S2I2: High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP)","ACI","OFFICE OF MULTIDISCIPLINARY AC|COMBUSTION|FIRE|& PLASMA SYS|PARTICULATE &MULTIPHASE PROCES|FLUID DYNAMICS|PHYSICS AT THE INFO FRONTIER|ICER|Software Institutes","9/1/2012","8/28/2012","John Mellor-Crummey","TX","William Marsh Rice University","Standard Grant","Daniel Katz","8/31/2014","$62,035.00 ","Vivek Sarkar","johnmc@rice.edu","6100 MAIN ST","Houston","TX","6100 MAIN ST, Houston, TX","770051827","7133484820","CSE","1253|1407|1415|1443|7553|7699|8004","056E|058E|7433|7483|8211|9216|9263|1253|1407|1415|1443|7553|7699|8004","$0.00 ","The starting point for this proposal is a view of scientific simulation articulated in the conclusions of the 2008 National Academy of Sciences Study, The Potential Impact of High-End Capability Computing on Four Illustrative Fields of Science and Engineering: ""Advanced computational science and engineering is a complex enterprise that requires models, algorithms, software, hardware, facilities, education and training, and a community of researchers attuned to its special needs."" (p. 122)<br/><br/>Over the last few years, the design of computer and software systems, particularly as they relate to simulation in the physical sciences, has been organized around a collection of algorithmic patterns / motifs. These patterns have been very productive because they are a natural ""common language"" in which application scientists can express their computations, and for which computer scientists can provide optimized libraries, domain specific languages, compilers, and other software tools.<br/><br/>This project will design an institute focused on a subset of these patterns --- structured grid discretizations of partial differential equations and particle methods, along with the linear and nonlinear solvers that enable their effective use --- with the specific goals of providing simulation capabilities for a set of scientific domains that make heavy use of these patterns. Two major components are envisioned to this proposed institute, called the Institute for High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP). The first component is a software infrastructure development activity that will be performed by a team whose expertise spans the design and development of mathematical algorithms and software frameworks, as well as the design and development of compilers, runtime systems, and tools that enable one to obtain high performance from emerging multicore and heterogeneous architectures. The second component is an outreach activity, in which algorithms, libraries, and software frameworks developed by the institute will be customized and integrated into simulation codes for stakeholder application domains. At the heart of this activity will be collaborations and partnerships, in which the institute will provide one or more software developers to collaborate with application scientists over a period of months to years to develop a new simulation capability or enhance an existing one.<br/><br/>The design of this institute will be carried out through a series of workshops, each focused on one of five stakeholder science domains that have been identified as using these motifs and that play a central role in various NSF Grand Challenge problems, with participation of both representatives of the science domain and the the relevant mathematics and computer science communities. In addition, there will be a final workshop that will bring together the relevant mathematics and computer science experts to identify cross-cutting themes. These information obtained from these workshops will be used by the project to develop the final conceptual design of the institute, in the form of a document that includes the input from all of the workshops and our analysis of how this leads to a design of a software institute.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1216747","fuchsia_atom","Collaborative Research - SI2-S2I2: High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP)"
"1535086","SI2-SSE: Human- and Machine-Intelligent Software Elements for Cost-Effective Scientific Data Digitization","ACI","Software Institutes|ADVANCES IN BIO INFORMATICS","8/1/2015","4/22/2016","Andrea Matsunaga","FL","University of Florida","Standard Grant","Rajiv Ramnath","7/31/2018","$488,048.00 ","Mauricio Tsugawa","ammatsun@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","1 UNIVERSITY OF FLORIDA, GAINESVILLE, FL","326112002","3523923516","CSE","8004|1165","7433|8005","$0.00 ","In the era of data-intensive scientific discovery, Big Data scientists in all communities spend the majority of their time and effort collecting, integrating, curating, transforming, and assessing quality before actually performing discovery analysis. Some endeavors may even start from information not being available and accessible in digital form, and when it is available, it is often in non-structured form, not compatible with analytics tools that require structured and uniformly-formatted data. Two main methods to deal with the volume and variety of data as well as to accelerate the rate of digitization have been to apply crowdsourcing or machine-learning solutions. However, very little has been done to simultaneously take advantage of both types of solutions, and to make it easier for different efforts to share and reuse developed software elements. The vision of the Human- and Machine-Intelligent Network (HuMaIN) project is to accelerate scientific data digitization through fundamental advances in the integration and mutual cooperation between human and machine processing in order to handle practical hurdles and bottlenecks present in scientific data digitization. Even though HuMaIN concentrates on digitization tasks faced by the biodiversity community, the software elements being developed are generic in nature, and expected to be applicable to other scientific domains (e.g., exploring the surface of the moon for craters require the same type of crowdsourcing tool as finding words in text, and the same questions of whether machine-learning tools could provide similar results can be tested).<br/><br/>The HuMaIN project proposes to conduct research and develop the following software elements: (a) configurable Machine-Learning applications for scientific data digitization (e.g., Optical Character Recognition and Natural Language Processing), which will be made automatically available as RESTful services for increasing the ability of HuMaIN software elements to interoperate with other elements while decreasing the software development time via a new application specification language; (b) workflows leading to a cyber-human coordination system that will take advantage of feedback loops (e.g., based on consensus of crowdsourced data and its quality) for self-adaptation to changes and increased sustainability of the overall system, (c) new crowdsourcing micro-tasks with ability of being reusable for a variety of scenarios and containing user activity sensors for studying time-effective user interfaces, and (d) services to support automated creation and configuration of crowdsourcing workflows on demand to fit the needs of individual groups. A cloud-based system will be deployed to provide the necessary execution environment with traceability of service executions involved in cyber-human workflows, and cost-effectiveness analysis of all the software elements developed in this project will provide assessment and evaluation of long standing what-if scenarios pertaining human- and machine-intelligent tasks. Crowdsourcing activities will attract a wide range of users with tasks that require low expertise, and at the same time it will expose volunteers to applied science and engineering, potentially attracting interest of K-12 teachers and students.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535086","fuchsia_atom","SI2-SSE: Human- and Machine-Intelligent Software Elements for Cost-Effective Scientific Data Digitization"
"1339893","SI2-SSI: Particle-In-Cell and Kinetic Simulation Center","ACI","Software Institutes|PHYSICS AT THE INFO FRONTIER|OFFICE OF MULTIDISCIPLINARY AC","9/1/2013","8/8/2016","Warren Mori","CA","University of California-Los Angeles","Continuing grant","Rajiv Ramnath","8/31/2018","$3,800,000.00 ","Russel Caflisch, Viktor Decyk, Frank Tsung, Michail Tzoufras","mori@physics.ucla.edu","11000 Kinross Avenue, Suite 211","LOS ANGELES","CA","11000 Kinross Avenue, Suite 211, LOS ANGELES, CA","900952000","3107940102","CSE","8004|7553|1253","7433|7483|8009|8084","$0.00 ","Computer simulations using the particle-in-cell (PIC) method are widely used for basic and applied research involving plasma physics applications. For example, simulations that calculate the self-consistent interaction of charged particles aid in the development of new accelerator technologies, new radiation sources, are used in magnetic and inertial fusion research, and help understand the physics of the solar wind. The Particle-in-Cell and Kinetic Simulation Software Center (PICKSC) at UCLA will aim to significantly broaden the impact of PIC simulations by making available and documenting illustrative software programs for different computing hardware, a flexible Framework for rapid construction of parallelized PIC programs, and several distinct production programs. This project will also include activities on developing and comparing different PIC algorithms and documenting best practices for developing and using PIC programs. The activities fostered by this project will bring together an interdisciplinary team of faculty, research staff, post-doctoral scholars, and graduate students. Important goals of this project include also the development of educational software for undergraduate and graduate courses in plasma physics and computer science and will, to build a large community of users through outreach and an annual workshop. <br/><br/>The broader impact of the activities fostered by this project will be significant. A well-documented set of components and example codes for running on large computers and a set of basic production codes will allow students and researchers from all levels and many disciplines to understand the inner workings of optimized PIC and kinetic simulation software used to model plasmas. It will allow them to build their own software, or to make independent comparisons against commercially available codes, against their own codes, and against published simulation data. The availability of production codes to more researchers will increase the rate of scientific discovery. The software will allow computer scientists who are developing tools that allow existing software to use next generation hardware to compare their performances against highly optimized codes, and will provide new code developers a test-bed of parallelized and optimized software for performance comparison. Furthermore, this projet will make state-of-the-art research software available for education, both for physics and computer science courses, will help train the next generation of plasma physicists (in many sub disciplines) and computational scientists. Interactive tools based on simpler skeleton codes will also be useful for undergraduate and high school education. Documenting examples of best practices will save graduate students and new researchers significant time in learning how to best employ PIC simulations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339893","fuchsia_tree","SI2-SSI: Particle-In-Cell and Kinetic Simulation Center"
"1450088","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC","8/1/2015","6/24/2015","Lyudmila Slipchenko","IN","Purdue University","Standard Grant","Rajiv Ramnath","7/31/2019","$600,000.00 ","","lslipchenko@purdue.edu","Young Hall","West Lafayette","IN","Young Hall, West Lafayette, IN","479072114","7654941055","CSE","8004|1253","7433|8009","$0.00 ","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450088","fuchsia_atom","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science"
"1550463","SI2-SSI: Advancing and Mobilizing Citizen Science Data through an Integrated Sustainable Cyber-Infrastructure","ACI","AISL|Software Institutes","11/1/2016","8/3/2016","Gregory Newman","CO","Colorado State University","Standard Grant","Rajiv Ramnath","10/31/2020","$1,000,000.00 ","Louis Liebenberg, Stacy Lynn, Melinda Laituri","Gregory.Newman@ColoState.Edu","601 S Howes St","Fort Collins","CO","601 S Howes St, Fort Collins, CO","805232002","9704916355","CSE","7259|8004","043Z|7433|8009|8004","$0.00 ","Citizen science engages members of the public in science. It advances the progress of science by involving more people and embracing new ideas. Recent projects use software and apps to do science more efficiently. However, existing citizen science software and databases are ad hoc, non-interoperable, non-standardized, and isolated, resulting in data and software siloes that hamper scientific advancement. This project will develop new software and integrate existing software, apps, and data for citizen science - allowing expanded discovery, appraisal, exploration, visualization, analysis, and reuse of software and data. Over the three phases, the software of two platforms, CitSci.org and CyberTracker, will be integrated and new software will be built to integrate and share additional software and data. The project will: (1) broaden the inclusivity, accessibility, and reach of citizen science; (2) elevate the value and rigor of citizen science data; (3) improve interoperability, usability, scalability and sustainability of citizen science software and data; and (4) mobilize data to allow cross-disciplinary research and meta-analyses. These outcomes benefit society by making citizen science projects such as those that monitor disease outbreaks, collect biodiversity data, monitor street potholes, track climate change, and any number of other possible topics more possible, efficient, and impactful through shared software. <br/><br/>The project will develop a cyber-enabled Framework for Advancing Buildable and Reusable Infrastructures for Citizen Science (Cyber-FABRICS) to elevate the reach and complexity of citizen science while adding value by mobilizing well-documented data to advance scientific research, meta-analyses, and decision support. Over the three phases of the project, the software of two platforms, CitSci.org and CyberTracker, will be integrated by developing APIs and reusable software libraries for these and other platforms to use to integrate and share data and software. Using participatory design and agile methods over four years, the project will: (1) broaden the inclusivity, accessibility, and reach of citizen science; (2) elevate the value and rigor of citizen science software and data; (3) improve interoperability, usability, scalability and sustainability of citizen science software and data; and (4) mobilize data to allow cross-disciplinary research and meta-analyses. These outcomes benefit society by making citizen science projects and any number of other possible topics more possible, efficient, and impactful through shared software and data. Adoption of Cyber-FABRICS infrastructure, software, and services will allow anyone with an Internet or cellular connection, including those in remote, underserved, and international communities, to contribute to research and monitoring, either independently or as a team. This project is also being supported by the Advancing Informal STEM Learning (AISL) program, which seeks to advance new approaches to, and evidence-based understanding of, the design and development of STEM learning in informal environments.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550463","fuchsia_atom","SI2-SSI: Advancing and Mobilizing Citizen Science Data through an Integrated Sustainable Cyber-Infrastructure"
"1534949","SI2-SSE: Open OnDemand: Transforming Computational Science through Omnidisciplinary Software Cyberinfrastructure","ACI","SPECIAL PROJECTS - CCF|Software Institutes","7/1/2015","8/25/2015","David Hudak","OH","Ohio State University","Standard Grant","Vipin Chaudhary","6/30/2018","$500,000.00 ","Douglas Johnson","dhudak@osc.edu","Office of Sponsored Programs","Columbus","OH","Office of Sponsored Programs, Columbus, OH","432101016","6146888735","CSE","2878|8004","7433|8005","$0.00 ","Supercomputing, or High-Performance Computing (HPC), has the power to advance work in all fields of science and engineering. Unfortunately, the impact of HPC is often limited simply because the computers themselves are difficult to use and scientists and engineers would rather spend their time advancing their disciplines than learn HPC. Learning esoteric commands is a hurdle to many students and researchers when they first begin to work with traditional HPC systems, which has contributed to the relatively small proportions of women, minorities, and persons with disabilities within related STEM fields. The Open OnDemand project addresses this problem through innovative software that makes HPC no more difficult than using a desktop computer or a web site, hence reducing that initial learning curve. Open OnDemand will provide an enhanced infrastructure for research and education, in that students and educators will be exposed to the same tools and techniques on smaller departmental and classroom systems that they'll eventually utilize at larger HPC centers. This streamlines the pipeline for developing a more globally competitive STEM workforce that is prepared to dive right into computational problems and resources once they graduate from academia. Open OnDemand also allows scientists and engineers to make specialized domain-specific workflows and expertise available to more collaborators and users, both from academia and industry. This should result in increased partnerships, and the transfer of more technologies from the public to private sectors.<br/><br/>The web has become the dominant access mechanism for remote compute services in every computing area except high-performance computing (HPC). Accessing HPC cyberinfrastructure (CI) resources today, either at the campus or national level, typically requires advanced knowledge of UNIX, familiarity with command-line interfaces and installation and configuration of custom client software. Web applications in HPC today do exist in the form of science gateways. However, gateways have not proliferated in part due to the development and administrative overheads required for each individual gateway. These factors demonstrate an accessibility gap for HPC. Open OnDemand is an open source platform for HPC and remote computing access that addresses the accessibility gap. Open OnDemand will be a public release based on the successful OSC OnDemand platform. OSC OnDemand is a web platform providing Ohio Supercomputer Center (OSC) users integrated access to HPC systems, web applications, and VNC services. OSC OnDemand has been in production since January 2013, has over 800 distinct users from 27 different NSF fields of science, and its apps have been launched over 70,000 times. In addition to easing access to HPC services, Open OnDemand centralizes web app overheads, easing support for custom visualization and science gateway apps. Open OnDemand has the ability to transform cyberinfrastructure by providing a platform to enable a new delivery method for scientific web tools like HubZero apps, XSEDE Science Gateways, iPython notebooks and workflow tools like Pegasus. <br/><br/>Under this project, the Open OnDemand platform will be created by (a) transitioning the existing OSC OnDemand software to a community developed project hosted on GitHub, (b) extending the per-user web server to serve Rails apps, (c) replacing the custom-developed Proxy with an existing open source project like NGINX and (d) replacing the existing Java-based VNC client with an HTML5 solution. File usage will be improved by updating or replacing AjaXplorer and integrating high performance file transfer functions (sftp and Globus Online). The accessibly apps will be updated by by (a) upgrading or replacing AnyTerm, (b) updating Job Constructor and (c) integrating Open XDMoD and SUPREMM for job and cluster performance metrics. In addition, community infrastructure will be created including (a) system administrator documentation and discussion forums and (b) training materials based on existing OSC OnDemand materials. A beta program will be conducted including (a) assisting beta sites with installs, (b) updating training materials to include new functionality, (c) beginning metric reporting and (d) documenting a Galaxy case study for app integration. HPC center staff will be engaged by through the ""boot camps"" and ""train the trainer"" sessions. Finally, Open OnDemand will be proposed as a project to the XSEDE Campus Bridging Technology Insertion Service. These activities will help meet the following objectives: (1) Transition OSC OnDemand to a community-developed open source software package called Open OnDemand, (2) Improve the interface capabilities of Open OnDemand by updating and expanding the accessibility apps, including integration of Globus Online and Open XDMoD projects. (3) Conduct a program to engage departmental, campus and national HPC users and administrators on enhancing HPC inclusivity through Open OnDemand and (4) Leverage Open OnDemand as a platform to support existing web-based applications such XSEDE Science Gateways and HubZero applications based on our experience supporting a data-intensive biomedical package (the Galaxy Project).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1534949","fuchsia_atom","SI2-SSE: Open OnDemand: Transforming Computational Science through Omnidisciplinary Software Cyberinfrastructure"
"1550526","Collaborative Research: SI2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure","ACI","Software Institutes","8/1/2016","8/2/2016","Michael Zentner","IN","Purdue University","Standard Grant","Rajiv Ramnath","7/31/2017","$107,269.00 ","","mzentner@purdue.edu","Young Hall","West Lafayette","IN","Young Hall, West Lafayette, IN","479072114","7654941055","CSE","8004","7433|8004|8009","$0.00 ","The aggregate computing power of consumer devices - desktop and laptop computers, tablets, smartphones - far exceeds that of institutional computing resources. ""Volunteer computing"" uses these consumer devices, volunteered by their owners, to do scientific computing. In addition to providing additional, much-needed computational resources to scientists, volunteer computing publicizes scientific research and engages citizens in science. BOINC is the primary software system for volunteer computing. It was developed at UC Berkeley with NSF support starting in 2002. Until now, BOINC has been based on a model of independent competing projects. Scientists set up their own BOINC servers, port their applications to run on BOINC, and publicize their projects to attract volunteers. There are about 40 such projects, in many areas of science: examples include Einstein@home, CERN, and SETI@home (astrophysics), Rosetta@home and GPUGrid.net (biomedicine), Climateprediction.net (climate study), and IBM World Community Grid (multiple applications). Together these projects have about 400,000 active volunteers and 12 PetaFLOPS of computing throughput. This model, while successful to an extent, has reached a limit. The number of projects and volunteers has stagnated. Volunteer computing is supplying lots of computing power, but only to a few research projects. For other scientists, there are two major barriers. First, creating a BOINC project has significant overhead: learning a new technology, creating a public web site, generating publicity, and so on. Second, volunteer computing is risky and uncertain; there is no guarantee that a new project will attract volunteers. This project aims to break this barrier, and to make volunteer computing available to all scientists doing high-throughput computing, by replacing the competing-projects model with a new ""central broker"" model. The new model has two related parts: 1) the integration of BOINC with existing high-throughput computing facilities such as supercomputing centers and science portals. Jobs currently run on cluster nodes will be transparently offloaded to volunteer computers. Scientists using these facilities will see faster turnaround times; they'll benefit from volunteer computing without even knowing it's there. 2) The project will change the volunteer interface so that participants sign up for scientific areas and goals rather then for particular projects. For example, a participant might sign up to contribute to cancer research. A central broker, to be developed as part of this project, would dynamically assign their computing resources to projects doing that type of research. This project mobilizes public support for and interest in scientific research by encouraging ""volunteer computing"" and engaging citizens in the conduct of the research itself. It simultaneously advances NSF's mission to advance science while broadening citizen engagement.<br/><br/>The first year of this project will prototype each of these parts, and will integrate BOINC with TACC and nanoHub. Integrating BOINC with existing HTC systems involves several subtasks: 1) Job routing: modifying existing job processing systems used by TACC and nanoHub (Launcher and Rappture respectively) to decide when a group of jobs should be offloaded to BOINC. This decision might involve the estimated runtime of the jobs, input and output file sizes, data sensitivity, the deadline or priority of the jobs, and the identity of the job submitter. 2) Job format conversion: mapping job descriptions (input/output file specifications, resource and timing requirements) to their BOINC equivalents. 3) Application packaging: adapting existing applications (such as nanoHub's simulation tools and TACC's Autodock) to run under BOINC. We will use BOINC's virtual machine facility, which packages an application as a virtual machine image (VirtualBox or Docker) and a program to be run within the VM. This allows existing Linux applications to run on consumer desktop platforms such as Windows and Mac, as well as providing a strong security sandbox and an efficient application-independent checkpoint/restart mechanism. 4) File handling: moving input and output files between existing storage systems (typically inaccessible from outside firewalls) to Internet-visible servers. This will use existing BOINC components that manage files based on hashes to eliminate duplicate transfer and storage of files. 5) Job monitoring and control: adapting existing web- or command-line based tools for monitoring the progress of batches of jobs, and for aborting jobs, to work with BOINC. This will use existing Web RPCs provided by BOINC for these purposes. This project will carry out these tasks by designing and implementing new software as needed, testing for correctness, performance, and scalability, and deploying it in a production environment. The second part of the project - a brokering system for allocating computing power based on volunteer scientific preferences - will be designed and prototyped. This involves several subtasks: 1) Designing a schema for volunteer preferences, including scientific areas and sub-areas, project nationality and institutions, specific projects and applications, inclusions/exclusions, and so on. 2) Designing a schema for assigning attributes to job streams (e.g. their area, sub-area, institution, etc.), and for assigning quotas or priorities to job streams. 3) Designing a relational database for storing the above information. 4) Designing and implementing policies for assigning volunteer resources to job streams in a way that respects volunteer preferences and optimizes quota, fairness, and throughput criteria. This will be implemented as a BOINC ""account manager"" so that volunteers see a single interface rather than lots of separate projects and web sites.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550526","fuchsia_atom","Collaborative Research: SI2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure"
"1550471","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","ACI","Software Institutes","8/1/2016","7/21/2016","Garnet Chan","NJ","Princeton University","Standard Grant","Rajiv Ramnath","7/31/2020","$600,000.00 ","","garnetc@caltech.edu","Off. of Research & Proj. Admin.","Princeton","NJ","Off. of Research & Proj. Admin., Princeton, NJ","85442020","6092583090","CSE","8004","7433|8009|9216","$0.00 ","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products. This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550471","fuchsia_atom","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids"
"1535108","SI2-SSE: Analyze Visual Data from Worldwide Network Cameras","ACI","Software Institutes","8/1/2015","2/3/2016","Yung-Hsiang Lu","IN","Purdue University","Standard Grant","Rajiv Ramnath","7/31/2018","$515,989.00 ","","yunglu@purdue.edu","Young Hall","West Lafayette","IN","Young Hall, West Lafayette, IN","479072114","7654941055","CSE","8004","7433|8005|9251","$0.00 ","Many network cameras have been deployed for a wide range of purposes, such as monitoring traffic, evaluating air pollution, observing wildlife, and watching landmarks. The data from these cameras can provide rich information about the natural environment and human activities. To extract valuable information from this network of cameras, complex computer programs are needed to retrieve data from the geographically distributed cameras and to analyze the data. This project creates a open source software infrastructure by solving many problems common to different types of analysis programs. By using this infrastructure, researchers can focus on scientific discovery, not writing computer programs. This project can improve efficiency and thus reduce the cost for running programs analyzing large amounts of data. This infrastructure promotes education because students can obtain an instantaneous view of the network cameras and use the visual information to understand the world. Better understanding of the world may encourage innovative solutions for many pressing issues, such as better urban planning and lower air pollution. This project can enhance diversity through multiple established programs that encourage underrepresented minorities to pursue careers in science and engineering. <br/><br/>This project will combine: (1) the ability to retrieve data from many heterogeneous and distributed cameras, (2) the management of computational and storage resources using cloud computing, and (3) improved performance by reducing data movement, balancing loads among multiple cloud instances, and enhancing data-level parallelism. The project provides an application programming interface (API) that hides the underlying sophisticated infrastructure. This infrastructure will handle both real-time streaming data and archival data in a uniform way, so that the same analysis programs can be reused. This project has four major components: (1) a web-based user interface, (2) a database that stores the details about the network cameras, (3) a resource manager that allocates cloud instances, and (4) a computational engine that execute the programs written by users. The service-oriented architecture will allow new functions to be integrated more easily by the research community.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535108","fuchsia_atom","SI2-SSE: Analyze Visual Data from Worldwide Network Cameras"
"1265278","SI2-SSE: General Tensor Software Elements for Quantum Chemistry, Tensor Network Theories, and Beyond","ACI","OFFICE OF MULTIDISCIPLINARY AC|CHEMISTRY PROJECTS|Software Institutes","6/26/2012","9/24/2012","Garnet Chan","NJ","Princeton University","Standard Grant","Daniel Katz","5/31/2015","$386,818.00 ","","garnetc@caltech.edu","Off. of Research & Proj. Admin.","Princeton","NJ","Off. of Research & Proj. Admin., Princeton, NJ","85442020","6092583090","CSE","1253|1991|8004","7433|7683|8005|9216|9263|1253|1991|8004","$0.00 ","The PI will develop reusable software components to accelerate innovation in science that relies on high-dimensional tensor computations. While the components are informed by use-cases taken from quantum chemistry, the challenges of tensor computation are universal, and span diverse areas of science and engineering. These problems range from simulations of nuclear spin spectra, to quantum chemical calculations on molecules, to psychometric analysis, to numerical general relativity. The overarching aim of the funded work is to develop reusable tensor software elements, based on modern sustainable software practices, to benefit tensor algorithmic development in the scientific community at large. In addition, beyond the broad scientific impacts of the software, our education and outreach agenda comprises a multi-tiered effort to uplift the ability of the science and engineering community to reason about high-dimensional tensor and matrix computations.<br/><br/>Important aspects of the software elements will include (i) expressive programming interfaces for rapid prototyping of tensor based theories (ii) layered tensor libraries for dense, block-sparse, and out-of-core tensors that provide peak-performance implementations of the above interfaces, (iii) a multi-linear algebra package for general high dimensional computations, based on the matrix product state approach, and (iv) tensor virtual machine technology that abstracts algorithm development from hardware, and which provides a framework for optimizing compiler transformations to adapt algorithms to the memory access, communication networks, and processor characteristics of modern computer architectures.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265278","fuchsia_atom","SI2-SSE: General Tensor Software Elements for Quantum Chemistry, Tensor Network Theories, and Beyond"
"1550601","Collaborative Research: SI2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure","ACI","Software Institutes","8/1/2016","8/2/2016","David Anderson","CA","University of California-Berkeley","Standard Grant","Rajiv Ramnath","7/31/2017","$259,999.00 ","","davea@ssl.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","CSE","8004","7433|8004|8009","$0.00 ","The aggregate computing power of consumer devices - desktop and laptop computers, tablets, smartphones - far exceeds that of institutional computing resources. ""Volunteer computing"" uses these consumer devices, volunteered by their owners, to do scientific computing. In addition to providing additional, much-needed computational resources to scientists, volunteer computing publicizes scientific research and engages citizens in science. BOINC is the primary software system for volunteer computing. It was developed at UC Berkeley with NSF support starting in 2002. Until now, BOINC has been based on a model of independent competing projects. Scientists set up their own BOINC servers, port their applications to run on BOINC, and publicize their projects to attract volunteers. There are about 40 such projects, in many areas of science: examples include Einstein@home, CERN, and SETI@home (astrophysics), Rosetta@home and GPUGrid.net (biomedicine), Climateprediction.net (climate study), and IBM World Community Grid (multiple applications). Together these projects have about 400,000 active volunteers and 12 PetaFLOPS of computing throughput. This model, while successful to an extent, has reached a limit. The number of projects and volunteers has stagnated. Volunteer computing is supplying lots of computing power, but only to a few research projects. For other scientists, there are two major barriers. First, creating a BOINC project has significant overhead: learning a new technology, creating a public web site, generating publicity, and so on. Second, volunteer computing is risky and uncertain; there is no guarantee that a new project will attract volunteers. This project aims to break this barrier, and to make volunteer computing available to all scientists doing high-throughput computing, by replacing the competing-projects model with a new ""central broker"" model. The new model has two related parts: 1) the integration of BOINC with existing high-throughput computing facilities such as supercomputing centers and science portals. Jobs currently run on cluster nodes will be transparently offloaded to volunteer computers. Scientists using these facilities will see faster turnaround times; they'll benefit from volunteer computing without even knowing it's there. 2) The project will change the volunteer interface so that participants sign up for scientific areas and goals rather then for particular projects. For example, a participant might sign up to contribute to cancer research. A central broker, to be developed as part of this project, would dynamically assign their computing resources to projects doing that type of research. This project mobilizes public support for and interest in scientific research by encouraging ""volunteer computing"" and engaging citizens in the conduct of the research itself. It simultaneously advances NSF's mission to advance science while broadening citizen engagement.<br/><br/>The first year of this project will prototype each of these parts, and will integrate BOINC with TACC and nanoHub. Integrating BOINC with existing HTC systems involves several subtasks: 1) Job routing: modifying existing job processing systems used by TACC and nanoHub (Launcher and Rappture respectively) to decide when a group of jobs should be offloaded to BOINC. This decision might involve the estimated runtime of the jobs, input and output file sizes, data sensitivity, the deadline or priority of the jobs, and the identity of the job submitter. 2) Job format conversion: mapping job descriptions (input/output file specifications, resource and timing requirements) to their BOINC equivalents. 3) Application packaging: adapting existing applications (such as nanoHub's simulation tools and TACC's Autodock) to run under BOINC. We will use BOINC's virtual machine facility, which packages an application as a virtual machine image (VirtualBox or Docker) and a program to be run within the VM. This allows existing Linux applications to run on consumer desktop platforms such as Windows and Mac, as well as providing a strong security sandbox and an efficient application-independent checkpoint/restart mechanism. 4) File handling: moving input and output files between existing storage systems (typically inaccessible from outside firewalls) to Internet-visible servers. This will use existing BOINC components that manage files based on hashes to eliminate duplicate transfer and storage of files. 5) Job monitoring and control: adapting existing web- or command-line based tools for monitoring the progress of batches of jobs, and for aborting jobs, to work with BOINC. This will use existing Web RPCs provided by BOINC for these purposes. This project will carry out these tasks by designing and implementing new software as needed, testing for correctness, performance, and scalability, and deploying it in a production environment. The second part of the project - a brokering system for allocating computing power based on volunteer scientific preferences - will be designed and prototyped. This involves several subtasks: 1) Designing a schema for volunteer preferences, including scientific areas and sub-areas, project nationality and institutions, specific projects and applications, inclusions/exclusions, and so on. 2) Designing a schema for assigning attributes to job streams (e.g. their area, sub-area, institution, etc.), and for assigning quotas or priorities to job streams. 3) Designing a relational database for storing the above information. 4) Designing and implementing policies for assigning volunteer resources to job streams in a way that respects volunteer preferences and optimizes quota, fairness, and throughput criteria. This will be implemented as a BOINC ""account manager"" so that volunteers see a single interface rather than lots of separate projects and web sites.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550601","fuchsia_atom","Collaborative Research: SI2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure"
"1339797","SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS)","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS","10/1/2013","6/29/2016","Julien Langou","CO","University of Colorado at Denver-Downtown Campus","Continuing grant","Rajiv Ramnath","9/30/2017","$392,492.00 ","Rodney James","julien.langou@ucdenver.edu","F428, AMC Bldg 500","Aurora","CO","F428, AMC Bldg 500, Aurora, CO","800452571","3037240090","CSE","1253|8004|8069","7433|8009|9251","$0.00 ","As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs' own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.<br/><br/>The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for ""big data"". No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339797","fuchsia_tree","SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS)"
"1047879","SI2-SSI: Accelerating the Pace of Research through Implicitly Parallel Programming","ACI","INFORMATION TECHNOLOGY RESEARC|SPECIAL PROJECTS - CCF|Software Institutes","10/1/2010","9/13/2010","David August","NJ","Princeton University","Standard Grant","Sol J. Greenspan","3/31/2016","$1,740,214.00 ","David Walker","august@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","Off. of Research & Proj. Admin., Princeton, NJ","85442020","6092583090","CSE","1640|2878|8004","1640|2878","$0.00 ","Today, two trends conspire to slow down the pace of science, engineering, and academic research progress in general. First, researchers increasingly rely on computation to process ever larger data sets and to perform ever more computationally-intensive simulations. Second, individual processor speeds are no longer increasing with every computer chip generation as they once were. To compensate, processor manufacturers have moved to including more processors, or cores, on a chip with each generation. To obtain peak performance on these multicore chips, software must be implemented so that it can execute in parallel and thereby use the additional processor cores. Unfortunately, writing efficient, explicitly parallel software programs using today's software-development tools takes advanced training in computer science, and even with such training, the task remains extremely difficult, error-prone, and time consuming. This project will create a new high-level programming platform, called Implicit Parallel Programming (IPP), designed to bring the performance promises of modern multicore machines to scientists and engineers without the costs associated with having to teach these users how to write explicitly parallel programs. In the short term, this research will provide direct and immediate benefit to researchers in several areas of science as the PIs will pair computer science graduate students with non-computer science graduate students to study, analyze, and develop high-value scientific applications. In the long term, this research has the potential to fundamentally change the way scientists obtain performance from parallel machines, improve their productivity, and accelerate the overall pace of science. This work will also have major educational impact by developing courseware and tutorial materials, useable by all scientists and engineers, on the topics of explicit and implicit parallel computing.<br/><br/>IPP will operate by allowing users to write ordinary sequential programs and then to augment them with logical specifications that expand (or abstract) the set of sequential program behaviors. This capacity for abstraction will provide parallelizing compilers with the flexibility to more aggressively optimize programs than would otherwise be possible. In fact, it will enable effective parallelization techniques where they were impossible before. The language design and compiler implementation will be accompanied by formal semantic analysis that will be used to judge the correctness of compiler transformations, provide a foundation for about reasoning programs, and guide the creation of static analysis and program defect detection algorithms. Moreover since existing programs and languages can be viewed as (degenerately) implicitly parallel, decades of investment in human expertise, languages, compilers, methods, tools, and applications is preserved. In particular, it will be possible to upgrade old legacy programs or libraries from slow sequential versions without overhauling the entire system architecture, but merely by adding a few auxiliary specifications. Compiler technology will help guide scientists and engineers through this process, further simplifying the task. Conceptually, IPP restores an important layer of abstraction, freeing programmers to write high-level code, designed to be easy to understand, rather than low-level code, architected according to the specific demands of a particular parallel machine.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047879","fuchsia_atom","SI2-SSI: Accelerating the Pace of Research through Implicitly Parallel Programming"
"1148346","SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain","ACI","INFORMATION TECHNOLOGY RESEARC|Software Institutes","6/1/2012","5/29/2012","Allen Malony","OR","University of Oregon Eugene","Standard Grant","Rajiv Ramnath","5/31/2016","$926,667.00 ","Sameer Shende","malony@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","5219 UNIVERSITY OF OREGON, Eugene, OR","974035219","5413465131","CSE","1640|8004","7433|8009|8004","$0.00 ","Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via ""black box"" tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications. `Opening up' the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.<br/><br/>The project will explore the information that can be shared 'across the software stack'. Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes. Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware. Working with the `Keeneland' NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today's petascale machines. Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148346","fuchsia_atom","SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain"
"1265624","Collaborative Research: SI2-CHE:Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in the CP2k Software Suite","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","4/15/2013","4/2/2013","Troy Van Voorhis","MA","Massachusetts Institute of Technology","Standard Grant","Evelyn M. Goldfield","3/31/2016","$137,775.00 ","","tvan@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","77 MASSACHUSETTS AVE, Cambridge, MA","21394301","6172531000","MPS","1253|8004","5918|5946|5950|7433|8009|8650","$0.00 ","An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), Jrg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre & Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite. These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals. Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement. The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.<br/><br/>The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications. The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base. The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry. The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265624","chartreuse_atom","Collaborative Research: SI2-CHE:Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in the CP2k Software Suite"
"1148484","Collaborative Research: SI2-SSI: SciDaaS - Data Management as a Service","ACI","Software Institutes","4/1/2012","7/26/2016","Ian Foster","IL","University of Chicago","Standard Grant","Rajiv Ramnath","3/31/2017","$2,798,891.00 ","","foster@uchicago.edu","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004","7433|8004|8009","$0.00 ","The SciDaaS project will develop and operate a suite of innovative research data management services for the NSF community. These services, to be accessible at www.globusonline.org, will allow research laboratories to outsource a range of time-consuming research data management functions, including storage and movement, publication, and metadata management. SciDaaS research will investigate what services are most needed by NSF researchers; how best to present these services to integrate with diverse research laboratory environments; and how these services are used in practice across different research communities.<br/><br/>SciDaaS will greatly reduce the cost to the individual researcher of acquiring and operating sophisticated scientific data management capabilities. In so doing, it has the potential to dramatically expand use of advanced information technology in NSF research and thus accelerate discovery across many fields of science and engineering. By providing a platform for researchers to publicly share data at an incremental cost, SciDaaS will also reduce barriers to free exchange among researchers and contribute to the democratization of science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148484","fuchsia_atom","Collaborative Research: SI2-SSI: SciDaaS - Data Management as a Service"
"1440585","Collaborative Research: SI2-SSE: Pythia Network Diagnosis Infrastructure (PuNDIT)","ACI","Software Institutes|Campus Cyberinfrastrc (CC-NIE)","9/1/2014","8/25/2014","Warren Matthews","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","8/31/2017","$237,407.00 ","","warren.matthews@oit.gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","8004|8080","7433|8005","$0.00 ","In today's world of distributed collaborations of scientists, there are many challenges to providing effective infrastructures to couple these groups of scientists with their shared computing and storage resources. The Pythia Network Diagnostic InfrasTructure (PuNDIT) project will integrate and scale research tools and create robust code suitable for operational needs to address the difficult challenge of automating the detection and location of network problems. <br/><br/>PuNDIT will build upon the de-facto standard perfSONAR network measurement infrastructure to gather and analyze complex real-world network topologies coupled with their corresponding network metrics to identify possible signatures of network problems from a set of symptoms. For example if the symptoms suggest a router along the path has buffers configured too small for high performance, Pythia will return a diagnosis of ""Small Buffer"". If symptoms indicate non-congestive packet-loss for a particular network segment, the user can be notified of a possible ""Bad Network Segment"". A primary goal for PuNDIT is to convert complex network metrics into easily understood diagnoses in an automated way.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440585","fuchsia_atom","Collaborative Research: SI2-SSE: Pythia Network Diagnosis Infrastructure (PuNDIT)"
"1450280","Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|Software Institutes","6/15/2015","6/11/2015","Volker Blum","NC","Duke University","Standard Grant","Rajiv Ramnath","5/31/2019","$1,358,608.00 ","Jianfeng Lu","volker.blum@duke.edu","2200 W. Main St, Suite 710","Durham","NC","2200 W. Main St, Suite 710, Durham, NC","277054010","9196843030","CSE","1253|1712|8004","7433|8009|8084|9216","$0.00 ","Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (""in silico"") design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the ""cubic scaling wall"" of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure ""ELSI"" that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.<br/><br/>ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the ""scaling wall"" of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450280","fuchsia_atom","Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory"
"1450488","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","PHYSICAL & DYNAMIC METEOROLOGY|Software Institutes|EarthCube","8/1/2015","6/14/2016","Carl Maltzahn","CA","University of California-Santa Cruz","Standard Grant","Rajiv Ramnath","7/31/2018","$695,525.00 ","","carlosm@soe.ucsc.edu","1156 High Street","Santa Cruz","CA","1156 High Street, Santa Cruz, CA","950641077","8314595278","CSE","1525|8004|8074","7433|8009","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450488","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1047734","SI2-SSE: Parallel and Adaptive Simulation Infrastructure for Biological Fluid-Structure Interaction","ACI","INTERFAC PROCESSES & THERMODYN|Mechanics of Materials and Str|DYNAMICAL SYSTEMS|Software Institutes","9/15/2010","9/7/2010","Boyce Griffith","NY","New York University Medical Center","Standard Grant","Daniel Katz","10/31/2014","$499,996.00 ","Charles Peskin, David McQueen","boyceg@email.unc.edu","545 First Avenue","New York","NY","545 First Avenue, New York, NY","100166481","2122638822","CSE","1414|1630|7478|8004","1414|1630|7478","$0.00 ","The immersed boundary (IB) method is both a mathematical formulation and a numerical approach to problems of fluid-structure interaction, treating the specific case in which an elastic structure is immersed in a viscous incompressible fluid. The IB method was introduced to describe the fluid dynamics of heart valves, but this methodology has also been applied to a wide range of problems in biological and non-biological fluid dynamics. The IB method typically requires high spatial resolution to resolve the viscous boundary layers at fluid-structure interfaces and, at higher Reynolds numbers, to resolve vortices shed from such interfaces. To improve the efficiency of the IB method, the principal investigator has developed an adaptive version of the IB method that employs block-structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where it is needed. IBAMR software is a distributed-memory parallel implementation of this adaptive scheme. The key goal of this project is to make IBAMR the unifying software framework for users of the IB method, thereby establishing a community of researchers who employ a common software infrastructure for biofluids model development and simulation. The project aims to enhance IBAMR substantially by (1) developing and implementing implicit IB schemes that will allow for the efficient use of large numerical timesteps; (2) developing and implementing extensions of the basic IB methodology, including a new variable-viscosity version of the IB method, and an existing stochastic version for microscale and nanoscale problems in which Brownian motion is important; (3) optimizing IBAMR for use with modern as well as projected-future high performance computing systems comprised of multi-core compute nodes interconnected by a high-speed network; and (4) developing front-end tools for model construction, validation, and execution, thereby facilitating the adoption and use of IBAMR, especially by students and researchers with limited computational experience.<br/><br/>From the writhing and coiling of DNA, to the beating and pumping motions of cilia and flagella, to the flow of blood in the heart and throughout the circulation, coupled fluid-structure systems are ubiquitous in biology and physiology. This project aims to enhance significantly the IBAMR software developed by the principal investigator. IBAMR is a framework for performing computer simulations of biological fluid mechanics, and this project seeks to establish IBAMR as a unifying software infrastructure that will serve as a common ""language"" for developing and exchanging such models. IBAMR is already being actively used within several independent research projects that aim to model different aspects of cardiovascular dynamics, such as platelet aggregation and the fluid dynamics of natural and prosthetic heart valves. Such simulations promise ultimately to improve the efficacy of devices and procedures for treating cardiovascular disease. This software also is being used within projects that study other problems in biofluid mechanics, including insect flight, aquatic locomotion, and the dynamics of phytoplankton. By enhancing IBAMR, this project will also enhance significantly the ability of these and other research groups to construct detailed biofluids models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will enhance the IBAMR software substantially, extending the range of problems to which it may be applied, and improving the methods implemented within the software as well as the efficiency of the implementation. The work of this project will extend greatly the community of students and researchers who are able to use IBAMR to model biological fluid-structure interaction, in part by implementing graphical software tools for building IB models and running IB simulations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047734","fuchsia_atom","SI2-SSE: Parallel and Adaptive Simulation Infrastructure for Biological Fluid-Structure Interaction"
"1148453","Collaborative Research: SI2-SSI: An Interactive Software Infrastructure for Sustaining Collaborative Community Innovation in the Hydrologic Sciences","ACI","ADVANCES IN BIO INFORMATICS|ECOSYSTEM STUDIES|METHOD|MEASURE & STATS|HYDROLOGIC SCIENCES|ENVIRONMENTAL SUSTAINABILITY|PetaApps|CDI TYPE I","7/1/2012","7/31/2014","David Tarboton","UT","Utah State University","Standard Grant","Rajiv Ramnath","6/30/2017","$2,401,939.00 ","Richard Hooper, Jennifer Arrigo, David Maidment, Daniel Ames, Jonathan Goodall","dtarb@usu.edu","Sponsored Programs Office","Logan","UT","Sponsored Programs Office, Logan, UT","843221415","4357971226","CSE","1165|1181|1333|1579|7643|7691|7750","145E|1579|7433|8009|1165|1181|1333|7643|7691|7750|8004","$0.00 ","Water, its quality, quantity, accessibility, and management, is crucial to society. However, our ability to model and quantitatively understand the complex interwoven environmental processes that control water and its availability is severely hampered by inadequate tools related to hydrologic data discovery, systems integration, modeling/ simulation, and education. This project develops sustainable cyberinfrastructure for better access to water-related data and models in the hydrologic sciences, enabling hydrologists and other associated communities to collaborate and combine data and models from multiple sources. It will provide new ways in which hydrologic knowledge is created and applied to better understand water availability, quality, and dynamics. It will also help to provide a more comprehensive understanding of the interactions between natural and engineered aspects of the water cycle. These goals will be achieved through the development of interoperable cyberinfrastructure tools and the creation of an online collaborative environment, called HydroShare, which enables scientists to easily discover and access hydrologic and related data and models, retrieve them to their desktop, and perform analyses in a high performance computing environment. The software to be developed will take advantage of existing NSF cyberinfrastructure (iRODS, HUBzero, CSDMS, CUAHSI HIS) and be created as open source code. Its development will be end user-driven. In terms of broader impacts, the project builds essential infrastructure for science by developing software tools and computing environments to allow better understanding of the impacts of climate change (i.e., floods, droughts, biofuels, etc.) and to allow improved water resource development and the management of freshwater resources both above and below ground. Resulting software will be made publicly available and provides a strong student and workforce training/education component. In addition, the project supports an institution in an EPSCoR state and engages, as a PI, a person who is from a group under-represented in the sciences and engineering.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148453","fuchsia_atom","Collaborative Research: SI2-SSI: An Interactive Software Infrastructure for Sustaining Collaborative Community Innovation in the Hydrologic Sciences"
"1550514","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","ACI","Software Institutes","7/1/2016","7/11/2016","Gabrielle Allen","IL","University of Illinois at Urbana-Champaign","Continuing grant","Rajiv Ramnath","6/30/2020","$225,000.00 ","Matthew Turk","gdallen@illinois.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","8004","7433|7569|8009|8084","$0.00 ","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies. A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building a simulation data repository. The repository will allows user to compare results, contribute data, test innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550514","fuchsia_tree","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration"
"1550223","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","ACI","COMPUTATIONAL PHYSICS|Software Institutes","7/1/2016","7/6/2016","Ulrich Heinz","OH","Ohio State University","Standard Grant","Bogdan Mihaila","6/30/2020","$263,673.00 ","","heinz@mps.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","Office of Sponsored Programs, Columbus, OH","432101016","6146888735","CSE","7244|8004","026Z|7433|7569|8009|8084","$0.00 ","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550223","fuchsia_atom","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)"
"1534872","SI2-SSE: ShareSafe: A Framework for Researchers and Data Owners to Help Facilitate Secure Graph Data Sharing","ACI","Software Institutes","9/1/2015","6/23/2016","Raheem Beyah","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","8/31/2018","$506,000.00 ","","rbeyah@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","8004","7433|8004|8005|9251","$0.00 ","There is a critical need for information/data sharing to solve some of our most significant academic and societal problems. These data are increasing in size and are becoming much more complex; in many cases, they can be considered structured. An example of structured data is data describing disease propagation in a specific population. Widespread sharing of data can, among many other things, help corporations increase their revenues, help reduce the spread of communicable diseases, accelerate the cure of some of the most significant diseases, and enable reproducible experiments amongst researchers. Although there is little disagreement that sharing data has tremendous benefits, it is still not as widespread as it should be. This is, in part, due to privacy concerns with sharing datasets. This project will develop an open source system (ShareSafe) that allows data owners to evaluate the security (such as resistance to de-anonymization attacks) and utility of their anonymized datasets before release, which will help facilitate the data sharing process.<br/><br/>The overarching goals of this project are to develop a software framework, ShareSafe, that (1) helps structured data owners (e.g., social network researchers, epidemiologists) evaluate the security (against modern de-anonymization attacks) and utility of their datasets when using simple and state-of-the-art anonymization techniques; and (2) to provide structured data security/privacy researchers a uniform platform to comprehensively study, evaluate, and compare existing/newly developed techniques for structured data utility and privacy. ShareSafe is a comprehensive, user-friendly framework with the following capabilities: ShareSafe will enable data owners to: (1) anonymize their datasets with all of the state-of-the art anonymization techniques; (2) measure the utility of anonymized datasets using state-of-the-art utility measurement techniques; (3) evaluate the practical security of their datasets by subjecting them to state-of-the-art de-anonymization attacks; and (4) evaluate the theoretical security of their datasets by subjecting them to state-of-the-art de-anonymization quantification (de-anonymizability analysis) techniques. Understanding the results from (2)-(4) allows data owners to determine which anonymization algorithm suits their needs when sharing datasets. Finally, the aforementioned techniques will be implemented in a uniform manner as open source software, allowing graph data security/privacy researchers the ability to comprehensively study, evaluate, and compare existing/newly developed techniques for graph data utility and privacy.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1534872","fuchsia_atom","SI2-SSE: ShareSafe: A Framework for Researchers and Data Owners to Help Facilitate Secure Graph Data Sharing"
"1450356","SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists","ACI","SPECIAL PROJECTS - CCF|Software Institutes","6/1/2015","6/1/2015","Barbara Lerner","MA","Mount Holyoke College","Standard Grant","Rajiv Ramnath","5/31/2018","$422,997.00 ","","blerner@mtholyoke.edu","50 College Street","South Hadley","MA","50 College Street, South Hadley, MA","10756456","4135382000","CSE","2878|8004","7433|8009","$0.00 ","Reproducability is the cornerstone of scientific progress. Historically, scientists make their work reproducible by including a formulaic description of the experimental methodology used in an experiment. In an age of computational science, such descriptions no longer adequately describe scientific methodology. Instead, scientific reproducibility relies on a precise and actionable description of the data and programs used to conduct the research. Provenance is the name given to the description of how a digital artifact came to be in its present state. Provenance includes a precise specification of an experiment's input data and the programs or procedures applied to that data. Most computational platforms do not record such data provenance, making it difficult to ensure reproducability. This project addresses this problem through the development of tools that transparently and automatically capture data provenance as part of a scientist's normal computational workflow.<br/><br/>An interdisciplinary team of computer scientists and ecologists have come together to develop tools to facilitate the capture, management, and query of data provenance -- the history of how a digital artifact came to be in its present state. Such data provenance improves the transparency, reliability, and reproducibility of scientific results. Most existing provenance systems require users to learn specialized tools and jargon and are unable to integrate provenance from different sources; these are serious obstacles to adoption by domain scientists. This project includes the design, development, deployment, and evaluation of an end-to-end system (eeProv) that encompasses the range of activity from original data analysis by domain scientists to management and analysis of the resulting provenance in a common framework with common tools. This project leverages and integrates development efforts on (1) an emerging system for generating provenance from a computing environment that scientists actually use (the R statistical language) with (2) an emerging system that utilizes a library of language and database adapters to store and manage provenance from virtually any source. Accomplishing the goals of this proposal requires fundamental research in resolving the semantic gap between provenance collected in different environments, capturing detailed provenance at the level of a programming language, defining precisely aspects of provenance required for different use cases, and making provenance accessible to scientists.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450356","fuchsia_atom","SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists"
"1440753","SI2-SSE: Adding Research Accounts to the ASSISTments' Platform: Helping Researchers Do Randomized Controlled Studies with Thousands of Students","ACI","PROGRAM EVALUATION|Software Institutes","9/1/2014","10/3/2014","Neil Heffernan","MA","Worcester Polytechnic Institute","Standard Grant","Rajiv Ramnath","8/31/2017","$486,209.00 ","","nth@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","100 INSTITUTE RD, WORCESTER, MA","16092247","5088315000","CSE","7261|8004","7433|8005|9177|SMET","$0.00 ","ASSISTments is a free, university-based platform created to perform controlled experiments with the potential to help increase the quality, speed, and reliability of results related to K12 education. ASSISTments' mission is ""to improve education through scientific research while not compromising student learning time."" Each day, teachers assign problems to thousands of students (currently 50,000 students) in ASSISTments. These problem sets often contain controlled experiments. ASSISTments has used this platform to do controlled experiments that have resulted in 17 peer-reviewed publications. For a typical education researcher, developing relationships with schools is costly. ASSISTments has built relationships with teachers and researchers to run experiments to improve education without disrupting classrooms. This project will add researcher accounts to ASSISTments to better facilitate the research process. Researchers will create their own experiments, get approval from WPI for release to teachers, and get anonymized data. ASSISTments will reach out to its community of teachers who trust ASSISTments, to invite them to run the study in their classrooms. The intellectual merit of this work will be the contribution of the studies that this system would support. ASSISTments' ten-year goal is to have a community of hundreds of scientists that use this tool to do their studies. <br/><br/>Psychologists tend to study human learning in lab studies; researchers in education and learning sciences point out that it's not clear if those studies generalize to K12. These communities need to work together, but are lacking common ground. Thousands of researchers in psychology, mathematics education, and learning sciences care about using science to better understand human learning. Some researchers study how to help students with motivational messages, spaced retesting, or comparing feedback. Many researchers have used thousands of psychology undergraduates as subjects, but want their ideas tested and validated in authentic K12 settings. Everyone understands physicists need a shared scientific instrument to do their work, but so do educational psychologists. The broader impact of this work will be as a demonstration, showing how a tool could be built that helps many researchers conduct controlled experiments. This will include showing how the project can increase the efficiency of the scientists? work.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440753","fuchsia_atom","SI2-SSE: Adding Research Accounts to the ASSISTments' Platform: Helping Researchers Do Randomized Controlled Studies with Thousands of Students"
"1216890","Collaborative Research: SI2-S2I2: High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP)","ACI","OFFICE OF MULTIDISCIPLINARY AC|COMBUSTION|FIRE|& PLASMA SYS|PARTICULATE &MULTIPHASE PROCES|FLUID DYNAMICS|PHYSICS AT THE INFO FRONTIER|ICER|Software Institutes","9/1/2012","8/28/2012","Phillip Colella","CA","University of California-Berkeley","Standard Grant","Daniel Katz","8/31/2014","$366,050.00 ","James Demmel","pcolella@lbl.gov","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","CSE","1253|1407|1415|1443|7553|7699|8004","056E|058E|7433|7483|8211|9216|9263|1253|1407|1415|1443|7553|7699|8004","$0.00 ","The starting point for this proposal is a view of scientific simulation articulated in the conclusions of the 2008 National Academy of Sciences Study, The Potential Impact of High-End Capability Computing on Four Illustrative Fields of Science and Engineering: ""Advanced computational science and engineering is a complex enterprise that requires models, algorithms, software, hardware, facilities, education and training, and a community of researchers attuned to its special needs."" (p. 122)<br/><br/>Over the last few years, the design of computer and software systems, particularly as they relate to simulation in the physical sciences, has been organized around a collection of algorithmic patterns / motifs. These patterns have been very productive because they are a natural ""common language"" in which application scientists can express their computations, and for which computer scientists can provide optimized libraries, domain specific languages, compilers, and other software tools.<br/><br/>This project will design an institute focused on a subset of these patterns --- structured grid discretizations of partial differential equations and particle methods, along with the linear and nonlinear solvers that enable their effective use --- with the specific goals of providing simulation capabilities for a set of scientific domains that make heavy use of these patterns. Two major components are envisioned to this proposed institute, called the Institute for High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP). The first component is a software infrastructure development activity that will be performed by a team whose expertise spans the design and development of mathematical algorithms and software frameworks, as well as the design and development of compilers, runtime systems, and tools that enable one to obtain high performance from emerging multicore and heterogeneous architectures. The second component is an outreach activity, in which algorithms, libraries, and software frameworks developed by the institute will be customized and integrated into simulation codes for stakeholder application domains. At the heart of this activity will be collaborations and partnerships, in which the institute will provide one or more software developers to collaborate with application scientists over a period of months to years to develop a new simulation capability or enhance an existing one.<br/><br/>The design of this institute will be carried out through a series of workshops, each focused on one of five stakeholder science domains that have been identified as using these motifs and that play a central role in various NSF Grand Challenge problems, with participation of both representatives of the science domain and the the relevant mathematics and computer science communities. In addition, there will be a final workshop that will bring together the relevant mathematics and computer science experts to identify cross-cutting themes. These information obtained from these workshops will be used by the project to develop the final conceptual design of the institute, in the form of a document that includes the input from all of the workshops and our analysis of how this leads to a design of a software institute.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1216890","fuchsia_atom","Collaborative Research: SI2-S2I2: High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP)"
"1440587","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions","ACI","SPECIAL PROJECTS - CCF|Software Institutes|CDS&E","2/1/2015","8/8/2014","Erion Plaku","DC","Catholic University of America","Standard Grant","Rajiv Ramnath","1/31/2018","$215,476.00 ","","plaku@cua.edu","620 Michigan Ave.N.E.","Washington","DC","620 Michigan Ave.N.E., Washington, DC","200640001","2026355000","CSE","2878|8004|8084","7433|8004|8005|8084|9216|2878","$0.00 ","This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.<br/><br/>The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440587","fuchsia_atom","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions"
"1450277","SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists","ACI","SPECIAL PROJECTS - CCF|Software Institutes","6/1/2015","8/13/2015","Margo Seltzer","MA","Harvard University","Standard Grant","Rajiv Ramnath","5/31/2018","$1,422,728.00 ","Emery Boose, Aaron Ellison","margo@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","1033 MASSACHUSETTS AVE, Cambridge, MA","21385366","6174955501","CSE","2878|8004","7433|8009","$0.00 ","Reproducability is the cornerstone of scientific progress. Historically, scientists make their work reproducible by including a formulaic description of the experimental methodology used in an experiment. In an age of computational science, such descriptions no longer adequately describe scientific methodology. Instead, scientific reproducibility relies on a precise and actionable description of the data and programs used to conduct the research. Provenance is the name given to the description of how a digital artifact came to be in its present state. Provenance includes a precise specification of an experiment's input data and the programs or procedures applied to that data. Most computational platforms do not record such data provenance, making it difficult to ensure reproducability. This project addresses this problem through the development of tools that transparently and automatically capture data provenance as part of a scientist's normal computational workflow.<br/><br/>An interdisciplinary team of computer scientists and ecologists have come together to develop tools to facilitate the capture, management, and query of data provenance -- the history of how a digital artifact came to be in its present state. Such data provenance improves the transparency, reliability, and reproducibility of scientific results. Most existing provenance systems require users to learn specialized tools and jargon and are unable to integrate provenance from different sources; these are serious obstacles to adoption by domain scientists. This project includes the design, development, deployment, and evaluation of an end-to-end system (eeProv) that encompasses the range of activity from original data analysis by domain scientists to management and analysis of the resulting provenance in a common framework with common tools. This project leverages and integrates development efforts on (1) an emerging system for generating provenance from a computing environment that scientists actually use (the R statistical language) with (2) an emerging system that utilizes a library of language and database adapters to store and manage provenance from virtually any source. Accomplishing the goals of this proposal requires fundamental research in resolving the semantic gap between provenance collected in different environments, capturing detailed provenance at the level of a programming language, defining precisely aspects of provenance required for different use cases, and making provenance accessible to scientists.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450277","fuchsia_atom","SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists"
"1148458","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments","ACI","ADVANCES IN BIO INFORMATICS|ECOSYSTEM STUDIES|Software Institutes|Cyber Secur - Cyberinfrastruc","8/1/2012","9/14/2015","Tony Fountain","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","7/31/2016","$1,455,429.00 ","Ilya Zaslavsky, Sameer Tilak","fountain@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","1165|1181|8004|8027","7434|8009|1165|1181|8004|8027","$0.00 ","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148458","fuchsia_atom","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments"
"1147463","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|Software Institutes|CDS&E-MSS","6/1/2012","6/19/2012","Daniel Bump","CA","Stanford University","Standard Grant","Rajiv Ramnath","5/31/2017","$143,700.00 ","","bump@math.stanford.edu","3160 Porter Drive","Palo Alto","CA","3160 Porter Drive, Palo Alto, CA","943041212","6507232300","CSE","1253|7478|8004|8069","7433|7683|8005|1253|5514|7478|8004","$0.00 ","Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is ""to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area"". There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s. The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thiery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.<br/><br/>The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147463","fuchsia_atom","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics"
"1147912","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse","ACI","Software Institutes|SPECIAL PROJECTS - CCF|IIS SPECIAL PROJECTS|SPECIAL PROJECTS - CISE|ROBUST INTELLIGENCE|COLLABORATIVE RESEARCH|SOFTWARE & HARDWARE FOUNDATION|COMPUTER SYSTEMS|","8/1/2012","9/11/2015","James Pustejovsky","MA","Brandeis University","Standard Grant","Rajiv Ramnath","7/31/2017","$1,764,929.00 ","Eric Nyberg, Christopher Cieri, Marc Verhagen","jamesp@cs.brandeis.edu","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","415 SOUTH ST MAILSTOP 116, WALTHAM, MA","24532728","7817362121","CSE","8004|2878|7484|1714|7495|7298|7798|7354|O422","8009|7433|5983|7944|1714|2878|7484|8004","$0.00 ","The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. <br/><br/>The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. <br/><br/>By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147912","fuchsia_atom","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse"
"1148310","SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain","ACI","INFORMATION TECHNOLOGY RESEARC|Software Institutes","6/1/2012","12/16/2015","Karsten Schwan","GA","Georgia Tech Research Corporation","Standard Grant","Patricia Knezek","5/31/2016","$926,666.00 ","Greg Eisenhauer, Matthew Wolf, Sudhakar Yalamanchili","schwan@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","1640|8004","7433|8009|8004","$0.00 ","Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via ""black box"" tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications. `Opening up' the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.<br/><br/>The project will explore the information that can be shared 'across the software stack'. Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes. Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware. Working with the `Keeneland' NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today's petascale machines. Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148310","fuchsia_atom","SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain"
"1148502","SI2-SSE: Developing and Deploying Path-Integral Quantum Simulation Tools for a Broad Research Community","ACI","OFFICE OF MULTIDISCIPLINARY AC|CONDENSED MATTER & MAT THEORY|Software Institutes","6/1/2012","9/3/2014","John Shumway","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","5/31/2014","$160,896.00 ","","john.shumway@asu.edu","ORSPA","TEMPE","AZ","ORSPA, TEMPE, AZ","852816011","4809655479","CSE","1253|1765|8004","6863|7237|7433|7569|8005|9162|AMPP|8004","$0.00 ","This award supports research and education activities that will make path integral Monte Carlo codes for simulation of atomic and molecular systems, and materials at the quantum mechanical level accessible to students and non-experts. This will stimulate new researchers to contribute fresh ideas and help build a larger base of users. The PI will pursue a strategy based on three principles: (i) On-going development of open-source software following software engineering practices must emphasize features that have a timely impact on science, encourage intuitive understanding, and assist non-expert users. (ii) Education and outreach in the form of tutorials, documentation, and workshops must dramatically lower the barrier for the use of path integral Monte Carlo by non-specialists. (iii) Development and execution of a set of high-impact projects -- in warm, dense matter, cold atoms, nano-electronics, and quantum chemistry -- by a network of collaborators which will demonstrate the research capabilities of the open-source path integral Monte Carlo code.<br/><br/>The code under development already embodies many software design principles, including object oriented design, XML input, HDF5 output, GNU GPL open-source distribution, and code management in subversion. Under this project the PI will: (i) complete documentation, including tutorials with explanations of the scientific concepts; (ii) develop a set of benchmarks and unit tests, to document standard applications and verify proper execution of the code; (iii) coordinate a set of high-impact projects across a network of collaborators, (iv) develop a timeline and milestones for formal releases, with executables for UNIX, Macintosh, and Windows, including the pyQt4 graphical user interface; and (v) integrate these improvements into existing nanoHUB port and other software infrastructures. New computer code will be distributed under the General Public License, and analysis tools will use open-source python libraries.<br/><br/>This award supports research and education activities to develop well engineered computer codes for the high accuracy simulation of electrons in materials and tiny structures made of a small number of atoms, and systems of atoms at the level of quantum mechanics. Developed computer codes will be made available to the broader research community through existing software centers, such as nanoHUB in a well documented form. This will enable students and non-experts to access and use the codes to perform simulations using path-integral techniques and to generate fresh ideas to advance the computational method. <br/><br/>Path-integral methods have been applied so far to various problems where quantum mechanics is important, including electronic systems and devices on the nanoscale, cold atoms trapped by lasers, and some atoms and small molecules. The technique has high accuracy and has advantages over existing methods, but it has so far not been adequately developed. It also has great pedagogical value for teaching the principles of quantum mechanics.<br/><br/>Students and junior scientists will be trained in the application of advanced computational methods to problems in nanotechnology.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148502","fuchsia_atom","SI2-SSE: Developing and Deploying Path-Integral Quantum Simulation Tools for a Broad Research Community"
"1550481","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","ACI","Software Institutes","8/1/2016","7/21/2016","Toru Shiozaki","IL","Northwestern University","Standard Grant","Rajiv Ramnath","7/31/2020","$600,000.00 ","","shiozaki@northwestern.edu","1801 Maple Ave.","Evanston","IL","1801 Maple Ave., Evanston, IL","602013149","8474913003","CSE","8004","7433|8009|9216","$0.00 ","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products. This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550481","fuchsia_atom","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids"
"1550456","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","ACI","Software Institutes","8/1/2016","7/21/2016","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Standard Grant","Rajiv Ramnath","7/31/2020","$600,000.00 ","","evaleev@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","Sponsored Programs 0170, BLACKSBURG, VA","240610001","5402315281","CSE","8004","7433|8009|9216","$0.00 ","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products. This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550456","fuchsia_atom","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids"
"1550551","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","ACI","Software Institutes","7/1/2016","7/11/2016","Frank Loffler","LA","Louisiana State University & Agricultural and Mechanical College","Continuing grant","Rajiv Ramnath","6/30/2020","$224,735.00 ","Steven Brandt, Peter Diener","knarf@cct.lsu.edu","202 Himes Hall","Baton Rouge","LA","202 Himes Hall, Baton Rouge, LA","708032701","2255782760","CSE","8004","7433|7569|8009|8084|9150","$0.00 ","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies. A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building a simulation data repository. The repository will allows user to compare results, contribute data, test innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550551","fuchsia_tree","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration"
"1550228","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","ACI","COMPUTATIONAL PHYSICS|Software Institutes","7/1/2016","7/6/2016","Barbara Jacak","CA","University of California-Berkeley","Standard Grant","Bogdan Mihaila","6/30/2020","$423,772.00 ","Xin-Nian Wang","bvjacak@lbl.gov","Sponsored Projects Office","BERKELEY","CA","Sponsored Projects Office, BERKELEY, CA","947045940","5106428109","CSE","7244|8004","026Z|7433|7569|8009|8084","$0.00 ","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550228","fuchsia_atom","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)"
"1550225","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","ACI","OFFICE OF MULTIDISCIPLINARY AC|APPLIED MATHEMATICS|Software Institutes","7/1/2016","7/6/2016","Steffen Bass","NC","Duke University","Continuing grant","Bogdan Mihaila","6/30/2020","$417,710.00 ","Robert Wolpert","bass@phy.duke.edu","2200 W. Main St, Suite 710","Durham","NC","2200 W. Main St, Suite 710, Durham, NC","277054010","9196843030","CSE","1253|1266|8004","026Z|4444|7433|7569|8009|8084|8251","$0.00 ","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550225","fuchsia_tree","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)"
"1550172","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","ACI","COMPUTATIONAL PHYSICS|Software Institutes","7/1/2016","7/6/2016","Gunther Roland","MA","Massachusetts Institute of Technology","Standard Grant","Bogdan Mihaila","6/30/2020","$189,500.00 ","","Gunther.Roland@cern.ch","77 MASSACHUSETTS AVE","Cambridge","MA","77 MASSACHUSETTS AVE, Cambridge, MA","21394301","6172531000","CSE","7244|8004","026Z|7433|7569|8009|8084","$0.00 ","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1550172","fuchsia_atom","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)"
"1535032","SI2-SSE: Scalable Multifaceted Graphical Processing Unit (GPU) Program Debugging","ACI","SPECIAL PROJECTS - CCF|Software Institutes","9/1/2015","6/23/2016","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Rajiv Ramnath","8/31/2017","$425,482.00 ","","ganesh@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","75 S 2000 E, SALT LAKE CITY, UT","841128930","8015816903","CSE","2878|8004","7433|8004|8005|9150|9251","$0.00 ","Modern scientific research crucially depends on software simulations that help model scientific phenomena, and accelerate the process of discoveries, and communal result sharing. With the availability of affordable computational accelerators known as GPUs, the scientific community has begun migrating their existing CPU codes as well as creating new codes targeting GPUs. Unfortunately, this has resulted in a situation where the generated scientific results do not often agree across CPUs and GPUs. This exacerbates the danger of drawing wrong conclusions in crucial areas such as physics, weather simulations, drug discovery, and engineering computations. This project offers a combination of existing and new techniques in dissecting scientific experiments conducted through simulations, obtaining believable results, finding the root causes of varying results, and developing best practices to ensure higher result fidelity. Its techniques have special emphasis on GPUs, given their often poorly specified and evolving nature.<br/><br/>Result variability has many causes, including evolving, incorrect, or ambiguous specifications of computer hardware and software, racing data accesses, varying floating point precision standards, and incorrect result association within compound computational steps. This project develops methods that help a scientist systematically search through and eliminate these causes, thus accelerating the process of debugging result variability. The produced tools and exemplars of known erroneous behaviors allow a scientist to avoid the use of incorrect specifications, isolate and eliminate data races, and isolate and eliminate unreliable numerical steps. It also develops methods that help a scientist maintain focus on their basic scientific pursuits while still keeping up with technology evolution. It trains students in critical software engineering techniques that help the nation build the talent pool necessary for the extreme scale computing era.<br/><br/>The project will combine six research thrusts (GPU concurrency; challenge problems and develop user interfaces; pedagogy for domain scientists; improved GPU concurrency debugging tool support; more reproducible simulation results; and evolving and scaling tools with standards) to build and deliver open source software that incorporates proven stress-testing methods into tools; builds challenge problems, supports formalization support, and designs the user interface; delivers demos, books, and tutorials that help illustrate concurrency nuances; exploits symbolic analysis for input generation in mixed formal and GPU runs; develops stress testing inputs for round-off errors and separable verification to root-cause roundoff; and componentizes the symbolic verifier to enable parallelism, targeting from new APIs.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535032","fuchsia_atom","SI2-SSE: Scalable Multifaceted Graphical Processing Unit (GPU) Program Debugging"
"1203182","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|COFFES|Software Institutes","8/18/2012","12/20/2012","George Biros","TX","University of Texas at Austin","Standard Grant","Thomas F. Russell","6/30/2013","$250,000.00 ","","biros@ices.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","101 E. 27th Street, Suite 5.300, Austin, TX","787121532","5124716424","CSE","1253|7478|7552|8004","","$0.00 ","We propose to develop and deploy mathematical software for boundary-value problems in three-dimensional complex geometries. The algorithms in the library will be based on integral equation formulations. The library will be designed to scale on novel computing platforms that comprise special accelerators and manycore architectures. <br/><br/>Integral equations can be used to conduct simulations on many problems in science and engineering with significant societal impact. Three example applications on which the proposed simulation technologies will have an impact in this project are microfluidic chips, biomolecular electrostatics, and plasma physics. First, microfluidic chips are submillimeter-sized devices used for medical diagnosis and drug design. Optimizing the function of such devices at low cost requires efficient computer simulation tools, such as the ones we propose to develop. Second, understanding the structure and function of biomolecules such as DNA and proteins is crucial in biotechnology. The proposed technologies can be used to resolve bimolecular electrostatic interactions. Third, plasma physics, which is related to fusion nuclear reactors, includes electrostatic interactions in complex geometries, and the proposed work will enable large-scale three-dimensional simulations. <br/><br/>The key features of the proposed software are: (1) parallel fast multipole methods, (2) efficient geometric modeling techniques for complex geometries, (3) simple library interfaces that allow use of the proposed software by non-experts, and (4) scalability on heterogeneous architectures.<br/><br/>Along with our research activities, an educational and dissemination program will be designed to communicate the results of this work to students and researchers. Several postdoctoral, graduate, and undergraduate students will be involved with the project. Additional educational activities will include research experiences for undergraduates, leveraging ongoing programs such as NSF REUs. We will encourage participation by women, minorities, and underrepresented groups.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1203182","fuchsia_atom","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures"
"1047961","SI2-SSE: Collaborative: Extensible Languages for Sustainable Development of High Performance Software in Materials Science","ACI","Software Institutes|COMPUTER SYSTEMS|OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT","9/15/2010","9/7/2010","Eric Van Wyk","MN","University of Minnesota-Twin Cities","Standard Grant","Daryl W. Hess","8/31/2015","$300,000.00 ","Yousef Saad","evw@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","200 OAK ST SE, Minneapolis, MN","554552070","6126245599","CSE","8004|7354|1253|1712","1765|7237|7569|1253|1712|7354","$0.00 ","This award is part of the Software Infrastructure for Sustained Innovation. The Office of Cyberinfrastructure, the Division of Computer and Network Systems, and the Division of Materials Research contribute funds to this award. <br/><br/>Developing large computational codes such as those used to perform simulations in quantum mechanics to calculate properties of materials, or to predict the aerodynamics forces around airplanes, still typically require several human-years. However the pace of research and industrial product development demands much more rapid software tool development to make progress and to remain competitive. This award contributes to developing the capability to rapidly create high performance large scale codes. <br/><br/>The PIs will augment a computer programming language with a very high level language that is interactive in the sense that the developer will enter language commands and get instantaneous interpreted answers, instead of processing the whole code. This approach of creating an interactive extensible language framework will provide a way to help speed development of large scale computer software. Efforts will be specifically targeted at software for materials science applications. This will enable progress in large scale computational research that aims to predict properties of materials starting from a knowledge of the constituent atoms and the way they are arranged in the material. <br/><br/>This award contributes to the education of knowledgeable specialists capable of developing large and complex computational codes. The PIs will design new graduate level courses outside of the current curriculum to increase the number of students who receive training in effective development of software for materials research and scientific computing in general. <br/><br/>This award also supports the research team's efforts to broaden participation of underrepresented groups through the existing Alice in Wonderland Program, which aims to recruit members of underrepresented groups at the high school level, and to attract female high school students to science and engineering by involving them in research over the summer before they make decisions about colleges. They will also revive the Summer Undergraduate Interns program to recruit undergraduate students interested in high performance computing for summer internships.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047961","fuchsia_atom","SI2-SSE: Collaborative: Extensible Languages for Sustainable Development of High Performance Software in Materials Science"
"1047871","SI2-SSI: REAL-TIME LARGE-SCALE PARALLEL INTELLIGENT CO2 DATA ASSIMILATION SYSTEM","ACI","ADVANCES IN BIO INFORMATICS|INTERFAC PROCESSES & THERMODYN|ICER|Software Institutes","9/15/2010","6/11/2012","Anna Michalak","MI","University of Michigan Ann Arbor","Continuing grant","Daniel Katz","6/30/2013","$1,564,244.00 ","Clayton Scott, Michael Cafarella, Xuanlong Nguyen, Vineet Yadav","michalak@stanford.edu","3003 South State St. Room 1062","Ann Arbor","MI","3003 South State St. Room 1062, Ann Arbor, MI","481091274","7347636438","CSE","1165|1414|7699|8004","1165|1414|7699","$0.00 ","Intellectual Merit: This proposal seeks to address this need by creating a state-of-the-art autonomous software platform for real-time integration of in-situ and satellite-based atmospheric CO2 measurements within a Data Assimilation (DA) system for producing estimates of global land and oceanic CO2 exchange at weekly to bi-weekly intervals. The proposed software infrastructure will be capable of autonomous processing of large volumes of data through a multi-stage pipeline, without the delays conventionally associated with such processing. Within the DA component, we will provide options for multiple DA algorithms for estimating global CO2 exchange. Users will, for the first time, have the capability to use these multiple methods as part of a single system for comparing estimates of CO2 exchange, and to obtain an improved understanding of the relative advantages of the various DA methods. As part of the analysis component of the software, we will build a carbon-climate surveillance system by drawing from a range of techniques in pattern recognition and high-dimensional statistical inference. This system will be able to detect and analyze localized variations in CO2 exchange within any user-specified spatio-temporal window. In addition, summaries of the CO2 exchange will be provided at annual and monthly temporal scales for continents and countries.<br/><br/>Broader Impacts: This software can be used by researchers and governmental institutions for evaluating both the natural components of the carbon cycle and anthropogenic carbon emissions, as well as in the design of new satellites for improved monitoring of CO2. All data and software will be publicly available and open-source development platforms will be used whenever possible. The algorithm prototypes developed as part of this project will be used in undergraduate and graduate courses at the University of Michigan, and will be made available online for educators at other institutions. Finally, the project will train three graduate students, with a focus on developing their cross-disciplinary skills in the field of Earth science, statistics, computer science, and atmospheric science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047871","fuchsia_tree","SI2-SSI: REAL-TIME LARGE-SCALE PARALLEL INTELLIGENT CO2 DATA ASSIMILATION SYSTEM"
"1047916","SI2-SSI: CyberGIS Software Integration for Sustained Geospatial Innovation","ACI","METHOD|MEASURE & STATS|GEOGRAPHY AND SPATIAL SCIENCES|CROSS-DIRECTORATE ACTIV PROGR|Software Institutes|Sustainable Energy Pathways","10/1/2010","7/31/2015","Shaowen Wang","IL","University of Illinois at Urbana-Champaign","Continuing grant","Cheryl L. Eavey","9/30/2016","$4,804,821.00 ","Timothy Nyerges, Nancy Wilkins-Diehr, Luc Anselin, Budhendra Bhaduri","shaowen@illinois.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","1333|1352|1397|8004|8026","004Z|1333|1352|7433|7556|7969|8004|8009|1397","$0.00 ","Originally developed by geographers in the mid-1960s, Geographic Information Systems (GIS) have flourished since that time. In the foreseeable future, GIS software will continue to play essential roles for breaking through scientific challenges in numerous fields and improving decision-making practices with broad societal impacts. However, fulfilling such roles is increasingly dependent on the ability to handle very large spatiotemporal data sets and complex analysis software based on synthesizing computational and spatial thinking enabled by cyberinfrastructure, which conventional GIS-based software approaches do not provide. This project will establish CyberGIS as a fundamentally new software framework comprising a seamless integration of cyberinfrastructure, GIS, and spatial analysis/modeling capabilities. Specifically, the project will: 1) engage a multidisciplinary community through a participatory approach in evolving CyberGIS software requirements; 2) integrate and sustain a core set of composable, interoperable, manageable, and reusable CyberGIS software elements based on community-driven and open source strategies; 3) empower high-performance and scalable CyberGIS by exploiting spatial characteristics of data and analytical operations for achieving unprecedented capabilities for geospatial knowledge discovery; 4) enhance an online geospatial problem solving environment to allow for the contribution, sharing, and learning of CyberGIS software by numerous users, which fosters the development of education, outreach, and training programs crosscutting multiple disciplines; 5) deploy and test CyberGIS software by linking with national and international cyberinfrastructure to achieve scalability to significant sizes of geospatial problems, cyberinfrastructure resources, and user communities; and 6) evaluate and improve the CyberGIS framework through domain science applications and vibrant partnerships to gain better understanding of the complexity of coupled human-natural systems.<br/><br/>The CyberGIS software framework will shift the current paradigm of GIS and associated spatial analysis/modeling software to create scalable and sustainable software ecosystems while achieving groundbreaking scientific advances in understanding coupled human-natural systems that would be impossible otherwise. These advances will, for example, dramatically advance the understanding of disaster preparedness and response and impacts of global climate change. This framework will empower high-performance and collaborative geospatial problem solving and serve as a key driver for the interoperability of international cyberinfrastructure based on broad engagement of user communities related to GIS for both research and education purposes. The project will establish an industrial partnership with the Environmental Systems Research Institute (ESRI), collaborations with the Department of Energy's Oak Ridge National Laboratory (ORNL) and the U.S. Geological Survey (USGS) National Map Project, and international partnerships with several institutions in Australia, China, and the United Kingdom to effectively extend the benefits to the nation and society in significant ways.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047916","fuchsia_tree","SI2-SSI: CyberGIS Software Integration for Sustained Geospatial Innovation"
"1047963","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis","ACI","OFFICE OF MULTIDISCIPLINARY AC|Mechanics of Materials and Str|DYNAMICAL SYSTEMS|COFFES|Software Institutes","9/15/2010","9/7/2010","Shawn Shadden","IL","Illinois Institute of Technology","Standard Grant","Daniel Katz","10/31/2013","$248,356.00 ","","shadden@berkeley.edu","10 West 35th Street","Chicago","IL","10 West 35th Street, Chicago, IL","606163717","3125673035","CSE","1253|1630|7478|7552|8004","1253|1630|7478|7552","$0.00 ","The Lagrangian Coherent Structures (LCS) software elements developed by this project will provide a valuable tool set for fluid mechanics research to extract new discoveries from the vast and growing body of computational and experimental fluid mechanics data. The computation of LCS enables a systematic approach to accurately characterize transport phenomena in complex systems that pose insurmountable challenges to traditional Eulerian approaches. Prior, ah hoc implementations of LCS have already helped in important, real-world challenges including, tracking pollutants in the ocean, developing novel diagnoses and therapies for cardiovascular disease, and helping airplanes to avoid turbulence. We will produce an open LCS software system to provide a modular, extensible and flexible infrastructure to broaden the community of scientists and engineers that benefit from LCS, in problems ranging from fluid dynamics to general dynamical systems. Prof. Shadden will lead the LCS algorithm design and numerical analysis, and Prof. Hart will oversee the package's architectural design and the efficient parallel implementation of its elements.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047963","fuchsia_atom","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis"
"1047956","SI2-SSI: A Productive and Accessible Development Workbench for HPC Applications Using the Eclipse Parallel Tools Platform","ACI","INTERFAC PROCESSES & THERMODYN|SPECIAL PROJECTS - CCF|Software Institutes","10/1/2010","8/28/2013","Jay Alameda","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rudolf Eigenmann","9/30/2014","$1,434,000.00 ","Allen Malony, Marc Snir, Steven Brandt, Gregory Watson","jalameda@ncsa.uiuc.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","1414|2878|8004","1414|2878","$0.00 ","As supercomputers become more powerful, they become more complex. In order to take advantage of the increased power, scientific applications that run on these supercomputers will have to become more complex and will have to take advantage of more processing cores. Even those who are expert at optimizing these applications are quickly being overwhelmed. The Workbench for HPC Applications (W-HPC) project is transforming the way these experts develop, debug, optimize, and run their applications. Using the Eclipse platform, W-HPC provides a robust and portable way to manage computational science and engineering code development for a range of research disciplines. W-HPC also includes a targeted education and outreach program including outreach to minority-serving institutions that will train new users, explain the advantages of using Eclipse-based tools, and encourage users participate in the development of new tools.<br/><br/>The next generation of petascale systems will give unprecedented power to the scientific community as they tackle grand challenge problems. However, in order to take advantage of the huge potential performance improvements, application size and complexity will increase substantially as projects become multi-institutional and multi-disciplinary. The Workbench for HPC Applications project is transforming the way the community develops, debugs, optimizes, and runs its applications. As part of the project, the Eclipse Parallel Tools Platform (Eclipse PTP) is being enhanced. Eclipse PTP provides an open source, robust, portable, and sustainable development environment suitable for use with a broad range of scientific codes. Targeted education and outreach activities are also part of the project. They will train new users, explain the advantages of using Eclipse-based tools, and encourage users participate in the development of new tools.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047956","fuchsia_atom","SI2-SSI: A Productive and Accessible Development Workbench for HPC Applications Using the Eclipse Parallel Tools Platform"
"1047875","Collaborative Research SI2-SSE: Comprehensive Sustained Innovation in Acceleration of Molecular Dynamics Simulation and Analysis on Graphics Processing Units.","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC","10/1/2010","9/15/2010","Ross Walker","CA","University of California-San Diego","Standard Grant","Evelyn M. Goldfield","3/31/2012","$73,469.00 ","","rcw@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8004|1253","9216|9263|1253|1978","$0.00 ","This collaborative pilot project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA Inc. is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using Graphics Processing Units (GPU). By porting MD techniques to GPUs this project is enabling users to both attain substantial increases in their own local calculations without the need for substantial investment in hardware or infrastructure, and to make effective use of GPU acceleration provided by new machines within the NSF supercomputing centers. The software elements being developed and distributed both within the AMBER MD package and as open source libraries are providing critical software infrastructure in support of transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy research.<br/><br/>The software elements being created in this project have very broad impact. For example, the integration of single and multi-GPU acceleration within the AMBER software alone benefits a very large and established national and international user base. Over 8,000 downloads of the AMBER Tools package from unique IP addresses and more than 500 sites which use the AMBER MD engine testify to the scope of the community of researchers this work impacts. Additionally the open source GPU MD acceleration libraries being produced provide broad impact across multiple domains while outreach workshops are helping to train the next generation of scientists not just in the use and potential benefits of GPU MD acceleration libraries but also in modern MD simulation techniques.<br/><br/>This award is co-funded by the Office of Cyberinfrastructure, the Division of Chemistry and the Office of Multidisciplinary Activites of the Directorate of Mathematical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047875","fuchsia_atom","Collaborative Research SI2-SSE: Comprehensive Sustained Innovation in Acceleration of Molecular Dynamics Simulation and Analysis on Graphics Processing Units."
"1047719","SI2-SSE: Adaptive Software for Quantum Chemistry","ACI","OFFICE OF MULTIDISCIPLINARY AC|PROJECTS|Software Institutes","9/15/2010","9/7/2010","So Hirata","FL","University of Florida","Standard Grant","Evelyn M. Goldfield","1/31/2011","$391,079.00 ","","sohirata@illinois.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","1 UNIVERSITY OF FLORIDA, GAINESVILLE, FL","326112002","3523923516","CSE","1253|1978|8004","1253|1978|9216|9263","$0.00 ","The goal of this project is to establish a new paradigm of scientific software, electing quantum chemistry as the domain science. The new software does not have a static, compiled code, but instead consists of an expert system and code generator. At every execution, it analyzes the hardware and application parameters, determines(parallel) algorithms, and implements them for one-time use. This strategy not only allows unprecedented flexibility in algorithm optimization but can also realize ideas that are impossible otherwise. Since the approach makes no assumption about hardware or application, it is more extensible, maintainable, and portable. It is particularly well suited for chemistry, where a variety of molecules and reactions is infinite.<br/><br/>The expected long-term impact of this project is a change in the way scientific and engineering computing software is developed and defined. It promises novel software technology, which simultaneously achieves development efficiency, high product quality, and increased ability to optimize the code and enhance the methodological capabilities, by having no fixed source code. This project also offers unique, interdisciplinary education for chemistry graduate students, which places exceptionally large focus on computing, the field that has been a driving force of the 21st century economy.<br/><br/>This is an award within the solicitation of Software Infrastructure for Sustained Innovation. The award is co-funded by the Office of Cyberinfrastructure, the Division of Chemistry and the Office of Multidisciplinary Activities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047719","fuchsia_atom","SI2-SSE: Adaptive Software for Quantum Chemistry"
"1047586","SI2-SSE: Statistical software for astronomical surveys","AST","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","9/15/2010","9/21/2010","Gutti Babu","PA","Pennsylvania State Univ University Park","Standard Grant","Nigel Sharp","8/31/2014","$450,000.00 ","Eric Feigelson","babu@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","110 Technology Center Building, UNIVERSITY PARK, PA","168027000","8148651372","MPS","1798|1253|8004","1206|7480","$0.00 ","Astronomical research is undergoing a transformation due to the proliferation of publicly available online datasets from all types of telescopes, and a large international effort is already underway to federate these diverse datasets for ready use by astronomers. A particularly important class of data arises from multi-epoch wide-field surveys, which are essentially 'movies' of the sky. These advances in time domain astronomy are crucial for such diverse and important research topics as exoplanet discovery, supernovae and other transients, variable stars, and accretion phenomena. However, most astronomers use only a narrow range of classical statistical methods for interpreting these large datasets. This problem can now be alleviated with the R statistical computing environment and its rapidly growing CRAN add-on packages. This project will bring the R software capabilities into the astronomical research community and introduce specialized astrostatistical methodology into R.<br/><br/>In particular, the research includes two complementary projects. First, CRAN packages will be developed for the analysis of time domain data with irregularly spaced observation times. This is a difficulty rarely encountered in other fields but common in multi-epoch astronomical studies, due to diurnal cycles, satellite orbits, survey cadence patterns, and telescope allocation limitations. Astronomers have developed a wide range of treatments for such problems, but most have not been evaluated statistically or incorporated into widely-used software packages, so a part of this study will be a statistical evaluation of competing methods. Second, the prototype VOStat Web service will be developed into a major tool and integrated into the growing Virtual Astronomical Observatory (VAO) software environment. VOStat will provide dozens of functionalities in many areas of applied statistics: data manipulation and visualization, nonparametric statistics and density estimation, probability density functions, regression and inference, multivariate analysis, clustering and classification, censoring and truncation, time series analysis, spatial point processes and image processing. These achievements will improve the statistical sophistication within the VAO and for thousands of other astronomical studies.<br/><br/>These software developments will improve the statistical analysis of a large number of astronomical research studies every year. Coding within R has the simultaneous advantage of inheriting the large infrastructure of methodology and graphics, itself of enormous value to the entire astronomical community. While the production of CRAN packages directly allows wide dissemination of the code, integrating the code into the VAO software environment through VOStat will make it conveniently accessible to all astronomers. A strong pedagogical component will further encourage less experienced astronomers to learn and use more advanced statistical methods. In addition, the CRAN packages on astrostatistical methods for irregular time series may have value to statisticians, physicists and economists who also might encounter datasets of this type.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047586","chartreuse_atom","SI2-SSE: Statistical software for astronomical surveys"
"1047919","Collaborative Research SI2-SSE: Comprehensive Sustained Innovation in Acceleration of Molecular Dynamics Simulation and Analysis on Graphical Processing Units.","ACI","PROJECTS","10/1/2010","9/15/2010","Adrian Roitberg","FL","University of Florida","Standard Grant","Evelyn M. Goldfield","3/31/2012","$50,000.00 ","","roitberg@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","1 UNIVERSITY OF FLORIDA, GAINESVILLE, FL","326112002","3523923516","CSE","1978","9216|9263|1253|1978","$0.00 ","This collaborative pilot project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA Inc. is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using Graphics Processing Units (GPU). By porting MD techniques to GPUs this project is enabling users to both attain substantial increases in their own local calculations without the need for substantial investment in hardware or infrastructure, and to make effective use of GPU acceleration provided by new machines within the NSF supercomputing centers. The software elements being developed and distributed both within the AMBER MD package and as open source libraries are providing critical software infrastructure in support of transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy research.<br/><br/>The software elements being created in this project have very broad impact. For example, the integration of single and multi-GPU acceleration within the AMBER software alone benefits a very large and established national and international user base. Over 8,000 downloads of the AMBER Tools package from unique IP addresses and more than 500 sites which use the AMBER MD engine testify to the scope of the community of researchers this work impacts. Additionally the open source GPU MD acceleration libraries being produced provide broad impact across multiple domains while outreach workshops are helping to train the next generation of scientists not just in the use and potential benefits of GPU MD acceleration libraries but also in modern MD simulation techniques.<br/><br/>This award is co-funded by the Office of Cyberinfrastructure, the Division of Chemistry and the Office of Multidisciplinary Activites of the Directorate of Mathematical Sciences.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1047919","fuchsia_atom","Collaborative Research SI2-SSE: Comprehensive Sustained Innovation in Acceleration of Molecular Dynamics Simulation and Analysis on Graphical Processing Units."
"1339768","SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards","ACI","Software Institutes|EarthCube","10/1/2013","9/16/2013","Charles Connor","FL","University of South Florida","Standard Grant","Rajiv Ramnath","9/30/2017","$194,869.00 ","","cbconnor@usf.edu","3702 Spectrum Blvd.","Tampa","FL","3702 Spectrum Blvd., Tampa, FL","336129446","8139742897","CSE","8004|8074","7433|8009","$0.00 ","This project is focused on creating and upgrading software infrastructure for a large community of scientists engaged in volcanology research and associated hazard analysis. Specifically, the project will reengineer three widely used tools (TITAN2D - block and ash flows, TEPHRA and Puff - ash transport and dispersal) and develop support for workflows that use these tools to analyze risk from volcanic hazards. Reengineering will encompass modularization so researchers may easily experiment with different modeling approaches, incorporation of techniques to make the tools efficient on new computing architectures like GPUs and many-core chips. The workflows are intended to tackle the challenges of managing complex and often large data flows associated with these tools in validation processes and in probabilistic inference based on the outcomes of the modeling. The tools and workflows will be made available using the popular vhub.org platform. This project will help provide a standard well managed hardware/software platform and approaches to standardize the documentation associated with input data, source code, and output data. This will ensure that model calculations are reproducible.<br/><br/>The wider use of these high fidelity tools and their use in mitigating hazards is likely to have a significant effect on hazard analysis and management. The project will also engage in several major workshops and in training activities. Project personnel will also engage in the Earthcube initiative - popularizing computational methodologies, online access and dissemination mechanisms through the VHub platform.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339768","fuchsia_atom","SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards"
"1339804","SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|CYBERINFRASTRUCTURE|Software Institutes|CDS&E","10/1/2013","9/4/2014","Sohrab Ismail-Beigi","CT","Yale University","Continuing grant","Rajiv Ramnath","9/30/2018","$1,380,247.00 ","","sohrab.ismail-beigi@yale.edu","Office of Sponsored Projects","New Haven","CT","Office of Sponsored Projects, New Haven, CT","65208327","2037854689","CSE","1253|1712|7231|8004|8084","7433|7569|8009|8084|9216|9263","$0.00 ","Computer simulation plays a central role in helping us understand, predict, and engineer the physical and chemical properties of technological materials systems such as semiconductor devices, photovoltaic systems, chemical reactions and catalytic behavior. Despite significant progress in performing realistic simulations in full microscopic detail, some problems are currently out of reach: two examples are the modeling of electronic devices with multiple functional parts based on new materials such as novel low power computer switches that would revolutionize the Information Technology industry, and the photovoltaic activity of complex interfaces between polymers and inorganic nanostructures that would enhance US energy self-reliance. The research program of this collaborative software institute aims to create an open and effective scientific software package that can make efficient use of cutting-edge high performance computers (HPC) to solve challenging problems involving the physics and chemistry of materials. By having such software available, this software initiative will have multiple broad impacts. First, the community of materials scientists will be able to study next-generation problems in materials physics and chemistry, and computer science advances that enable the software will be demonstrated and made accessible for both communities which will help cross-fertilize further such collaborative efforts. Second, the capability of simulating and engineering more complex materials systems and technological devices could play a role in helping the US continue is competitive edge in science, technology, and education. Third, through training of young scientists, direct outreach to the broader scientific community through workshops and conferences, and educational programs ranging from secondary to graduate levels, the power, importance, and capabilities of computational modeling, materials science, and computer science methodologies that enable the science will be communicated to a broad audience. Finally, by enabling the refinement of existing materials systems as well as discovery of new materials systems, the resulting scientific advances can help broadly impact society via technological improvements: in terms of the two examples provided above, (a) the successful design of new electronic device paradigms helps significantly advance the digital revolution by permitting the introduction of smaller, more efficient, and more capable electronic circuits and information processing systems, and (b) successful creation of inexpensive, easy-to-fabricate, and durable photovoltaic materials and devices can lead to cleaner forms of energy production while reducing reliance on fossil fuels.<br/><br/>The technical goal is to greatly enhance the open software tool OPENATOM to advance discovery in nanoscience and technology. OPENATOM will be delivered as a open, robust and validated software package capable of utilizing HPC architectures efficiently to describe the electronic structure of complex materials systems from first principles. In terms of describing electronic ground-states, OPENATOM will be enhanced by features such as improved configurational sampling methods, hybrid density functionals, and incorporation of fast super-soft pseudopotential techniques. In addition, the team will incorporate the many-body GW-BSE approach for electronic excitations that permits accurate computation of electronic energy levels, optical absorption and emission, and luminescence. Ultimately, such an extensible software framework will permit accurate electronic structure computations to employ effectively future HPC platforms with 10,000,000 cores.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339804","fuchsia_tree","SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems"
"1339844","SI2-SSE: Multiscale Software for Quantum Simulations in Materials Design, Nano Science and Technology","ACI","DMR SHORT TERM SUPPORT|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","9/1/2013","8/29/2013","Jerzy Bernholc","NC","North Carolina State University","Standard Grant","Rajiv Ramnath","8/31/2016","$500,000.00 ","Carl Kelley, Wenchang Lu, Miroslav Hodak","bernholc@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","CAMPUS BOX 7514, RALEIGH, NC","276957514","9195152444","CSE","1712|1253|8004","7433|8005|7569|9216|9263","$0.00 ","The emergence of petascale computing platforms brings unprecedented opportunities for transformational research through simulation. However, future breakthroughs will depend on the availability of high-end simulation software, which will fully utilize these unparalleled resources and provide the long-sought third avenue for scientific progress in key areas of national interest. This award will deliver a set of open source petascale quantum simulation tools in the broad areas of materials design, nano science and nanotechnology. Materials prediction and design are key aspects to the recently created Materials Genome initiative, which seeks to ""deploy advanced materials at least twice as fast, at a fraction of the cost."" Computational materials design is the critical aspect of that initiative, which relies on computation guiding experiments. The outcomes of the latter will in turn lead to follow-up computation in an iterative feedback loop. Nanoscience, which studies properties of materials and processes on fundamental scale of nanometers, promises development of materials and systems with radically new properties. However, the nanoscale properties are hard to measure and even harder to predict theoretically. Only simulations that can fully account for the complexity and variability at that fundamental scale stand a chance of predicting and utilizing the macroscopic properties that emerge. This truly requires petascale resources and efficient petascale software tools.<br/><br/>This award will develop software tools build on the real-space multigrid (RMG) software suite and distribute them to the national user community. The RMG code already scales to 128,000 CPU cores and 18,000 GPU nodes. The award will further enhance RMG through development of new iterative methods with improved convergence, optimization of additional modules for existing and new petascale computing platforms, and creation of ease-to-use interfaces to the main codes. Workshops in RMG usage will be conducted at XSEDE workshops and other meetings of NSF supercomputing centers. RMG will be distributed through a web portal, which will also contain user forums and video tutorials, recorded at live user sessions. A library of representative examples for the main petascale platforms will be maintained. RMG will enable quantum simulations of unprecedented size, enabling studies of the building blocks of functional nano or bio-nano structures, which often involve thousands of atoms and must be described with the requisite fidelity. The development of petascale quantum simulation software and its user community will lead to cross-fertilization of ideas both within and across fields. Students and postdocs trained in this area will have significant opportunities for advancement and making substantial impact on their own.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339844","fuchsia_atom","SI2-SSE: Multiscale Software for Quantum Simulations in Materials Design, Nano Science and Technology"
"1339785","SI2-SSE: Development of Cassandra, A General, Efficient and Parallel Monte Carlo Multiscale Modeling Software Platform for Materials Research","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|DMR SHORT TERM SUPPORT","10/1/2013","3/11/2015","Edward Maginn","IN","University of Notre Dame","Standard Grant","Rajiv Ramnath","9/30/2016","$395,133.00 ","Jindal Shah","ed@nd.edu","940 Grace Hall","NOTRE DAME","IN","940 Grace Hall, NOTRE DAME, IN","465565708","5746317432","CSE","1253|8004|1712","7433|8005|7569|9216|9263","$0.00 ","The properties of materials are the result of the interactions between the atoms that make up these materials. These properties can now be predicted with great accuracy, even for materials that have not yet existed in nature, by using advanced computational methods to study how the constituent atoms of the materials interact with one another and their environment. This field relies upon the existence of sophisticated software packages that enable researchers to conduct these simulations. There are two general approaches for simulating bulk materials: molecular dynamics and Monte Carlo, each of which is appropriate for certain problems. There are many molecular dynamics software packages available but almost no general purpose Monte Carlo codes. This project seeks to develop an efficient, general-purpose open source Monte Carlo code called Cassandra.<br/><br/>To do this, the academic Monte Carlo code developed in the PI's group will be extended and enhanced. The code will be capable of simulating any type of molecule in bulk and heterogeneous environments. The code will contain a wide range of advanced features, making it useful for a range of problems. By providing a general purpose code to the research community and establishing a mechanism whereby users can add their own features and extend the code, this project will have a broad impact on the research community. It will enable non-experts to use Monte Carlo simulations to study new problems. It will enable experienced molecular modelers to utilize and contribute features to a single optimized and validated code, thereby alleviating the time and expense associated with developing specialized codes for individual applications. Because the code will be used in teaching and workshops, materials will be made available to educators to use the code in the classroom when teaching courses such as thermodynamics, molecular modeling and statistical mechanics.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339785","fuchsia_atom","SI2-SSE: Development of Cassandra, A General, Efficient and Parallel Monte Carlo Multiscale Modeling Software Platform for Materials Research"
"1339793","Collaborative Research: SI2-SSI: The Community-Driven Big-CZ Software System for Integration and Analysis of Bio- and Geoscience Data in the Critical Zone","ACI","Software Institutes","12/1/2013","9/20/2013","Ilya Zaslavsky","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","11/30/2016","$433,911.00 ","David Valentine","zaslavsk@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8004","7433|8009","$0.00 ","The Critical Zone (CZ) science community takes as its charge the effort to integrate theory, models and data from the multitude of disciplines collectively studying processes on the Earth's surface. The Critical Zone is Earth's permeable near-surface layer - from the atmosphere at the vegetation's canopy to the lower boundary of actively circulating groundwaters. The Critical Zone was a term coined by the National Research Council's Basic Research Opportunities in the Earth Sciences (BROES) Report (2001) to highlight the imperative for a new approach to thoroughly multi-disciplinary research on the zone of the Earth?s surface that is critical to sustaining terrestrial life on our planet. In January 2013, 103 members of the CZ community met for the CZ-EarthCube Domain Workshop (NSF Award #1252238) to prioritize the CZ community's key science drivers, key computational and information technology (""cyber"") challenges and key cyber needs. They identified that the central scientific challenge of the critical zone science community is to develop a ""grand unifying theory"" of the critical zone through a theory-model-data fusion approach. Work participants unanimously described that the key missing need of this approach was a future cyberinfrastructure for seamless 4D visual exploration of the integrated knowledge (data, model outputs and interpolations) from all the bio and geoscience disciplines relevant to critical zone structure and function, similar to today?s ability to easily explore historical satellite imagery and photographs of the earth's surface using Google Earth. This project takes the first ""BiG"" steps toward answering that need.<br/><br/>The overall goal of this project is to co-develop with the CZ science and broader community, including natural resource managers and stakeholders, a web-based integration and visualization environment for joint analysis of cross-scale bio and geoscience processes in the critical zone (BiG CZ), spanning experimental and observational designs. Our Project Objectives are to: (1) Engage the CZ and broader community to co-develop and deploy the BiG CZ software stack; (2) Develop the BiG CZ Portal web application for intuitive, high-performance map-based discovery, visualization, access and publication of data by scientists, resource managers, educators and the general public; (3) Develop the BiG CZ Toolbox to enable cyber-savvy CZ scientists to access BiG CZ Application Programming Interfaces (APIs); and (4) Develop the BiG CZ Central software stack to bridge data systems developed for multiple critical zone domains into a single metadata catalog. The entire BiG CZ Software system will be developed on public repositories as a modular suite of fully open source software projects. It will be built around a new Observations Data Model Version 2.0 (ODM2) that is being developed by members of the BiG CZ project team, with community input, under separate funding (NSF Award #1224638).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339793","fuchsia_atom","Collaborative Research: SI2-SSI: The Community-Driven Big-CZ Software System for Integration and Analysis of Bio- and Geoscience Data in the Critical Zone"
"1339820","SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI","ACI","SPECIAL PROJECTS - CCF|Software Institutes","9/1/2013","8/29/2013","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Rajiv Ramnath","8/31/2016","$347,216.00 ","","bosilca@eecs.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","1 CIRCLE PARK, KNOXVILLE, TN","379960003","8659743466","CSE","2878|8004","7433|8005|9150","$0.00 ","High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today's architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.<br/>The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.<br/>MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339820","fuchsia_atom","SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI"
"1342076","SI2-SSI: REAL-TIME LARGE-SCALE PARALLEL INTELLIGENT CO2 DATA ASSIMILATION SYSTEM","ACI","ADVANCES IN BIO INFORMATICS|INTERFAC PROCESSES & THERMODYN|ICER|Software Institutes","9/1/2012","3/6/2015","Anna Michalak","DC","Carnegie Institution of Washington","Continuing grant","Rajiv Ramnath","8/31/2016","$1,667,410.00 ","","michalak@stanford.edu","1530 P ST NW","WASHINGTON","DC","1530 P ST NW, WASHINGTON, DC","200051910","2023876400","CSE","1165|1414|7699|8004","","$0.00 ","Intellectual Merit: This proposal seeks to address this need by creating a state-of-the-art autonomous software platform for real-time integration of in-situ and satellite-based atmospheric CO2 measurements within a Data Assimilation (DA) system for producing estimates of global land and oceanic CO2 exchange at weekly to bi-weekly intervals. The proposed software infrastructure will be capable of autonomous processing of large volumes of data through a multi-stage pipeline, without the delays conventionally associated with such processing. Within the DA component, we will provide options for multiple DA algorithms for estimating global CO2 exchange. Users will, for the first time, have the capability to use these multiple methods as part of a single system for comparing estimates of CO2 exchange, and to obtain an improved understanding of the relative advantages of the various DA methods. As part of the analysis component of the software, we will build a carbon-climate surveillance system by drawing from a range of techniques in pattern recognition and high-dimensional statistical inference. This system will be able to detect and analyze localized variations in CO2 exchange within any user-specified spatio-temporal window. In addition, summaries of the CO2 exchange will be provided at annual and monthly temporal scales for continents and countries.<br/><br/>Broader Impacts: This software can be used by researchers and governmental institutions for evaluating both the natural components of the carbon cycle and anthropogenic carbon emissions, as well as in the design of new satellites for improved monitoring of CO2. All data and software will be publicly available and open-source development platforms will be used whenever possible. The algorithm prototypes developed as part of this project will be used in undergraduate and graduate courses at the University of Michigan, and will be made available online for educators at other institutions. Finally, the project will train three graduate students, with a focus on developing their cross-disciplinary skills in the field of Earth science, statistics, computer science, and atmospheric science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1342076","fuchsia_tree","SI2-SSI: REAL-TIME LARGE-SCALE PARALLEL INTELLIGENT CO2 DATA ASSIMILATION SYSTEM"
"1339715","SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|SPECIAL PROJECTS - CCF|CYBERINFRASTRUCTURE|Software Institutes|CDS&E","10/1/2013","9/10/2014","Laxmikant Kale","IL","University of Illinois at Urbana-Champaign","Continuing grant","Rajiv Ramnath","9/30/2018","$2,383,226.00 ","","kale@uiuc.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","1253|1712|2878|7231|8004|8084","7433|7569|8009|8084|9216|9263","$0.00 ","Computer simulation plays a central role in helping us understand, predict, and engineer the physical and chemical properties of technological materials systems such as semiconductor devices, photovoltaic systems, chemical reactions and catalytic behavior. Despite significant progress in performing realistic simulations in full microscopic detail, some problems are currently out of reach: two examples are the modeling of electronic devices with multiple functional parts based on new materials such as novel low power computer switches that would revolutionize the Information Technology industry, and the photovoltaic activity of complex interfaces between polymers and inorganic nanostructures that would enhance US energy self-reliance. The research program of this collaborative software institute aims to create an open and effective scientific software package that can make efficient use of cutting-edge high performance computers (HPC) to solve challenging problems involving the physics and chemistry of materials. By having such software available, this software initiative will have multiple broad impacts. First, the community of materials scientists will be able to study next-generation problems in materials physics and chemistry, and computer science advances that enable the software will be demonstrated and made accessible for both communities which will help cross-fertilize further such collaborative efforts. Second, the capability of simulating and engineering more complex materials systems and technological devices could play a role in helping the US continue is competitive edge in science, technology, and education. Third, through training of young scientists, direct outreach to the broader scientific community through workshops and conferences, and educational programs ranging from secondary to graduate levels, the power, importance, and capabilities of computational modeling, materials science, and computer science methodologies that enable the science will be communicated to a broad audience. Finally, by enabling the refinement of existing materials systems as well as discovery of new materials systems, the resulting scientific advances can help broadly impact society via technological improvements: in terms of the two examples provided above, (a) the successful design of new electronic device paradigms helps significantly advance the digital revolution by permitting the introduction of smaller, more efficient, and more capable electronic circuits and information processing systems, and (b) successful creation of inexpensive, easy-to-fabricate, and durable photovoltaic materials and devices can lead to cleaner forms of energy production while reducing reliance on fossil fuels.<br/><br/>The technical goal is to greatly enhance the open software tool OPENATOM to advance discovery in nanoscience and technology. OPENATOM will be delivered as a open, robust and validated software package capable of utilizing HPC architectures efficiently to describe the electronic structure of complex materials systems from first principles. In terms of describing electronic ground-states, OPENATOM will be enhanced by features such as improved configurational sampling methods, hybrid density functionals, and incorporation of fast super-soft pseudopotential techniques. In addition, the team will incorporate the many-body GW-BSE approach for electronic excitations that permits accurate computation of electronic energy levels, optical absorption and emission, and luminescence. Ultimately, such an extensible software framework will permit accurate electronic structure computations to employ effectively future HPC platforms with 10,000,000 cores.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339715","fuchsia_tree","SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems"
"1339707","SI2-SSE: Developing Sustainable Software Elements to Support the Growing Field of Public Participation in Scientific Research","ACI","AISL|CROSS-DIRECTORATE ACTIV PROGR|Software Institutes","11/1/2013","8/8/2013","Gregory Newman","CO","Colorado State University","Standard Grant","Rajiv Ramnath","10/31/2016","$493,076.00 ","Melinda Laituri, Stacy Lynn","Gregory.Newman@ColoState.Edu","601 S Howes St","Fort Collins","CO","601 S Howes St, Fort Collins, CO","805232002","9704916355","CSE","7259|1397|8004","7433|7477|8009","$0.00 ","Across the globe, citizen science projects are becoming increasingly poised to address social and environmental challenges and answer broad scientific questions. Although rapidly increasing in number, these projects need easy-to-use software tools for data management, analysis, and visualization to be successful. This project transforms how citizen science projects unfold locally, regionally, and globally by creating software that supports the full spectrum of project activities. It empowers projects to ask and answer their own local questions while contributing data critical to larger-scale issues. These tools will allow projects to announce training events; track volunteers; create datasheets; enter, review, analyze, and visualize data; publish reports; discover resources; integrate data; and ensure that data are contributed to repositories (e.g., DataONE, NEON, GBIF, HydroShare, and EOL). Tools will be made available to citizen science projects and will be delivered as reusable software elements for use in existing websites; as website features on CitSci.org; and as Application Programming Interface (API) services and mobile applications.<br/><br/>The tools will expand the national reach, local appeal, computational abilities, visualization techniques, statistical analysis capabilities, and interoperability of the nations? cyber-infrastructure. Using participatory design and agile methods, the project will: (1) develop reusable software elements that citizen science organizations can embed into their own websites, (2) harden and expand the functionality and capabilities of CitSci.org through new website features, and (3) extend the APIs of CitSci.org and develop associated mobile applications to increase system and tool interoperability. The target user communities will include citizen science project coordinators. It will deliver customizable tools and services related to all project activities and engage projects across a wide array of disciplines. Project coordinators will be able to customize all tools developed to suit their specific project needs. Adoption and use of the tools developed will create a cyber-ready workforce capable of collecting, contributing, and applying high quality ecological, geophysical, social, and human health related observations to solve real-world problems. These broader impacts will help the citizen science community better understand effective models of public engagement to ensure more impactful application of citizen science to societal challenges.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339707","fuchsia_atom","SI2-SSE: Developing Sustainable Software Elements to Support the Growing Field of Public Participation in Scientific Research"
"1339772","SI2-SSI: SAGEnext: Next Generation Integrated Persistent Visualization and Collaboration Services for Global Cyberinfrastructure","ACI","Software Institutes","10/1/2013","9/16/2013","Jason Leigh","IL","University of Illinois at Chicago","Continuing grant","Daniel Katz","6/30/2014","$1,828,077.00 ","Maxine Brown, Luc Renambot","leighj@hawaii.edu","809 S MARSHFIELD","Chicago","IL","809 S MARSHFIELD, Chicago, IL","606124305","3129962862","CSE","8004","7433|8009","$0.00 ","Cyberinfrastructure runs the gamut - from computing, networks, data stores, instruments, observatories, and sensors, to software and application codes - and promises to enable research at unprecedented scales, complexity, resolution, and accuracy. However, it is the research community that must make sense of all the data being amassed, so the SAGEnext (Scalable Adaptive Graphics Environment) framework is an innovative user-centered platform that connects people to their data and colleagues, locally and remotely, via tiled display walls, creating a portal, or wide-aperture ""digital lens,"" with which to view their data and one another. It enables Cyber-Mashups, or the ability to juxtapose and integrate information from multiple sources in a variety of resolutions, as easily as the Web makes access to lower-resolution images today. <br/><br/>SAGEnext expands on a vibrant partnership among national and international universities, supercomputer centers, government laboratories and industry; 100 institutions worldwide use the current version of SAGE. For computational scientists, from such diverse fields as biology, earth science, genomics, or physics, SAGEnext will transform the way they manage the scale and complexity of their data. For computer scientists, SAGEnext is a platform for conducting research in human-computer interaction, cloud computing, and advanced networking. SAGEnext capabilities, integrating visualization application codes, cloud documents, stereo 3D, and new user-interaction paradigms, is unprecedented and heretofore not available, and will have a transformative effect on data exploration and collaboration, making cyberinfrastructure more accessible to end users, in both the laboratory and in the classroom. SAGEnext, integrated with advanced cyberinfrastructure tools, will transform the way today's scientists and future scientists manage the scale and complexity of their data, enabling them to more rapidly address problems of national priority - such as global climate change or homeland security - which benefits all mankind. These same tools can better communicate scientific concepts to public policy and government officials, and via museum exhibitions, to the general public.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339772","fuchsia_tree","SI2-SSI: SAGEnext: Next Generation Integrated Persistent Visualization and Collaboration Services for Global Cyberinfrastructure"
"1339881","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments","ACI","Software Institutes","10/1/2013","9/13/2013","Charles Hansen","UT","University of Utah","Standard Grant","Rajiv Ramnath","9/30/2016","$282,225.00 ","","hansen@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","75 S 2000 E, SALT LAKE CITY, UT","841128930","8015816903","CSE","8004","7433|8009|9150","$0.00 ","Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this work. <br/><br/>To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. <br/><br/>This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339881","fuchsia_atom","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments"
"1339723","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","ACI","PHYSICAL OCEANOGRAPHY|SPECIAL PROJECTS - CISE|SPECIAL PROJECTS - CCF|Software Institutes|EarthCube","10/1/2014","8/26/2014","Richard Luettich","NC","University of North Carolina at Chapel Hill","Standard Grant","Rajiv Ramnath","9/30/2018","$759,047.00 ","","rick_luettich@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","104 AIRPORT DR STE 2200, CHAPEL HILL, NC","275991350","9199663411","CSE","1610|1714|2878|8004|8074","7433|8009","$0.00 ","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339723","fuchsia_atom","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling"
"1339841","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis","ACI","Software Institutes|CDS&E","10/1/2013","9/20/2013","Shawn Shadden","IL","Illinois Institute of Technology","Standard Grant","Daniel Katz","12/31/2013","$306,276.00 ","","shadden@berkeley.edu","10 West 35th Street","Chicago","IL","10 West 35th Street, Chicago, IL","606163717","3125673035","CSE","8004|8084","7433|8009|8084","$0.00 ","The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. <br/><br/>This project is addressing the above roadblocks through the following aims: 1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community. Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339841","fuchsia_atom","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis"
"1339581","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics","ACI","STELLAR ASTRONOMY & ASTROPHYSC|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","1/1/2014","8/29/2013","Lars Bildsten","CA","University of California-Santa Barbara","Standard Grant","Rajiv Ramnath","12/31/2016","$163,586.00 ","","bildsten@itp.ucsb.edu","Office of Research","SANTA BARBARA","CA","Office of Research, SANTA BARBARA, CA","931062050","8058934188","CSE","1215|1253|8004","7433|8005|1206","$0.00 ","As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of ""time domain astronomy"". The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011. MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.<br/><br/>This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community. MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation. This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339581","fuchsia_atom","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics"
"1339834","Collaborative Research: SI2-SSI: The Community-Driven BiG CZ Software System for Integration and Analysis of Bio- and Geoscience Data in the Critical Zone","ACI","GEOBIOLOGY & LOW TEMP GEOCHEM|EAR|ECOSYSTEM STUDIES|EarthCube|Software Institutes|CZO: CRITICAL ZONE OBSER SOLIC","12/1/2013","9/20/2013","Anthony Aufdenkampe","PA","Stroud Water Research Center","Standard Grant","Rajiv Ramnath","11/30/2016","$1,366,089.00 ","Kerstin Lehnert, Jeffery Horsburgh, Robert Cheetham, Emilio Mayorga","aufdenkampe@stroudcenter.org","970 Spencer Road","Avondale","PA","970 Spencer Road, Avondale, PA","193119514","6102682153","CSE","7295|6898|1181|8074|8004|7693","7433|8009","$0.00 ","The Critical Zone (CZ) science community takes as its charge the effort to integrate theory, models and data from the multitude of disciplines collectively studying processes on the Earth's surface. The Critical Zone is Earth's permeable near-surface layer - from the atmosphere at the vegetation's canopy to the lower boundary of actively circulating groundwaters. The Critical Zone was a term coined by the National Research Council's Basic Research Opportunities in the Earth Sciences (BROES) Report (2001) to highlight the imperative for a new approach to thoroughly multi-disciplinary research on the zone of the Earth?s surface that is critical to sustaining terrestrial life on our planet. In January 2013, 103 members of the CZ community met for the CZ-EarthCube Domain Workshop (NSF Award #1252238) to prioritize the CZ community's key science drivers, key computational and information technology (""cyber"") challenges and key cyber needs. They identified that the central scientific challenge of the critical zone science community is to develop a ""grand unifying theory"" of the critical zone through a theory-model-data fusion approach. Work participants unanimously described that the key missing need of this approach was a future cyberinfrastructure for seamless 4D visual exploration of the integrated knowledge (data, model outputs and interpolations) from all the bio and geoscience disciplines relevant to critical zone structure and function, similar to today?s ability to easily explore historical satellite imagery and photographs of the earth's surface using Google Earth. This project takes the first ""BiG"" steps toward answering that need.<br/><br/>The overall goal of this project is to co-develop with the CZ science and broader community, including natural resource managers and stakeholders, a web-based integration and visualization environment for joint analysis of cross-scale bio and geoscience processes in the critical zone (BiG CZ), spanning experimental and observational designs. Our Project Objectives are to: (1) Engage the CZ and broader community to co-develop and deploy the BiG CZ software stack; (2) Develop the BiG CZ Portal web application for intuitive, high-performance map-based discovery, visualization, access and publication of data by scientists, resource managers, educators and the general public; (3) Develop the BiG CZ Toolbox to enable cyber-savvy CZ scientists to access BiG CZ Application Programming Interfaces (APIs); and (4) Develop the BiG CZ Central software stack to bridge data systems developed for multiple critical zone domains into a single metadata catalog. The entire BiG CZ Software system will be developed on public repositories as a modular suite of fully open source software projects. It will be built around a new Observations Data Model Version 2.0 (ODM2) that is being developed by members of the BiG CZ project team, with community input, under separate funding (NSF Award #1224638).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339834","fuchsia_atom","Collaborative Research: SI2-SSI: The Community-Driven BiG CZ Software System for Integration and Analysis of Bio- and Geoscience Data in the Critical Zone"
"1339649","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)","ACI","CROSS-EF ACTIVITIES|Software Institutes","10/1/2013","8/29/2013","Borries Demeler","TX","University of Texas Health Science Center San Antonio","Standard Grant","Rajiv Ramnath","9/30/2018","$600,065.00 ","","demeler@biochem.uthscsa.edu","7703 FLOYD CURL DR","SAN ANTONIO","TX","7703 FLOYD CURL DR, SAN ANTONIO, TX","782293901","2105672340","CSE","7275|8004","7433|8009","$0.00 ","Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources' complexities. Given Science Gateways' demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.<br/><br/>SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP's adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339649","fuchsia_atom","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)"
"1339824","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis","ACI","CDS&E|Software Institutes","10/1/2013","9/20/2013","Alison Marsden","CA","University of California-San Diego","Standard Grant","Daniel Katz","10/31/2015","$1,237,578.00 ","","amarsden@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8084|8004","7433|8009|8084","$0.00 ","The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. <br/><br/>This project is addressing the above roadblocks through the following aims: 1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community. Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339824","fuchsia_atom","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis"
"1339708","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users","ACI","Software Institutes","10/1/2013","9/13/2013","Robert McLay","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","9/30/2016","$233,046.00 ","","mclay@cfdlab.ae.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","101 E. 27th Street, Suite 5.300, Austin, TX","787121532","5124716424","CSE","8004","7433|8005","$0.00 ","This research addresses two important questions: what software do researchers actually use on high-end computers, and how successful are they in their efforts to use it? It is a plan to improve our understanding of individual users' software needs, then leverage that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The signature product, UTWrangler, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe. For the first time, complete, accurate, detailed, and continuous ground truth information about software needs, trends, and issues at the level of the individual job are being delivered.<br/><br/>UTWrangler will instrument, monitor, and analyze individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. UTWrangler's prototypes prove its value and future impact: simplifying end users' workflows; improving support, training and documentation; saving money; and helping administrators prioritize maintenance of their large base of installed software. UTWrangler will build on the capabilities of its prototypes, providing a robust, sustainable, second generation mechanism that will help the computational research community make the most effective use of limited computing cycles and labor hours. And UTWrangler will mitigate the difficulties new users encounter, reporting configuration problems as soon as jobs begin, and identifying opportunities to improve documentation, education and outreach programs.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339708","fuchsia_atom","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users"
"1339863","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments","ACI","Software Institutes","10/1/2013","8/6/2014","Paul Navratil","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","9/30/2016","$1,198,122.00 ","","pnav@tacc.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","101 E. 27th Street, Suite 5.300, Austin, TX","787121532","5124716424","CSE","8004","7433|8009","$0.00 ","Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this work. <br/><br/>To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. <br/><br/>This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339863","fuchsia_atom","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments"
"1339765","SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards","ACI","PETROLOGY AND GEOCHEMISTRY|DEEP EARTH PROCESSES SECTION|Software Institutes|Front in Earth Sys Dynamics|EarthCube","10/1/2013","8/25/2015","Abani Patra","NY","SUNY at Buffalo","Continuing grant","Rajiv Ramnath","9/30/2018","$1,300,921.00 ","Marcus Bursik, Greg Valentine, Tevfik Kosar, Matthew Jones","abani@buffalo.edu","402 Crofts Hall","Buffalo","NY","402 Crofts Hall, Buffalo, NY","142607016","7166452634","CSE","1573|7571|8004|8016|8074","7433|8009","$0.00 ","This project is focused on creating and upgrading software infrastructure for a large community of scientists engaged in volcanology research and associated hazard analysis. Specifically, the project will reengineer three widely used tools (TITAN2D - block and ash flows, TEPHRA and Puff - ash transport and dispersal) and develop support for workflows that use these tools to analyze risk from volcanic hazards. Reengineering will encompass modularization so researchers may easily experiment with different modeling approaches, incorporation of techniques to make the tools efficient on new computing architectures like GPUs and many-core chips. The workflows are intended to tackle the challenges of managing complex and often large data flows associated with these tools in validation processes and in probabilistic inference based on the outcomes of the modeling. The tools and workflows will be made available using the popular vhub.org platform. This project will help provide a standard well managed hardware/software platform and approaches to standardize the documentation associated with input data, source code, and output data. This will ensure that model calculations are reproducible.<br/><br/>The wider use of these high fidelity tools and their use in mitigating hazards is likely to have a significant effect on hazard analysis and management. The project will also engage in several major workshops and in training activities. Project personnel will also engage in the Earthcube initiative - popularizing computational methodologies, online access and dissemination mechanisms through the VHub platform.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339765","fuchsia_tree","SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards"
"1339690","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users","ACI","Software Institutes","10/1/2013","9/13/2013","Mark Fahey","TN","University of Tennessee Knoxville","Standard Grant","Rajiv Ramnath","4/30/2016","$259,931.00 ","","markrfahey@uchicago.edu","1 CIRCLE PARK","KNOXVILLE","TN","1 CIRCLE PARK, KNOXVILLE, TN","379960003","8659743466","CSE","8004","7433|8005|9150","$0.00 ","This research addresses two important questions: what software do researchers actually use on high-end computers, and how successful are they in their efforts to use it? It is a plan to improve our understanding of individual users' software needs, then leverage that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The signature product, UTWrangler, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe. For the first time, complete, accurate, detailed, and continuous ground truth information about software needs, trends, and issues at the level of the individual job are being delivered.<br/><br/>UTWrangler will instrument, monitor, and analyze individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. UTWrangler's prototypes prove its value and future impact: simplifying end users' workflows; improving support, training and documentation; saving money; and helping administrators prioritize maintenance of their large base of installed software. UTWrangler will build on the capabilities of its prototypes, providing a robust, sustainable, second generation mechanism that will help the computational research community make the most effective use of limited computing cycles and labor hours. And UTWrangler will mitigate the difficulties new users encounter, reporting configuration problems as soon as jobs begin, and identifying opportunities to improve documentation, education and outreach programs.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339690","fuchsia_atom","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users"
"1339835","SI2-SSE: E-SDMS: Energy Simulation Data Management System Software","ACI","Software Institutes|CDS&E","10/1/2013","9/11/2013","K. Selcuk Candan","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","9/30/2016","$499,699.00 ","Maria Luisa Sapino","candan@asu.edu","ORSPA","TEMPE","AZ","ORSPA, TEMPE, AZ","852816011","4809655479","CSE","8004|8084","7433|8005","$0.00 ","The building sector was responsible for nearly half of CO2 emissions in US in 2009. According to the US Energy Information Administration, buildings consume more energy than any other sector, with 48.7% of the overall energy consumption, and building energy consumption is projected to grow faster than the consumptions of industry and transportation sectors. As a response to this, by 2030 only 18% of the US building stock is expected to be relying on the current energy management technologies, with the rest either having been retrofitted or designed from the ground up using smart and cleaner energy technologies. These building energy management systems (BEMSs) need to integrate large volumes of data, including (a) continuously collected heating, ventilation, and air conditioning (HVAC) sensor and actuation data, (b) other sensory data, such as occupancy, humidity, lighting levels, air speed and quality, (c) architectural, mechanical, and building automation system configuration data for these buildings, (d) local whether and GIS data that provide contextual information, as well as (e) energy price, consumption, and cost data from electricity (such as smart grid) and gas utilities. In theory, these data can be leveraged from the initial design and/or retrofitting of buildings with data driven building optimization (including the evaluation of the building location, orientation, and alternative energy-saving strategies) to total cost of ownership (TCOs) simulation tools and day-to-day operation decisions. In practice, however, because of the size and complexity of the data, the varying spatial and temporal scales at which the key processes operate, (a) creating models to support such simulations, (b) executing simulations that involve 100s of inter-dependent parameters spanning multiple spatio-temporal frames, affected by complex dynamic processes operating at different resolutions, and (c) analyzing simulation results are extremely costly. The energy simulation data management system (e-SDMS) software will address challenges that arise from the need to model, index, search, visualize, and analyze, in a scalable manner, large volumes of multi-variate series resulting from observations and simulations. e-SDMS will, therefore, fill an important hole in data-driven building design and clean-energy (an area of national priority) and will enable applications and services with significant economic and environmental impact.<br/><br/>The key observations driving the research is that many data sets of urgent interest to energy simulations include the following: (a) voluminous, (b) heterogeneous, (c) multi-variate, (d) temporal, (e) inter-related (meaning that the parameters of interest are dependent on each other and constrained with the structure of the building), and (f) multi-resolution (meaning that simulations and observations cover days to months of data and may be considered at different granularities of space, time, and parameters). Moreover, generating an appropriate ensemble of simulations for decision making often requires multiple simulations, each with different parameters settings corresponding to slightly different, but plausible, scenarios. Therefore, significant savings in modeling and analysis can be obtained through data management software supporting modular re-use of existing simulation results in new settings, such as re-contextualization and modular recomposition (or ""sketching"") of building models and if-then analysis of simulation traces under new parameters, new building floorplans, and new contexts. In developing the energy simulation data management system (e-SDMS), the research addresses the key data challenges that render data-driven energy simulations, today, difficult. This requires (a) a novel building models, simulation traces, and sensor/actuation traces (BSS) data model to accommodate energy simulation data and models, (b) feature analysis and indexing of sensory data and simulation traces along with the corresponding building models, and (c) algorithms for analysis and exploration of simulation traces and re-contextualization of models for new building plans and contextual metadata. This research will therefore, impact computational challenges that arise from the need to model, analyze, index, visualize, search, and recompose, in a scalable manner, large volumes of multi-variate series resulting from energy observations and simulations. E-SDMS consists of an (a) eViz server, which works as a frontend to e-SDMS, an (b) eDMS middleware for feature extraction, indexing, simulation analysis, and sketching, and an (c) eStore backend for data storage. To avoid waste and achieve scalabilities needed for managing large data sets, e-SDMS employs novel multi-resolution data partitioning and resource allocation strategies. The multi-resolution data encoding, partitioning, and analysis algorithms are efficiently computable, leverage massive parallelism, and result in high quality, compact data descriptions.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339835","fuchsia_atom","SI2-SSE: E-SDMS: Energy Simulation Data Management System Software"
"1460032","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials","ACI","OFFICE OF MULTIDISCIPLINARY AC|CDS&E-MSS|Software Institutes","8/16/2014","9/4/2014","Jonathan Hauenstein","IN","University of Notre Dame","Standard Grant","Rajiv Ramnath","8/31/2017","$149,995.00 ","","hauenstein@nd.edu","940 Grace Hall","NOTRE DAME","IN","940 Grace Hall, NOTRE DAME, IN","465565708","5746317432","CSE","1253|8069|8004","7433|8005|8251","$0.00 ","Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.<br/><br/>Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1460032","fuchsia_atom","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials"
"1339606","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics","ACI","Software Institutes|STELLAR ASTRONOMY & ASTROPHYSC|OFFICE OF MULTIDISCIPLINARY AC","1/1/2014","8/29/2013","Richard Townsend","WI","University of Wisconsin-Madison","Standard Grant","Rajiv Ramnath","12/31/2016","$75,888.00 ","","townsend@astro.wisc.edu","21 North Park Street","MADISON","WI","21 North Park Street, MADISON, WI","537151218","6082623822","CSE","8004|1215|1253","7433|8005|1206","$0.00 ","As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of ""time domain astronomy"". The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011. MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.<br/><br/>This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community. MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation. This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339606","fuchsia_atom","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics"
"1450455","SI2-SSI: CRESCAT, A Computational Research Ecosystem for Scientific Collaboration on Ancient Topics, Spanning the Full Data Life Cycle","ACI","Software Institutes","9/1/2015","8/28/2015","David Schloen","IL","University of Chicago","Standard Grant","Rajiv Ramnath","8/31/2019","$1,500,000.00 ","Thomas Levy, Kathleen Morrison, Hakizumwami B. Runesha","d-schloen@uchicago.edu","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004","7433|8009|8004","$0.00 ","This project integrates, tests, and documents a suite of interoperable software tools to support collaborative research. The tools are collectively called CRESCAT (Computational Research Ecosystem for Scientific Collaboration on Ancient Topics). The initial focus is on disciplines that deal with dynamic interactions and structural changes within spatially situated populations over long time spans in the past, e.g., paleobiology, archaeology, and economic history. Despite their differences, these disciplines have similar computational needs for modeling and analyzing data. Moreover, the same software can be used in many other disciplines, enabling economies of scale by building and maintaining a common set of interoperable tools to serve a wide range of researchers, while spanning the full research data life cycle, consisting of (1) acquisition, (2) integration, (3) analysis, (4) publication, and (5) archiving of data. An intuitive graphical user interface is provided for end-user researchers to work with their data in all stages of the life cycle without cumbersome manual data transfers and transformations. The project will address a major computational problem that affects many scientific disciplines due to the challenge of integrating and analyzing data of diverse origins based on heterogeneous spatial, temporal, and taxonomic ontologies. Thus it will have a broad impact in the sciences and beyond by showing how to represent explicitly the full variability of individual judgments and the divergent conceptualizations and terminologies through which those judgments are expressed, with explicit attribution of each observation, interpretation, and conceptual ontology to a particular named person or group. Unlike many computational tools for scientific research, which assume a degree of ontological consensus that does not exist, CRESCAT conforms to actual research practices. It does not impose a standardized ontology, thereby ignoring or suppressing the inevitable disagreements and conflicting interpretations that arise among researchers. Instead, it represents ontological diversity, observational uncertainty, and interpretive disagreement explicitly within a larger common framework in which end users can query, analyze, and compare the full range of observations, interpretations, and terminologies to inform their own judgments about the evidence. CRESCAT is designed to allow scientific disagreements and observational and interpretive uncertainties to be represented digitally in a way that exposes these differences themselves as data for analysis and debate. Thus, in addition to the practical goal of building a more efficient shared framework for advanced research, the proposed work will provoke theoretical reflection about how computational tools should relate to scientific practice.<br/><br/>The CRESCAT project is an interdisciplinary collaboration between computer scientists, paleobiologists, geoscientists, archaeologists, economic historians, and other social scientists. The goal is to demonstrate the value of an integrative software ecosystem that spans the social and natural sciences and can facilitate any research characterized by overlapping models of temporal and spatial relations or by conflicting terminologies and taxonomies. CRESCAT's representation of scientific knowledge eschews forced standardization, which is impractical in many cases due to lack of an enforcement mechanism and is also questionable in principle since divergent ontologies often legitimately reflect different theoretical assumptions and research agendas. Central to the CRESCAT suite of tools is an innovative data-integration system that represents explicitly both research data and the ontologies inherent in the data. An ontology is defined here as a conceptual model of entities and the relationships among them in a given domain of knowledge, in contrast to a schema,&#148;which is the implementation of an ontology in logical data structures within a working system. CRESCAT's data-integration system operates at a level of abstraction sufficient to provide a predictable and efficiently queryable database structure based on an abstract global schema, which in turn is based on an upper ontology specified in terms of fundamental concepts and relationships applicable to all scientific and scholarly disciplines. The data-integration system is implemented in an enterprise-class XML/XQuery DBMS that serves as a data warehouse (using the non-relational graph data model), in which is stored diverse data from a wide range of research projects representing many disciplines. The terminology and conceptual distinctions of each research project are fully preserved. The approach to research data taken in the CRESCAT project is (1) coherent, tightly integrating software tools and data formats within a single analytical framework; (2) open-ended, interconnecting existing tools while allowing the addition of new tools in the future; (3) non-exclusive, in no way preventing its component tools from participating in other software ecosystems; (4) scalable, designed to handle large-scale data management, analysis, and visualization; and (5) sustainable, maintaining shared resources to meet common needs for software and technical support and thus enabling substantial economies of scale.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450455","fuchsia_atom","SI2-SSI: CRESCAT, A Computational Research Ecosystem for Scientific Collaboration on Ancient Topics, Spanning the Full Data Life Cycle"
"1450323","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC|COMPUTATIONAL PHYSICS","5/1/2015","5/6/2016","Brian Bockelman","NE","University of Nebraska-Lincoln","Continuing grant","Rajiv Ramnath","4/30/2019","$871,324.00 ","","bbockelm@cse.unl.edu","2200 Vine St, 151 Whittier","Lincoln","NE","2200 Vine St, 151 Whittier, Lincoln, NE","685031435","4024723171","CSE","8004|1253|7244","7433|8009|9150|8084","$0.00 ","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces. However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community. Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450323","fuchsia_tree","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)"
"1450122","SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)","ACI","Software Institutes","9/1/2015","5/4/2016","Vincent Weaver","ME","University of Maine","Standard Grant","Rajiv Ramnath","8/31/2019","$282,828.00 ","","vincent.weaver@maine.edu","5717 Corbett Hall","ORONO","ME","5717 Corbett Hall, ORONO, ME","44695717","2075811484","CSE","8004","7433|8004|8009|9150|9251","$0.00 ","Modern High Performance Computing (HPC) systems continue to increase in size and complexity. Tools to measure application performance in these increasingly complex environments must also increase the richness of their measurements to provide insights into the increasingly intricate ways in which software and hardware interact. The PAPI performance-monitoring library has provided a clear, portable interface to the hardware performance counters available on all modern CPUs and some other components of interest (scattered across the chip and system). Widely deployed and widely used, PAPI has established itself as fundamental software infrastructure, not only for the scientific computing community, but for many industry users of HPC as well. But the radical changes in processor and system design that have occurred over the past several years pose new challenges to PAPI and the HPC software infrastructure as a whole. The PAPI-EX project integrates critical PAPI enhancements that flow from both governmental and industry research investments, focusing on processor and system design changes that are expected to be present in every extreme scale platform on the path to exascale computing.<br/><br/>The primary impact of PAPI-EX is a direct function of the importance of the PAPI library. PAPI has been in predominant use by tool developers, major national HPC centers, system vendors, and application developers for over 15 years. PAPI-EX builds on that foundation. As important research infrastructure, the PAPI-EX project allows PAPI to continue to play its essential role in the face of the revolutionary changes in the design and scale of new systems. In terms of enhancing discovery and education, the list of partners working with PAPI-EX includes NSF computing centers, major tool developers, major system vendors, and individual community leaders, and this diverse group will help facilitate training sessions, targeted workshops, and mini-symposia at national and international meetings. Finally, the active promotion of PAPI by many major system vendors means that PAPI, and therefore PAPI-EX, will continue to deliver major benefits for government and industry in many domains.<br/><br/>PAPI-EX addresses a hardware environment in which the cores of current and future multicore CPUs share various performance-critical resources (a.k.a., 'inter-core' resources), including power management, on-chip networks, the memory hierarchy, and memory controllers between cores. Failure to manage contention for these 'inter-core' resources has already become a major drag on overall application performance. Consequently, the lack of ability to reveal the actual behavior of these resources at a low level, has become very problematic for the users of the many performance tools (e.g., TAU, HPCToolkit, Open|SpeedShop, Vampir, Scalasca, CrayPat, Active Harmony, etc.). PAPI-EX enhances and extends PAPI to solve this critical problem and prepare it to play its well-established role in HPC performance optimization. Accordingly, PAPI-EX targets the following objectives: (1) Develop shared hardware counter support that includes system-wide and inter-core measurements; (2) Provide support for data-flow based runtime systems; (3) Create a sampling interface to record streams of performance data with relevant context; (4) Combine an easy-to-use tool for text-based application performance analysis with updates to PAPI?s high-level API to create a basic, ?out of the box? instrumentation API.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450122","fuchsia_atom","SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)"
"1450412","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","ACI","Software Institutes|GEOMORPHOLOGY & LAND USE DYNAM|EarthCube","8/1/2015","7/14/2015","Erkan Istanbulluoglu","WA","University of Washington","Standard Grant","Rajiv Ramnath","7/31/2020","$676,836.00 ","","erkani@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","4333 Brooklyn Ave NE, Seattle, WA","981950001","2065434043","CSE","8004|7458|8074","7433|8009","$0.00 ","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450412","fuchsia_atom","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics"
"1440769","SI2-SSE: AMASS - An Automated Monitoring AnalySis Service for Cyberinfrastructure","ACI","Software Institutes","9/1/2014","12/3/2014","Shava Smallen","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","8/31/2017","$515,205.00 ","Lawrence Saul, Sameer Tilak","ssmallen@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8004","7433|8005|9251","$0.00 ","A science gateway is a community-developed set of tools, applications, and data collections that are integrated via a portal or a suite of applications. It provides easy, typically browser-based, access to supercomputers, software tools, and data repositories to allow researchers to focus on their scientific goals and less on the cyberinfrastructure. These gateways are fostering collaboration and exchange of ideas among thousands of researchers from multiple communities ranging from atmospheric science, astrophysics, chemistry, biophysics, biochemistry, earthquake engineering, geophysics, to neuroscience, and biology. However due to limited development and administrative personnel resources, science gateways often leverage only a small subset of the NSF-funded CI to mitigate the complexities involved with using multiple resource and services at scale in part due to software and hardware failures. Since many successful science gateways have had unprecedented growth in their user base and ever increasing datasets, increasing their usage of CI resources without introducing additional complexity would help them meet this demand.<br/><br/>In response to this need, an Automated Monitoring AnalySis Service (AMASS) will be built to provide a flexible and extensible service for automated analysis of monitoring data initially focused on science gateways. AMASS will be based on data mining and machine learning techniques and emerging big data technologies to analyze monitoring data for improving the reliability and operational efficiency of CI as well as progress on fundamental questions in systematic and population biology, computational neuroscience, and biophysics communities. Along with AMASS, a simulation framework will be built for testing automated analysis algorithms and adaptive execution techniques. An intuitive query API will be provided for science gateway software to use and will be integrated into the following three target science gateways that will drive the project's research and development: the Cyberinfrastructure for Phylogenetic Research (CIPRES), the Neuroscience Gateway (NSG), and UltraScan. The proposed approach does not require any changes to the end user applications, and the software developments will significantly enhance the science productivity and user satisfaction of science gateways by integrating monitoring data into their infrastructure to enable adaptive execution of their applications, allowing scientists to answer more sophisticated questions without having to understand the complexities of a large-scale distributed environment. The developed software products will be available as open source products under an Apache License and will be integrated into the NSF-funded SciGap project in order to impact a broader range of science gateways.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440769","fuchsia_atom","SI2-SSE: AMASS - An Automated Monitoring AnalySis Service for Cyberinfrastructure"
"1450471","SI2-SSI: Collaborative Research: A Software Infrastructure for MPI Performance Engineering: Integrating MVAPICH and TAU via the MPI Tools Interface","ACI","Software Institutes","9/1/2015","8/31/2015","Sameer Shende","OR","University of Oregon Eugene","Standard Grant","Rajiv Ramnath","8/31/2019","$1,200,000.00 ","Allen Malony","sameer@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","5219 UNIVERSITY OF OREGON, Eugene, OR","974035219","5413465131","CSE","8004","7433|8009","$0.00 ","Message-Passing Interface (MPI) continues to dominate the supercomputing landscape, being the primary parallel programming model of choice. A large variety of scientific applications in use today are based on MPI. On the current and next-generation High-End Computing (HEC) systems, it is essential to understand the interaction between time-critical applications and the underlying MPI implementations in order to better optimize them for both scalability and performance. Current users of HEC systems develop their applications with high-performance MPI implementations, but analyze and fine tune the behavior using standalone performance tools. Essentially, each software component views the other as a blackbox, with little sharing of information or access to capabilities that might be useful in optimization strategies. Lack of a standardized interface that allows interaction between the profiling tool and the MPI library has been a big impediment. The newly introduced MPI_T interface in the MPI-3 standard provides a simple mechanism that allows MPI implementers to expose variables representing configuration parameters or performance measurements from within the implementation for the benefit of tools, tuning frameworks, and other support libraries. However, few performance analysis and tuning tools take advantage of the MPI_T interface and none do so to dynamically optimize at execution time. This research and development effort aims to build a software infrastructure for MPI performance engineering using the new MPI_T interface.<br/><br/>With the adoption of MPI_T in the MPI standard, it is now possible to take positive steps to realize close interaction between and integration of MPI libraries and performance tools. This research, undertaken by a team of computer scientists from OSU and UO representing the open source MVAPICH and TAU projects, aims to create an open source integrated software infrastructure built on the MPI_T interface which defines the API for interaction and information interchange to enable fine grained performance optimizations for HPC applications. The challenges addressed by the project include: 1) enhancing existing support for MPI_T in MVAPICH to expose a richer set of performance and control variables; 2) redesigning TAU to take advantage of the new MPI_T variables exposed by MVAPICH; 3) extending and enhancing TAU and MVAPICH with the ability to generate recommendations and performance engineering reports; 4) proposing fundamental design changes to make MPI libraries like MVAPICH ``reconfigurable'' at runtime; and 5) adding support to MVAPICH and TAU for interactive performance engineering sessions. The framework will be validated on a variety of HPC benchmarks and applications. The integrated middleware and tools will be made publicly available to the community. The research will have a significant impact on enabling optimizations of HPC applications that have previously been difficult to provide. As a result, it will contribute to deriving ""best practice"" guidelines for running on next-generation Multi-Petaflop and Exascale systems. The research directions and their solutions will be used in the curriculum of the PIs to train undergraduate and graduate students.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450471","fuchsia_atom","SI2-SSI: Collaborative Research: A Software Infrastructure for MPI Performance Engineering: Integrating MVAPICH and TAU via the MPI Tools Interface"
"1450300","Collaborative Research: SI2-SSI:Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)","ACI","OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|Software Institutes","5/15/2015","5/20/2015","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Rajiv Ramnath","4/30/2018","$1,178,068.00 ","Thomas Herault","bosilca@eecs.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","1 CIRCLE PARK, KNOXVILLE, TN","379960003","8659743466","CSE","1253|1712|8004","7433|8009|8084|9150|9216","$0.00 ","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US. TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects. TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450300","fuchsia_atom","Collaborative Research: SI2-SSI:Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)"
"1450319","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","ACI","COMPUTATIONAL PHYSICS|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","5/1/2015","5/6/2016","Michael Sokoloff","OH","University of Cincinnati Main Campus","Continuing grant","Rajiv Ramnath","4/30/2019","$870,000.00 ","","sokoloff@physics.uc.edu","University Hall, Suite 530","Cincinnati","OH","University Hall, Suite 530, Cincinnati, OH","452210222","5135564358","CSE","7244|1253|8004","7433|8009|8084","$0.00 ","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces. However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community. Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450319","fuchsia_tree","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)"
"1440583","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS","9/1/2014","8/1/2014","Jonathan Hauenstein","NC","North Carolina State University","Standard Grant","Daniel Katz","10/31/2014","$149,995.00 ","","hauenstein@nd.edu","CAMPUS BOX 7514","RALEIGH","NC","CAMPUS BOX 7514, RALEIGH, NC","276957514","9195152444","CSE","1253|8004|8069","7433|8005|8251","$0.00 ","Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.<br/><br/>Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440583","fuchsia_atom","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials"
"1440443","SI2-SSE: A Next-Generation Open-Source Computational Fluid Dynamic Code for Polydisperse Multiphase Flows in Science and Engineering","ACI","Software Institutes|FLUID DYNAMICS","10/1/2014","8/21/2014","Alberto Passalacqua","IA","Iowa State University","Standard Grant","Rajiv Ramnath","9/30/2017","$499,551.00 ","Rodney Fox, Simanta Mitra","albertop@iastate.edu","1138 Pearson","AMES","IA","1138 Pearson, AMES, IA","500112207","5152945225","CSE","8004|1443","7433|8005|9150|058E|1443","$0.00 ","Many processes for the production of drugs, fuels and plastic materials, as well as energy from coal or biomasses, involve multiphase flows, which are composed by a combination of a fluid, either liquid or gas, and particles, droplets or bubbles. This type of flow is also naturally present in the environment. Examples are the formation of a mixture of air and solid particles due to volcanic eruptions, and particles of sand and other materials transported by the wind. Scientists and engineers use software to study how these flows behave in order to improve the yield of industrial processes, reduce their environmental impact, and energy consumption. The computer programs used to perform these studies solve complex mathematical problems, and require powerful computers to be able to obtain the results in a useful time. This project focuses on developing the next generation of computer software for the simulation of multiphase flows, enabling it to use the latest generation of computers which combine traditional and graphical processors for improved performance. This software will be released to the public and will enable, scientists and engineers from different research areas to tackle real-world problems by taking advantage of the latest developments in multiphase flow science, combined with the benefit of being able to use the software on powerful computer infrastructures. Students and educators will be able to use the software and learn about multiphase flows through the examples and the documentation that will be provided.<br/><br/>The objectives of the project will be achieved by first developing computational models to describe turbulent flows in the framework of quadrature-based moment methods, an efficient and accurate approach to describe this type of flows. These computational models will then be implemented, together with appropriate numerical methods that will ensure the accuracy of the computational codes, in the open-source framework OpenFOAM. Three representative problems of typical multiphase flows will be considered: a population balance equation for particles with negligible inertia, such as in the formation of nanoparticles in a fluid flow; the description of gas-liquid flows, where the bubble inertia is small but not zero; and the most complex case of gas-solid flows, where particle inertia is large. The possibility of using graphical processing units will be added to the OpenFOAM framework, to enable it to run on hybrid computational systems involving traditional processors and graphical processing units. The source code and its documentation will be made available to the public at an early stage of their development, under the GNU GPL 3 license, in order to disseminate the results of the research and gather feedback. Detailed code documentation, verification and validation cases, and tutorials will also be created to favor external contributions to the software.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440443","fuchsia_atom","SI2-SSE: A Next-Generation Open-Source Computational Fluid Dynamic Code for Polydisperse Multiphase Flows in Science and Engineering"
"1440420","SI2-SSE: Scalable Big Data Clustering by Random Projection Hashing","ACI","SPECIAL PROJECTS - CCF|Software Institutes","9/1/2014","8/8/2014","Philip Wilsey","OH","University of Cincinnati Main Campus","Standard Grant","Rajiv Ramnath","8/31/2017","$498,127.00 ","","philip.wilsey@uc.edu","University Hall, Suite 530","Cincinnati","OH","University Hall, Suite 530, Cincinnati, OH","452210222","5135564358","CSE","2878|8004","7433|8005|8004|2878","$0.00 ","This project plans to develop a distributed algorithm for secure clustering of high dimensional data sets. Fields in health and biology are significantly benefited by data clustering scalability. Bioinformatic problems such as Micro Array clustering, Protein-Protein interaction clustering, medical resource decision making, medical image processing, and clustering of epidemiological events all serve to benefit from larger dataset sizes. The algorithm under development, called Random Projection Hash or RPHash, utilizes aspects of locality sensitive hashing (LSH) and multi-probe random projection for computational scalability and linear achievable gains from parallel speed. Furthermore, RPHash provides data anonymization through destructive manipulation of the data preventing de-anonymization attacks beyond standard best practices database security methods. RPHash will be deployable on commercially available cloud resources running the Hadoop (MRv2) implementation of MapReduce. The exploitation of general purpose cloud processing solutions allows researchers to scale their processing needs using virtually limitless commercial processing resources.<br/><br/>The RPHash algorithm uses various recent techniques in data mining along with a new approach toward achieving algorithmic scalability on distributed systems. The basic intuition of RPHash is to combine multi-probe random projection with discrete space quantization. Regions of high density are then regarded as centroid candidates. To follow common parameterized, k-means methods, the top k regions will be selected. The focus on a randomized, and thus non-deterministic, clustering algorithm is somewhat uncommon in computing, but common for ill-posed, combinatorially restrictive problems such as clustering and partitioning. Despite theoretical results showing that k-means has an exponential worst case complexity, many real world problems tend to fair much better under k-means and other similar algorithms.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440420","fuchsia_atom","SI2-SSE: Scalable Big Data Clustering by Random Projection Hashing"
"1450089","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","Software Institutes|PHYSICAL & DYNAMIC METEOROLOGY|EarthCube","8/1/2015","8/4/2015","Russ Schumacher","CO","Colorado State University","Standard Grant","Rajiv Ramnath","7/31/2018","$177,173.00 ","","russ.schumacher@colostate.edu","601 S Howes St","Fort Collins","CO","601 S Howes St, Fort Collins, CO","805232002","9704916355","CSE","8004|1525|8074","7433|8009|4444","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450089","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1450439","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","Software Institutes|PHYSICAL & DYNAMIC METEOROLOGY|EarthCube","8/1/2015","8/4/2015","Allen Evans","WI","University of Wisconsin-Milwaukee","Standard Grant","Rajiv Ramnath","7/31/2018","$164,381.00 ","","evans36@uwm.edu","P O BOX 340","Milwaukee","WI","P O BOX 340, Milwaukee, WI","532010340","4142294853","CSE","8004|1525|8074","7433|8009|4444","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450439","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1440685","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud","ACI","CDS&E|Software Institutes|DMR SHORT TERM SUPPORT","10/1/2014","7/31/2014","Coray Colina","PA","Pennsylvania State Univ University Park","Standard Grant","Daniel Katz","12/31/2015","$195,000.00 ","","colina@chem.ufl.edu","110 Technology Center Building","UNIVERSITY PARK","PA","110 Technology Center Building, UNIVERSITY PARK, PA","168027000","8148651372","CSE","8084|8004|1712","7433|8005|8400|7237|9216|024E|085E","$0.00 ","The use of polymers, their composites, and nanostructures is growing at a fast pace, both by displacing traditional materials and by enabling emerging technologies. Examples range from the all-composite airframe of the Boeing 787 and the new Airbus 350 to wearable electronics. This project aims to develop a software infrastructure to simulate these materials with atomic resolution and make these tools universally accessible and useful via on-line computing. These simulations have the potential to accelerate the development of optimized material formulations that can benefit society and reduce the associated costs by combining physical with computational experiments. Making these advanced tools available for free online simulations and complementing them with tutorials and educational material will encourage their use in the classroom and will impact the education of new generations of engineers and scientists familiar with these powerful tools that will be required to address tomorrow's challenges.<br/><br/>The objective of this effort is to enable pervasive, reproducible molecular simulations of polymeric materials using state-of-the-art tools and with quantified uncertainties building on recent breakthroughs in molecular simulations, cyber-infrastructure and uncertainty quantification. The framework will consist of three main components: i) powerful simulation tools for polymer nano structures including: state-of-the-art molecular builders, a parallel MD engine with stencils that enable efficient structure relaxation and property characterization and post-processing codes; ii) a UQ framework to orchestrate the molecular simulations and propagate uncertainties in input parameters to predictions and compare the predictions to experimental values; iii) databases of force fields and molecular structures as well as predicted and experimental properties. The simulation framework will be deployed via NSF's nanoHUB where users will be able to run online simulations without downloading or installing any software while expert users will have the option to download, modify and contribute to the infrastructure. Usage of and contributions to the software framework will be facilitated and encouraged via online and in-person user guides, learning modules and research tutorials.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440685","fuchsia_atom","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud"
"1440467","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials","ACI","OFFICE OF MULTIDISCIPLINARY AC|CDS&E-MSS|Software Institutes","9/1/2014","8/1/2014","Daniel Bates","CO","Colorado State University","Standard Grant","Rajiv Ramnath","8/31/2017","$149,346.00 ","","bates@math.colostate.edu","601 S Howes St","Fort Collins","CO","601 S Howes St, Fort Collins, CO","805232002","9704916355","CSE","1253|8069|8004","7433|8005|8251","$0.00 ","Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.<br/><br/>Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440467","fuchsia_atom","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials"
"1450374","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS|CDS&E","8/1/2015","8/11/2015","Neelesh Patankar","IL","Northwestern University","Standard Grant","Rajiv Ramnath","7/31/2020","$512,966.00 ","","n-patankar@northwestern.edu","1801 Maple Ave.","Evanston","IL","1801 Maple Ave., Evanston, IL","602013149","8474913003","CSE","1253|8004|8069|8084","7433|8004|8009|8084","$0.00 ","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450374","fuchsia_atom","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction"
"1440412","SI2-SSE: Wavelet Enabled Progressive Data Access and Storage Protocol (WASP)","ACI","EarthCube|PHYSICAL & DYNAMIC METEOROLOGY|ADVANCES IN BIO INFORMATICS|Software Institutes","10/1/2014","7/25/2014","Lawrence Frank","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","9/30/2017","$500,000.00 ","","lfrank@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8074|1525|1165|8004","7433|8005|4444","$0.00 ","Advances in digital imaging methods are revolutionizing a wide range of scientific disciplines by facilitating the acquisition of huge amounts of data that allow the visualization and analysis of complex, multidimensional images. Concurrently, modern computing technologies enable numerical modeling of a broad gamut of scientific phenomena, resulting in vast quantities of numerical data, which are just the starting point for the scientific exploration that modern computational and visualization methods enable. This is particularly true in the biological and geosciences, two seemingly very different disciplines. These capabilities come with a cost: increasing data size and complexity require more sophisticated methods for data analysis and visualization. This project will conduct research that will lead to a common software framework for supporting a multi scale progressive data refinement method based upon the representation of the data as a wavelet expansion, and enabling interactive exploration of large data sets for the bio and geoscience communities. The development of a general toolkit for wavelet based representations of data will have broad impact, allowing the multi scale analysis, storage, and visualization for data collected in a wide range of fields and on a multitude of platforms, from high end computing facilities to laptop computers used by students, field biologists, and others.<br/><br/>Analysis and visualization of large data sets play an important role in scientific discovery. Efficient, and broadly available tools to accomplish these tasks are crucial for a wide range of scientific and educational fields. However, efficient analysis and visualization is a non trivial problem as the size and complexity of data increases. This research addresses this challenge through a general progressive access, multi scale data representation for efficient handling of structured data sets across a range of science domains. The development is based upon a wavelet enabled data representation developed by NCAR for geoscience applications. The tools will utilize the very flexible and open source standard NetCDF format, and the methods will be documented as a set of conventions and a toolkit developed that incorporates and integrates these components for dissemination. In addition to an open source toolkit, these tools will be integrated into the VAPOR (NCAR) and STK (CSCI) platforms, thus expanding the capabilities and efficiencies of these platforms for the geo and bio sciences communities, respectively.  Advancements generated by this project will be openly disseminated to the user community through an open source toolkit.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440412","fuchsia_atom","SI2-SSE: Wavelet Enabled Progressive Data Access and Storage Protocol (WASP)"
"1440547","SI2-SSE: Genetic Algorithm Software Package for Prediction of Novel Two-Dimensional Materials and Surface Reconstructions","ACI","DMR SHORT TERM SUPPORT|Software Institutes","1/1/2015","7/24/2014","Richard Hennig","NY","Cornell University","Standard Grant","Rajiv Ramnath","12/31/2017","$344,696.00 ","","rhennig@ufl.edu","373 Pine Tree Road","Ithaca","NY","373 Pine Tree Road, Ithaca, NY","148502820","6072555014","CSE","1712|8004","7433|8005|8400|7237|9216","$0.00 ","The ability to control structure and composition at the nanoscale has introduced exciting scientific and technological opportunities. Advances in the creation of nanomaterials such as single-layer materials and nanocrystals have led to improved understanding of basic structure-property relationships that, in turn, have enabled impressive progress in a broad range of nanotechnologies with applications for energy storage, catalysis and electronic devices. Yet, significant knowledge gaps persist in what single-layer materials could be synthesized and in our understanding of the nature of the surfaces of nanocrystals, particularly in the complex environment of solvents and ligands. The discovery of potentially stable novel single-layer materials and the prediction of nanocrystal surface structures are arguably among the most critical aspects of nanoscale materials. This research will provide the computational tools for the detailed prediction of the structure of two-dimensional materials and nanostructure surfaces in complex environments. This will impact the development and the design of novel nanomaterials with properties optimized for applications ranging from catalyst for chemical reactions, to energy conversion materials, to low-power and high-speed electronic devices.<br/><br/>Progress in the field requires better computational methods for structure prediction. This project will (i) transform the Genetic Algorithm for Structure Prediction (GASP) software package developed by the PI into a sustainable scientific tool, (ii) extend its functionality to 2D materials and materials interfaces, and (iii) increase its performance by coupling to surrogate energy models that are optimized on the fly. These complementary goals will be achieved through expansion of the developer and user base, transition to portable software interfaces and data structures, and the addition of modular algorithms for functionality and performance enhancements. To enhance the functionality, the GASP algorithms will be extended to two two-dimensional materials and materials surfaces with adsorbates and ligands. To enhance the performance of the genetic algorithm, the optimization approach will be coupled to surrogate energy models such as machine-learning techniques and empirical energy models that are optimized on the fly. The publication of user tutorials, and documentation on the data structures and software interfaces will enhance the GASP codes overall utility, increase the user and developer base, and enable further extension to other data-mining and structure prediction approaches. The students involved in this project will receive extensive training and experience in algorithm development, scientific computation, and structure/property determination of complex nanomaterials. As part of the education and outreach component of the project, the PI will develop a course module on Materials Structure Predictions and widely distribute it. A weeklong workshop for students and postdocs in the third year of the project on Materials Discovery and Design will broaden the research?s impact beyond the creation of new software and the discovery of novel single-layer materials and nanocrystal surface and ligand configurations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440547","fuchsia_atom","SI2-SSE: Genetic Algorithm Software Package for Prediction of Novel Two-Dimensional Materials and Surface Reconstructions"
"1449723","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","8/1/2015","6/24/2015","Charles Sherrill","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","7/31/2019","$600,000.00 ","","sherrill@gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","1253|8004","7433|8009","$0.00 ","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1449723","fuchsia_atom","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science"
"1440523","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions","ACI","Software Institutes|SPECIAL PROJECTS - CCF|CDS&E","2/1/2015","8/8/2014","Adrian Roitberg","FL","University of Florida","Standard Grant","Rajiv Ramnath","1/31/2018","$67,235.00 ","","roitberg@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","1 UNIVERSITY OF FLORIDA, GAINESVILLE, FL","326112002","3523923516","CSE","8004|2878|8084","7433|8005|8004|9216|8084|2878","$0.00 ","This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.<br/><br/>The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440523","fuchsia_atom","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions"
"1450169","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC","8/1/2015","6/24/2015","Thomas Crawford","VA","Virginia Polytechnic Institute and State University","Standard Grant","Rajiv Ramnath","7/31/2019","$600,000.00 ","","crawdad@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","Sponsored Programs 0170, BLACKSBURG, VA","240610001","5402315281","CSE","8004|1253","7433|8009","$0.00 ","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450169","fuchsia_atom","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science"
"1440788","SI2-SSE: Enhancement and Support of DMTCP for Adaptive, Extensible Checkpoint-Restart","ACI","SPECIAL PROJECTS - CCF|Software Institutes","9/1/2014","11/17/2014","Gene Cooperman","MA","Northeastern University","Standard Grant","Rajiv Ramnath","8/31/2017","$514,427.00 ","","gene@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","360 HUNTINGTON AVE, BOSTON, MA","21155005","6173732508","CSE","2878|8004","7433|8005|8004|9251|2878","$0.00 ","Society's increasingly complex cyberinfrastructure creates a concern for software robustness and reliability. Yet, this same complex infrastructure is threatening the continued use of fault tolerance. Consider when a single application or hardware device crashes. Today, in order to resume that application from the point where it crashed, one must also consider the complex subsystem to which it belongs. While in the past, many developers would write application-specific code to support fault tolerance for a single application, this strategy is no longer feasible when restarting the many inter-connected applications of a complex subsystem. This project will support a plugin architecture for transparent checkpoint-restart. Transparency implies that the software developer does not need to write any application-specific code. The plugin architecture implies that each software developer writes the necessary plugins only once. Each plugin takes responsibility for resuming any interrupted sessions for just one particular component. At a higher level, the checkpoint-restart system employs an ensemble of autonomous plugins operating on all of the applications of a complex subsystem, without any need for application-specific code.<br/><br/>The plugin architecture is part of a more general approach called process virtualization, in which all subsystems external to a process are virtualized. It will be built on top of the DMTCP checkpoint-restart system. One simple example of process virtualization is virtualization of ids. A plugin maintains a virtualization table and arranges for the application code of the process to see only virtual ids, while the outside world sees the real id. Any system calls and library calls using this real id are extended to translate between real and virtual id. On restart, the real ids are updated with the latest value, and the process memory remains unmodified, since it contains only virtual ids. Other techniques employing process virtualization include shadow device drivers, record-replay logs, and protocol virtualization. Some targets of the research include transparent checkpoint-restart support for the InfiniBand network, for programmable GPUs (including shaders), for networks of virtual machines, for big data systems such as Hadoop, and for mobile computing platforms such as Android.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440788","fuchsia_atom","SI2-SSE: Enhancement and Support of DMTCP for Adaptive, Extensible Checkpoint-Restart"
"1440700","SI2-SSE: Enhancing the PReconditioned Iterative MultiMethod Eigensolver Software with New Methods and Functionality for Eigenvalue and Singular Value Decomposition (SVD) Problems","ACI","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|DMR SHORT TERM SUPPORT","9/1/2014","8/11/2014","Andreas Stathopoulos","VA","College of William and Mary","Standard Grant","Rajiv Ramnath","8/31/2017","$447,884.00 ","","andreas@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","Office of Sponsored Programs, Williamsburg, VA","231878795","7572213966","CSE","1253|8004|1712","7433|8005|8400|9216","$0.00 ","The numerical solution of large, sparse Hermitian Eigenvalue Problems (HEP) and Generalized HEP (GHEP) for a few extreme eigenvalues is one of the most important but also computationally intensive tasks in a variety of applications. Examples abound in spectral graph partitioning, large scale spectral graph analysis, structural engineering, electromagnetics, lattice Quantum Chromodynamics, and electronic structure applications from atomic scale physics to molecular scale materials science. Closely related is the problem of computing a partial Singular Value Decomposition (SVD) of a matrix, which finds everyday use in numerous applications including data mining and machine learning. The importance of the problem is evidenced by the significant resources that have been invested over the last decade in developing high quality eigenvalue software packages. However, these packages still do not include the near-optimal methods that have made the package PRIMME the software to beat. PRIMME, or PReconditioned Iterative MultiMethod Eigensolver, is a software package developed in 2005 for the solution of HEP. PRIMME brings state-of-the-art preconditioned iterative methods from ""bleeding edge"" to production, with a flexible, yet highly usable interface. Yet, it is its focus on numerical robustness and computational efficiency that has gained PRIMME the recognition as one of the best eigenvalue packages. This success calls for a new effort to extend PRIMME with some long awaited functionality but also to include new algorithms to address some outstanding problems in eigenvalue computations. This work is critical to many groups whose research depends on the lattice QCD and materials science software packages that PRIMME will improve through collaborations. PRIMME already has a PETSc interface, and with the proposed development of Hypre and Trilinos interfaces, it will be accessible by a far wider community of users. The most requested feature, however, has been a MATLAB interface. This will unleash the power of an ""industrial strength"" software to end users. Last but not least, this project will educate and train two graduate and several undergraduate students in the art of high performance numerical software.<br/><br/>Specific goals for this projects include: PRIMME extension to GHEP, with special attention to ill conditioned mass matrices; PRIMME extension to SVD, with special attention to obtaining results at high accuracy (the solution must include not only PRIMME's robust components but a combination of known and new methods, as well as a dynamic way to choose between them); implementation of new methods and techniques for the solution of highly interior eigenvalue problems and for the computation of a large number of eigenvalues; interoperability with DOE libraries and MATLAB, and improved means of dissemination. As a numerical linear algebra kernel, PRIMME has a large potential audience in the computational sciences community. However, two specific collaborations will provide real-world, challenging problems and serve as a stress-test evaluator of the resulting methods and software. One involves the lattice QCD group at the DOE's Jefferson Lab, and the other involves the high performance computing and materials science group at IBM, Zurich.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440700","fuchsia_atom","SI2-SSE: Enhancing the PReconditioned Iterative MultiMethod Eigensolver Software with New Methods and Functionality for Eigenvalue and Singular Value Decomposition (SVD) Problems"
"1440534","SI2-SSE: Solving Polynomial Systems with PHCpack and phcpy","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC|CDS&E-MSS","10/1/2014","8/1/2014","Jan Verschelde","IL","University of Illinois at Chicago","Standard Grant","Rajiv Ramnath","9/30/2017","$464,352.00 ","","jan@math.uic.edu","809 S MARSHFIELD","Chicago","IL","809 S MARSHFIELD, Chicago, IL","606124305","3129962862","CSE","8004|1253|8069","7433|8005|8251","$0.00 ","Solving polynomial systems is a fundamental problem in mathematics with applications to various fields of science and engineering. The free and open source software PHCpack applies symbolic-numeric and polyhedral methods to solve polynomial systems. As a new interface to PHCpack written in the Python scripting language, phcpy improves the functionality of PHCpack. The implementation on parallel computers to compensate for the cost overhead of multi-precision arithmetic will enable scientists and engineers to solve larger systems faster and more accurately. A web server developed with phcpy will give anyone with an internet connection access to the developed software.<br/><br/>The solvers in PHCpack apply homotopy continuation methods, blending symbolic-numeric with polyhedral algorithms. Numerical approximations to solutions are computed with Newton's method. Solutions are approximated symbolically by Puiseux series, which originate at initial forms defined by the Newton polytopes of the polynomials in the system. The design of phcpy gives a flexible interactive scripting interface, without sacrificing efficiency as compiled code in PHCpack is executed. The package phcpy will provide the tools for a scalable compute server to serve requests submitted to the web server. Multithreaded implementations on multicore processors accelerated by graphics processing units compensate for the cost overhead of double double and quad double arithmetic.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440534","fuchsia_atom","SI2-SSE: Solving Polynomial Systems with PHCpack and phcpy"
"1440709","SI2-SSE: Petascale Enzo: Software Infrastructure Development and Community Engagement","ACI","Software Institutes||OFFICE OF MULTIDISCIPLINARY AC","9/1/2014","8/13/2014","Michael Norman","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","8/31/2017","$500,000.00 ","","mlnorman@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","Office of Contract & Grant Admin, La Jolla, CA","920930934","8585344896","CSE","8004|1798|1253","7433|8005|1206","$0.00 ","The purpose of this project is to develop an astrophysics and cosmology software application ""Enzo-P"", built on the highly scalable parallel adaptive mesh refinement (AMR) software framework ""Cello"" that is being developed concurrently. The Enzo-P application will be capable of running extreme scale numerical simulations to investigate frontier questions in star formation, molecular cloud turbulence, interstellar medium dynamics, galaxy formation, intergalctic medium, formation of the first stars and galaxies, galaxy clusters, and cosmic reionization. This new software will empower the current large and diverse Enzo user/developer community to take full advantage of current and future high performance computer (HPC) systems. The Cello AMR framework can be used independently of Enzo-P, thus enabling researchers in other diverse scientific fields to develop AMR applications capable of running on ""Petascale-and-beyond"" HPC platforms. <br/><br/>The novel approach used for Cello is to implement a ""forest-of-octree"" AMR scheme using the Charm++ parallel programming system. Octree-based AMR has been shown to be among the highest scaling AMR approaches, with demonstrated scaling to over 200K CPU cores. The Charm++ object-oriented parallel programming language supports data-driven asynchronous execution, is inherently latency-tolerant and automatically overlaps computation with communication, and provides support for developing Exascale applications, including in-memory distributed checkpointing and sophisticated dynamic load balancing schemes. Enzo-P development will be directed by the vibrant Enzo open development community, who will migrate Enzo's self-gravity, cosmology, chemistry and cooling, MHD, and radiation hydrodynamics capabilities to use the Cello scalable AMR framework.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440709","fuchsia_atom","SI2-SSE: Petascale Enzo: Software Infrastructure Development and Community Engagement"
"1450409","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","ACI","Software Institutes|GEOMORPHOLOGY & LAND USE DYNAM|EarthCube","8/1/2015","7/14/2015","Gregory Tucker","CO","University of Colorado at Boulder","Standard Grant","Rajiv Ramnath","7/31/2020","$789,777.00 ","Daniel Hobley","gtucker@cires.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","3100 Marine Street, Room 481, Boulder, CO","803031058","3034926221","CSE","8004|7458|8074","7433|8009","$0.00 ","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450409","fuchsia_atom","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics"
"1440749","SI2-SSE: Improving Vectorization","ACI","Software Institutes","9/1/2014","7/1/2014","Ponnuswamy Sadayappan","OH","Ohio State University","Standard Grant","Patricia Knezek","8/31/2017","$500,000.00 ","","sadayappan.1@osu.edu","Office of Sponsored Programs","Columbus","OH","Office of Sponsored Programs, Columbus, OH","432101016","6146888735","CSE","8004","7433|8005","$0.00 ","The increasing width of vector instruction sets in processors and accelerators raises the importance of effective vectorization. Although the topic of automatic vectorization by compilers has received significant attention over the last few decades, current vectorizing compilers can typically realize only a small fraction of a processor's peak performance. This project will explore several compiler optimization approaches for generating high-performance vectorized code. Advanced vectorization techniques will be incorporated in the open-source LLVM/Clang compiler through the Polly/LLVM module. A benchmark suite will also be developed, aimed at testing the effectiveness of vectorizing compilers.<br/><br/>Production compilers limit their optimization search space in order to control the time taken to compile programs. This is because the majority of users expect rapid compile times. However, the developers of high-performance applications are generally very willing to tolerate a much longer wait for program compilation, in return for a boost in the performance of the compiled code. A significant focus of this project will be the development of vectorization approaches for such users who prioritize high application performance over short compile times. The project will investigate semantically-driven pattern-based approaches to vector optimization, a vectorization-friendly approach to tiling, and aggressive vector instruction scheduling approaches that promise higher performance at the expense of possibly high compile times.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440749","fuchsia_atom","SI2-SSE: Improving Vectorization"
"1450451","SI2-SSI: Community Software for Extreme-Scale Computing in Earthquake System Science","ACI","GEOPHYSICS|GEOINFORMATICS|Software Institutes|EarthCube|CDS&E","9/1/2015","8/17/2015","Thomas Jordan","CA","University of Southern California","Standard Grant","Rajiv Ramnath","8/31/2019","$2,200,000.00 ","Kim Olsen, Yifeng Cui, Ricardo Taborda","tjordan@usc.edu","University Park","Los Angeles","CA","University Park, Los Angeles, CA","900890001","2137407762","CSE","1574|7255|8004|8074|8084","7433|8009|8004","$0.00 ","The Software Environment for Integrated Seismic Modeling (SEISM) Project of the Southern California Earthquake Center (SCEC) will develop advanced earthquake simulation software capable of using high-performance computing to produce new information about earthquakes and the hazards they present. SCEC's SEISM project is developing an integrated, sustainable community software framework for earthquake system science to serve diverse communities of earthquake scientists and engineers, computer scientists, and at-risk stakeholders. The SEISM project is a collaboration among several diverse user communities with shared interests in reducing seismic risk and enhancing seismic resilience. SCEC SEISM researchers are addressing scientific problems that limit the accuracy and scale in current numerical representations of earthquake processes. SEISM computational improvements in seismic hazard calculations will benefit earthquake system science worldwide. The SCEC SEISM project will educate a diverse STEM workforce from the undergraduate to early-career level, and it will cross-train scientists and engineers in a challenging high-performance environment. As one application of SEISM, the researchers will develop new simulations for the Great California ShakeOut, which is engaging millions of people in earthquake preparedness exercises.<br/><br/>Earthquake simulations at the spatiotemporal scales required for probabilistic seismic hazard analysis present some of the toughest computational challenges in geoscience, requiring extreme-scale computing. The Southern California Earthquake Center is creating a Software Environment for Integrated Seismic Modeling (SEISM) that will provide the extreme-scale simulation capability needed to transform probabilistic seismic hazard analysis into a physics-based science. This project will advance SEISM through a user-driven research and development agenda that will push validated SEISM capabilities to higher seismic frequencies and towards extreme-scale computing. It will develop an integrated, sustainable community software framework for earthquake system science to serve diverse communities of earthquake scientists and engineers, computer scientists and at-risk stakeholders. A new SEISM-T framework will support both in-situ and post-hoc data processing to make efficient use of available heterogeneous architectures. The main goal of the project is to increase the 4D outer-scale/inner-scale ratio of simulations at constant time-to-solution by two orders of magnitude above current capabilities. The software development plan will use an agile process of test-driven development, continuous software integration, automated acceptance test suites for each application, frequent software releases, and attention to user feedback. The researchers will take advantage of the SCEC Implementation Interface to develop a dialog among user communities regarding the application of SEISM to the reduction of seismic risk and enhancement of seismic resilience. This research will address fundamental scientific problems that limit the accuracy and scale range in current numerical representations of earthquake processes, which will benefit earthquake system science worldwide. This project will educate a diverse STEM workforce from the undergraduate to early-career level, and it will cross-train scientists and engineers in a challenging high-performance environment. As one application of SEISM, the project team will develop new simulations for the Great California ShakeOut, which is engaging millions of people in earthquake preparedness exercises.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450451","fuchsia_atom","SI2-SSI: Community Software for Extreme-Scale Computing in Earthquake System Science"
"1450168","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","ACI","PHYSICAL & DYNAMIC METEOROLOGY|EarthCube|Software Institutes","8/1/2015","8/4/2015","Gretchen Mullendore","ND","University of North Dakota Main Campus","Standard Grant","Rajiv Ramnath","7/31/2018","$168,182.00 ","","gretchen@atmos.und.edu","University Station","Grand Forks","ND","University Station, Grand Forks, ND","582026059","7017774278","CSE","1525|8074|8004","7433|8009|9150|4444","$0.00 ","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1450168","fuchsia_atom","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities"
"1440756","SI2-SSE: An Active File System for Processing and Sharing Petascale Multi-Dimentional Datasets","ACI","SPECIAL PROJECTS - CCF|ADVANCES IN BIO INFORMATICS|Software Institutes","9/1/2014","8/8/2014","Arthur Wetzel","PA","Carnegie-Mellon University","Standard Grant","Rajiv Ramnath","8/31/2017","$500,000.00 ","","awetzel@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","5000 Forbes Avenue, PITTSBURGH, PA","152133815","4122689527","CSE","2878|1165|8004","7433|8005|8004|1165|2878","$0.00 ","Data sets in diverse areas such as biology, engineering, or astronomy routinely reach terabyte scales and are expected to grow to petabytes within the next few years. Typical examples include time series measurement or multidimensional volumetric data sets. Due to their rapidly increasing size, these data present severe limitations for data storage, transmission, and processing, and will thus become serious bottlenecks for analysis pipelines and collaborative data analysis. New approaches and frameworks are needed to enable the timely and cost effictive analyses at next generation data scales. The Active Data Processing and Transformation File System (ADAPT FS) combines efficient storage of original data with on-the-fly processing to enable collaborative processing and sharing of scientific datasets at the largest scales with minimal data duplication and latency. The remote access will leverage PSC's SLASH2 distributed filesystem and be extended to provide visualization. ADAPT-FS processing will enable easy access to leading-edge scientific datasets for teaching and training at institutions of all sizes.<br/><br/>ADAPT-FS provides a FUSE based file system interface allowing seamless use from programs or web servers. The guiding principles behind ADAPT-FS are to: 1) eliminate unwanted data duplication; 2) minimize data transfer by working directly from original data when possible; 3) minimize delays between data capture and end-user analyses; and 4) provide a flexible workflow which incorporates active computation. The latter aspect enables improved parallel I/O performance by exploiting untapped CPU and GPU power on the nodes of large data servers. We will provide ADAPT-FS as open source, including a flexible and well-documented API and a plug-in framework enabling users to insert their own GPU and CPU codes into the data pipeline to extend and customize its data processing capabilities. Thus, ADAPT-FS will provide a critical technology to tackle the next generation of massive data intensive processing, allowing efficient and rapid analysis of petabytes size data sets with minimal data duplication in a collaborative multi-site framework.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1440756","fuchsia_atom","SI2-SSE: An Active File System for Processing and Sharing Petascale Multi-Dimentional Datasets"
"1449918","SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Performance, Security, and Correctness Tools","ACI","Software Institutes","8/1/2015","7/20/2015","Barton Miller","WI","University of Wisconsin-Madison","Standard Grant","Rajiv Ramnath","7/31/2020","$1,500,000.00 ","","bart@cs.wisc.edu","21 North Park Street","MADISON","WI","21 North Park Street, MADISON, WI","537151218","6082623822","CSE","8004","7433|8009","$0.00 ","Software has become indispensable to society, used by computational scientists for science and engineering, by analysts mining big data for value, and to connect society over the Internet. However, the properties of software systems for any of these purposes cannot be understood without accounting for code transformations applied by optimizing compilers used to compose algorithm and data structure templates, and libraries available only in binary form. To address this need, this project will overhaul, integrate, and enhance static binary analysis and runtime technologies to produce components that provide a foundation for performance, correctness, and security tools. The project will build upon three successful and widely adopted open source software packages: the DynInst library for analysis and transformation of application binaries, the MRNet infrastructure for control of large-scale parallel executions and data analysis of their results, and the HPCToolkit performance analysis tools. The project team will engage the community to participate in the design and evaluation of the emerging components, as well as to adopt its components. <br/><br/>This project will have a wide range of impacts. First, software components built by the project will enable the development of sophisticated, high-quality, end-user performance, correctness, and security tools built by the project team, as well as others in academia, government, and industry. Software developed by the project team will help researchers and developers tackle testing, debugging, monitoring, analysis, and tuning of applications for systems at all scales. Second, end-user tools produced by the project have a natural place in the classroom to help students write efficient, correct, and secure programs. Third, components produced by the project will lower the barrier for new researchers to enter the field and build tools that have impact on production applications without years of investment. Fourth, the project will provide training for graduate students and interns in the area of software for performance, correctness, and security. Finally, through workshops and tutorials, the project will disseminate project results, provide training to enable others to leverage project software, and grow a community of tool researchers who depend on project components and thus have a strong motivation to help sustain project software into the future.<br/><br/>Modernizing open-source software components and tools for binary analysis will enable static analysis of application characteristics at the level of executable machine code, transformation of binaries to inject monitoring code, measurement to capture a detailed record of application?s interactions with all facets of a target platform, analysis of recorded data in parallel, and attribution of analysis results back to application source code in meaningful ways. Providing innovative, software components that support development of robust performance, correctness, and security tools will accelerate innovation by tools researchers and help them grapple with the increasing complexity of modern software. Of particular note, helping tools researchers and computational scientists grapple with the challenges of software for modern parallel systems and producing training materials that help people use this software, addresses several of the needs identified in the NSF Vision for Cyberinfrastructure for the 21st Century.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1449918","fuchsia_atom","SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Performance, Security, and Correctness Tools"
"1148127","SI2-SSE: Correctness Verification Tools for Extreme Scale Hybrid Concurrency","ACI","Software Institutes|INFORMATION TECHNOLOGY RESEARC","6/1/2012","4/24/2015","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Rajiv Ramnath","5/31/2016","$490,279.00 ","Mary Hall","ganesh@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","75 S 2000 E, SALT LAKE CITY, UT","841128930","8015816903","CSE","8004|1640","8004|8005|9150|7942|9251|1640|7433","$0.00 ","High Performance Computing is strategically important to national competitiveness. Advances in computational capabilities involve the use of unprecedented levels of parallelism: programming methods that involve billions of concurrent activities. Multiple styles of concurrency involving shared and distributed memory programming (""hybrid"") are necessary. Unfortunately, such programs are very difficult to debug using existing methods. This project develops formal (mathematically based) verification tools that can debug hybrid concurrent programs with very high certainty of bug elimination, while consuming only modest computational resources for verification. <br/><br/>The project develops execution-based tools that eliminate search over semantically equivalent alternative schedules as well as solver-based techniques that eliminate classes of bugs over single runs. Scalable methods based on non-determinism classification and heuristic execution-space reduction are also being developed. <br/><br/>Expected results include: (1) development of tools based on formal algorithmic techniques that verify large-scale hybrid programs; (2) amalgamation of incisive bug-hunting methods developed at other research organizations within formally based tools developed in our group; (3) incorporation of our verification tools and techniques within popular tool-integration frameworks; (4) large-scale case studies handled using our tools; and (5) training of undergraduate and graduate students on these advanced verification methods, building the talent pool vital to continued progress in high performance computing with applications to science and engineering, energy/sustainability, and homeland security.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148127","fuchsia_atom","SI2-SSE: Correctness Verification Tools for Extreme Scale Hybrid Concurrency"
"1147519","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling","ACI","Software Institutes|EXP PROG TO STIM COMP RES|GEOMORPHOLOGY & LAND USE DYNAM","6/1/2012","7/6/2012","Nicole Gasparini","LA","Tulane University","Standard Grant","Rajiv Ramnath","5/31/2016","$245,572.00 ","","ngaspari@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","6823 ST CHARLES AVENUE, NEW ORLEANS, LA","701185698","5048654000","CSE","8004|9150|7458","8005|7433|7458|8004","$0.00 ","Presently there are no widely adopted software conventions for holistic computational landscape models. To fill this important gap this project adapts and enhances existing landscape modeling codes by introducing a component-based approach to software development. This project adapts and enhances an existing model -- CHILD (Channel-Hill slope Integrated Landscape Development) -- to provide a set of interoperable, independent modeling components that provide flexible and modular approaches to landscape modeling which are fully compatible with the NSF-funded Community Surface Dynamics Modeling System CSDMS) infrastructure. In accord with the CSDMS architecture, the software to be developed will also employ the standards and tools of the Common Component Architecture (CCA) software architecture. Included is the design of an interface for communication with and between the developed components. The end result will be a set of independent, interoperable C++ software modeling modules that are compatible with the CSDMS modeling toolkit as well as the standards and tools of the CCA structure. The software will be tested against data on post-wildfire erosion.<br/><br/>This approach was selected to provide maximum flexibility to users by allowing them to plug-and-play, seamlessly linking together selected computing modules to enable custom combinations of components to support modeling for a wide variety of research topics. Work will include the development of a gridding engine to handle both regular and unstructured meshes and an interface for space-time rainfall input, as well as a surface hydrology component, a sediment erosion-deposition component, a vegetation component, and a simulation driver. <br/><br/>If successful this project will impact many branches of the Earth and environmental sciences by building a new modeling platform and creating essential software infrastructure for science with applications that span hydrology, soil erosion, tectonics, geomorphology, vegetation ecology, stratigraphy, and planetary science. Broader impacts of the work include creation of classroom modeling exercises for both undergraduate and high schools students, and support of a PI whose gender is under-represented in the STEM fields and who is employed at an institution in an EPSCoR state. It also includes workforce-training in computational geoscience for graduate students and a postdoc as well as minority undergraduates from Xavier University, a Historically Black University in the New Orleans area.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147519","fuchsia_atom","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling"
"1148330","SI2-SSE: Connecting Cyberinfrastructure with the Cooperative Computing Tools","ACI","Software Institutes","4/15/2012","3/12/2013","Douglas Thain","IN","University of Notre Dame","Standard Grant","Rajiv Ramnath","12/31/2016","$507,020.00 ","","dthain@nd.edu","940 Grace Hall","NOTRE DAME","IN","940 Grace Hall, NOTRE DAME, IN","465565708","5746317432","CSE","8004","8004|7433|8005|9251","$0.00 ","This project supports the maintenance and development of the Cooperative Computing Tools. This software package is designed to enable non-privileged users to<br/>harness hundreds to thousands of cores from multiple clusters, clouds, and grids simultaneously. The main components of the software package include Parrot, a virtual file system that interfaces with multiple distributed storage systems, and Makeflow, a workflow engine that interfaces with multiple computing systems. This project will develop, maintain, and support the software across a wide variety of operating systems and national scale cyberinfrastructure in support of high impact scientific applications in fields such as bioinformatics, biometrics, data mining, high energy physics, and molecular dynamics.<br/><br/>Large scale computing systems such as cluster, clouds, and grids now make it easy for end users to purchase large amounts of computing power at the touch of a button. However, these computing systems are difficult to harness because they each present a different user interface, principle of operation, and programming model. This project addresses this problem by supporting the development of the Cooperative Computing Tools, a software package that makes it possible for ordinary computer applications to move seamlessly between different service providers. The software is primarily of interest to researchers in scientific domains that require large amounts of computation. It is currently used by researchers in the fields of bioinformatics, biometrics, data mining, high energy physics, and molecular dynamics.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148330","fuchsia_atom","SI2-SSE: Connecting Cyberinfrastructure with the Cooperative Computing Tools"
"1148213","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments","ACI","ADVANCES IN BIO INFORMATICS|ECOSYSTEM STUDIES|Software Institutes|Cyber Secur - Cyberinfrastruc","8/1/2012","8/7/2012","Duane Edgington","CA","Monterey Bay Aquarium Research Institute","Standard Grant","Rajiv Ramnath","7/31/2016","$101,710.00 ","","duane@mbari.org","7700 SANDHOLDT RD","MOSS LANDING","CA","7700 SANDHOLDT RD, MOSS LANDING, CA","950399644","8317751803","CSE","1165|1181|8004|8027","8009|7434|1165|1181|8004|8027","$0.00 ","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148213","fuchsia_atom","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments"
"1148052","SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain","ACI","Software Institutes|INFORMATION TECHNOLOGY RESEARC","6/1/2012","5/29/2012","Barbara Chapman","TX","University of Houston","Standard Grant","Rajiv Ramnath","12/31/2016","$926,666.00 ","Edgar Gabriel","barbara.chapman@stonybrook.edu","4800 Calhoun Boulevard","Houston","TX","4800 Calhoun Boulevard, Houston, TX","772042015","7137435773","CSE","8004|1640","8009|7433|8004","$0.00 ","Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via ""black box"" tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications. `Opening up' the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.<br/><br/>The project will explore the information that can be shared 'across the software stack'. Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes. Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware. Working with the `Keeneland' NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today's petascale machines. Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148052","fuchsia_atom","SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain"
"1147422","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Liner Equations","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|Software Institutes|CDS&E-MSS","6/1/2012","6/20/2012","Ahmed Sameh","IN","Purdue University","Standard Grant","Almadena Y. Chtchelkanova","5/31/2016","$240,000.00 ","","sameh@cs.purdue.edu","Young Hall","West Lafayette","IN","Young Hall, West Lafayette, IN","479072114","7654941055","CSE","1253|7478|8004|8069","7433|8005|1253|7478","$0.00 ","Drs. Negrut, Sameh, and Knepley will investigate, produce, and maintain a methodology and its software implementation that leverage emerging heterogeneous hardware architectures to solve billion-unknowns linear systems in a robust, scalable, and efficient fashion. The two classes of problems targeted under this project are banded dense and sparse general linear systems.<br/><br/>This project is motivated by the observation that the task of solving a linear system is one of the most ubiquitous ingredients in the numerical solution of Applied Mathematics problems. It is relied upon for the implicit integration of Ordinary Differential Equation (ODE) and Differential Algebraic Equation (DAE) problems, in the numerical solution of Partial Differential Equation (PDE) problems, in interior point optimization methods, in least squares approximations, in solving eigenvalue problems, and in data analysis. In fact, the vast majority of nonlinear problems in Scientific Computing are solved iteratively by drawing on local linearizations of nonlinear operators and the solution of linear systems. Recent advances in (a) hardware architecture; i.e., the emergence of General Purpose Graphics Processing Unit (GP-GPU) cards, and (b) scalable solution algorithms, provide an opportunity to develop a new class of parallel algorithms, called SPIKE, which can robustly and efficiently solve very large linear systems of equations.<br/><br/>Drawing on its divide-and-conquer paradigm, SPIKE builds on several algorithmic primitives: matrix reordering strategies, dense linear algebra operations, sparse direct solvers, and Krylov subspace methods. It provides a scalable solution that can be deployed in a heterogeneous hardware ecosystem and has the potential to solve billion-unknown linear systems in the cloud or on tomorrow?s exascale supercomputers. Its high degree of scalability and improved efficiency stem from (i) optimized memory access pattern owing to an aggressive pre-processing stage that reduces a generic sparse matrix to a banded one through a novel reordering strategy; (ii) good exposure of coarse and fine grain parallelism owing to a recursive, divide-and-conquer solution strategy; (iii) efficient vectorization in evaluating the coupling terms in the divide-and-conquer stage owing to a CPU+GPU heterogeneous computing approach; and (iv) algorithmic polymorphism, given that SPIKE can serve both as a direct solver or an effective preconditioner in an iterative Krylov-type method.<br/><br/>In Engineering, SPIKE will provide the Computer Aided Engineering (CAE) community with a key component; i.e., fast solution of linear systems, required by the analysis of complex problems through computer simulation. Examples of applications that would benefit from this technology are Structural Mechanics problems (Finite Element Analysis in car crash simulation), Computational Fluid Dynamics problems (solving Navier-Stokes equations in the simulation of turbulent flow around a wing profile), and Computational Multibody Dynamics problems (solving Newton-Euler equations in large granular dynamics problems).<br/><br/>SPIKE will also be interfaced to the Portable, Extensible Toolkit for Scientific Computation (PETSc), a two decades old flexible and scalable framework for solving Science and Engineering problems on supercomputers. Through PETSc, SPIKE will be made available to a High Performance Computing user community with more than 20,000 members worldwide. PETSc users will be able to run SPIKE without any modifications on vastly different supercomputer architectures such as the IBM BlueGene/P and BlueGene/Q, or the Cray XT5. SPIKE will thus run scalably on the largest machines in the world and will be tuned for very different network and hardware topologies while maintaining a simple code base.<br/><br/>The experience collected and lessons learned in this project will augment a graduate level class, ?High Performance Computing for Engineering Applications? taught at the University of Wisconsin-Madison. A SPIKE tutorial and research outcomes will be presented each year at the International Conference for High Performance Computing, Networking, Storage and Analysis. A one day High Performance Computing Boot Camp will be organized each year in conjunction with the American Society of Mechanical Engineers (ASME) conference and used to disseminate the software outcomes of this effort. Finally, this project will shape the research agendas of two graduate students working on advanced degrees in Computational Science.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147422","fuchsia_atom","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Liner Equations"
"1148305","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling","ACI","GEOMORPHOLOGY & LAND USE DYNAM|Software Institutes","6/1/2012","5/6/2014","Erkan Istanbulluoglu","WA","University of Washington","Continuing grant","Daniel Katz","5/31/2015","$180,289.00 ","","erkani@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","4333 Brooklyn Ave NE, Seattle, WA","981950001","2065434043","CSE","7458|8004","7433|8005|7458|8004","$0.00 ","Presently there are no widely adopted software conventions for holistic computational landscape models. To fill this important gap this project adapts and enhances existing landscape modeling codes by introducing a component-based approach to software development. This project adapts and enhances an existing model -- CHILD (Channel-Hill slope Integrated Landscape Development) -- to provide a set of interoperable, independent modeling components that provide flexible and modular approaches to landscape modeling which are fully compatible with the NSF-funded Community Surface Dynamics Modeling System CSDMS) infrastructure. In accord with the CSDMS architecture, the software to be developed will also employ the standards and tools of the Common Component Architecture (CCA) software architecture. Included is the design of an interface for communication with and between the developed components. The end result will be a set of independent, interoperable C++ software modeling modules that are compatible with the CSDMS modeling toolkit as well as the standards and tools of the CCA structure. The software will be tested against data on post-wildfire erosion.<br/><br/>This approach was selected to provide maximum flexibility to users by allowing them to plug-and-play, seamlessly linking together selected computing modules to enable custom combinations of components to support modeling for a wide variety of research topics. Work will include the development of a gridding engine to handle both regular and unstructured meshes and an interface for space-time rainfall input, as well as a surface hydrology component, a sediment erosion-deposition component, a vegetation component, and a simulation driver. <br/><br/>If successful this project will impact many branches of the Earth and environmental sciences by building a new modeling platform and creating essential software infrastructure for science with applications that span hydrology, soil erosion, tectonics, geomorphology, vegetation ecology, stratigraphy, and planetary science. Broader impacts of the work include creation of classroom modeling exercises for both undergraduate and high schools students, and support of a PI whose gender is under-represented in the STEM fields and who is employed at an institution in an EPSCoR state. It also includes workforce-training in computational geoscience for graduate students and a postdoc as well as minority undergraduates from Xavier University, a Historically Black University in the New Orleans area.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148305","fuchsia_tree","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling"
"1148168","SI2-SSE: Development of a GPU Accelerated Gibbs Ensemble Monte Carlo Simulation Engine","ACI","Software Institutes|DMR SHORT TERM SUPPORT|CHEMISTRY PROJECTS|OFFICE OF MULTIDISCIPLINARY AC|MATERIALS AND SURFACE ENG","9/1/2012","6/7/2013","Jeffrey Potoff","MI","Wayne State University","Standard Grant","Rajiv Ramnath","8/31/2016","$336,000.00 ","Loren Schwiebert","jpotoff@wayne.edu","5057 Woodward","Detroit","MI","5057 Woodward, Detroit, MI","482023622","3135772424","CSE","8004|1712|1991|1253|1633","7433|8005|7237|7569|7573|9215|9216|9263|1982|9251|1253|1633|1712|1991|8004","$0.00 ","This award supports the development of a general purpose Gibbs Ensemble Monte Carlo (GEMC) simulation engine that uses low-cost graphics processing units (GPU) for acceleration. The primary objectives of this work are to develop and implement: 1) GPU accelerated configurational-bias methods, 2) efficient algorithms for the computation of Ewald sums on the GPU and 3) automated tuning of the code for different GPUs. This work builds on the PIs? existing particle-based GPU-GEMC engine and will introduce functionality that enables the simulation of biological processes and adsorption in porous materials. The code will be written to maintain compatibility with the file formats used by the software packages NAMD and VMD, simplifying simulation setup and data analysis. The resulting simulation engine will be released under the GNU General Public License v3 (GPLv3) and made available to users via the Internet. <br/><br/>The software tools developed with support from this award will enable high throughput computational screening of materials for CO2 sequestration, improved materials for the stabilization of drug dispersions, and provide molecular level insight to fundamental biological processes such as membrane fusion. The use of graphics processors for the bulk of the computational effort is expected to provide one to two orders of magnitude reduction in computational time compared to traditional serial, CPU bound code, which will allow for the simulation of systems of greater size and complexity than with existing tools. The development of the proposed GPU-accelerated Monte Carlo simulation engine will enhance the cyber-infrastructure of the biology chemistry, chemical engineering, materials science and physics communities. The GPU-GEMC simulation engine will be promoted through conference presentations at national and international meetings, via a dedicated website, and through publication in peer-reviewed literature.<br/><br/>This award will enhance education at the graduate and undergraduate levels. Research topics from this work will be integrated into existing courses on GPU computing and molecular simulation. Graduate and undergraduate students will have the opportunity to work as part of a multidisciplinary team composed of engineers and computer scientists. Students will be recruited from groups traditionally underrepresented in STEM fields.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148168","fuchsia_atom","SI2-SSE: Development of a GPU Accelerated Gibbs Ensemble Monte Carlo Simulation Engine"
"1147161","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics","ACI","OFFICE OF MULTIDISCIPLINARY AC|DYNAMICAL SYSTEMS|Software Institutes|CDS&E-MSS","6/1/2012","6/19/2012","Gregg Musiker","MN","University of Minnesota-Twin Cities","Standard Grant","Rajiv Ramnath","5/31/2016","$195,688.00 ","","musiker@math.umn.edu","200 OAK ST SE","Minneapolis","MN","200 OAK ST SE, Minneapolis, MN","554552070","6126245599","CSE","1253|7478|8004|8069","8005|7433|7683|1253|5514|7478|8004","$0.00 ","Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is ""to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area"". There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s. The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thiery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.<br/><br/>The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147161","fuchsia_atom","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics"
"1148255","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments","ACI","Software Institutes|Cyber Secur - Cyberinfrastruc|ADVANCES IN BIO INFORMATICS|ECOSYSTEM STUDIES","8/1/2012","3/12/2013","Shirley Dyke","IN","Purdue University","Standard Grant","Rajiv Ramnath","7/31/2016","$42,000.00 ","","sdyke@purdue.edu","Young Hall","West Lafayette","IN","Young Hall, West Lafayette, IN","479072114","7654941055","CSE","8004|8027|1165|1181","8009|7434|9251|1165|1181|8004|8027","$0.00 ","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148255","fuchsia_atom","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments"
"1148085","Collaborative Research SI2 SSE: Pipeline Framework for Ensemble Runs on Clouds","ACI","Software Institutes","5/1/2012","5/7/2012","Craig Mattocks","FL","University of Miami Rosenstiel School of Marine&Atmospheric Sci","Standard Grant","Daniel Katz","4/30/2014","$199,725.00 ","Brian Soden","cmattock@rsmas.miami.edu","4600 RICKENBACKER CSWY","KEY BISCAYNE","FL","4600 RICKENBACKER CSWY, KEY BISCAYNE, FL","331491031","3054214089","CSE","8004","8005|8004","$0.00 ","Cloud computing is an attractive computational resource for e-Science because of the ease with which cores can be accessed on demand, and because the virtual machine implementation that underlies cloud computing reduces the cost of porting a numeric or analysis code to a new platform. It is difficult to use cloud computing resources for large-scale, high throughput ensemble jobs however. Additionally, the computationally oriented researcher is increasingly encouraged to make data sets available to the broader community. For the latter to be achieved, using capture tools during experimentation to harvest metadata and provenance reduces the manual burden of marking up results. Better automatic capture of metadata and provenance is the only means by which sharing of scientific data can scale to meet the burgeoning explosion of data.<br/><br/>This project develops a pipeline framework for running ensemble simulations on the cloud; the framework has two key components: ensemble deployment and metadata harvest. Regarding the former, on commercial cloud platforms typically a much smaller number of jobs than desired can be started at any one time. An ensemble run will need to be pipelined to a cloud resource, that is, executed in well-controlled batches over a period of time. We will use platform features of Azure, and employ machine learning techniques to continuously refine the pipeline submission strategy and workflow strategies for ensemble parameter specification, pipelined deployment, and metadata capture. Regarding the latter key component, we expect to reduce the burden of sharing scientific datasets resulting from the use of cloud resources through automatic metadata and provenance capture and representation that aligns the metadata with emerging best practices in data sharing and discovery. Ensemble simulations result in complex data sets, whose reuse could be increased by expressive, granule and collection level metadata, including the lineage of the resulting products, to contribute towards trust.<br/><br/>In this project we focus on a compelling and timely application from climate research: One of the more immediate and dangerous impacts of climate change could be a change in the strength of storms that form over the oceans. In addition, as sea level rises due to global warming and melting of the polar ice caps, coastal communities will become increasingly vulnerable to storm surge. There have already been indications that even modest changes in ocean surface temperature can have a disproportionate effect on hurricane strength and the damage inflicted by these storms. In an effort to understand these impacts, modelers turn to predictions generated by hydrodynamic coastal ocean models such as the Sea, Lake and Overland Surges from Hurricanes (SLOSH) model. The proposed research advances the knowledge and understanding of probabilistic storm surge products by enhancements to the SLOSH model itself and through mechanisms that take advantage of commercial cloud resources. This knowledge is expected to have application in research, the classroom, and in operational settings.<br/><br/>The broader significance of the project is several-fold. Cloud computing is an important economic driver but it remains difficult for use in computationally driven scientific research. This project lowers the barriers to conducting e-Science research that utilizes cloud resources, specifically Azure. It will contribute tools to help researchers share, preserve, and publicize the scientific data sets that result from their research. Because we focus on and improve an application that predicts storm surge in response to sea level changes and severe storms, our work contributes to societal responses and adaptations to climate change, including planning and building the sustainable, hazard-resilient coastal communities of the future.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148085","fuchsia_atom","Collaborative Research SI2 SSE: Pipeline Framework for Ensemble Runs on Clouds"
"1148215","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments","ACI","Cyber Secur - Cyberinfrastruc|ADVANCES IN BIO INFORMATICS|Software Institutes|ECOSYSTEM STUDIES","8/1/2012","8/7/2012","Richard Christenson","CT","University of Connecticut","Standard Grant","Rajiv Ramnath","7/31/2016","$30,000.00 ","","rchriste@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","438 Whitney Road Ext., Storrs, CT","62691133","8604863622","CSE","8027|1165|8004|1181","8009|7434|1165|1181|8004|8027","$0.00 ","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148215","fuchsia_atom","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments"
"1147503","SI2-SSI: Collaborative Research: A Computational Materials Data and Design Environment","ACI","Software Institutes|DMR SHORT TERM SUPPORT|CHEMISTRY PROJECTS|OFFICE OF MULTIDISCIPLINARY AC","10/1/2012","9/20/2012","Gerbrand Ceder","MA","Massachusetts Institute of Technology","Standard Grant","Rajiv Ramnath","9/30/2017","$450,000.00 ","","gceder@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","77 MASSACHUSETTS AVE, Cambridge, MA","21394301","6172531000","CSE","8004|1712|1991|1253","7433|8009|7237|7569|7644|9216|9263|1982|1253|1712|1991|8004","$0.00 ","TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. The PIs will use state-of-the-art first principles quantum mechanical methods. Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution. Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems. New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments. This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.<br/><br/>NON-TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147503","fuchsia_atom","SI2-SSI: Collaborative Research: A Computational Materials Data and Design Environment"
"1148515","SI2-SSI: Distributed Workflow Management Research and Software in Support of Science","ACI","Software Institutes","4/1/2012","8/30/2012","Ewa Deelman","CA","University of Southern California","Standard Grant","Rajiv Ramnath","3/31/2017","$2,153,597.00 ","Miron Livny","deelman@isi.edu","University Park","Los Angeles","CA","University Park, Los Angeles, CA","900890001","2137407762","CSE","8004","8009|8004","$0.00 ","This award funds the enhancement of state-of-the-art workflow technologies and their promotion within a broad range of scientific domains. The overarching goal is to advance scientific discovery by providing scientists with tools that can manage computations on national cyberinfrastructure in a way that is reliable and scalable. <br/><br/>The key technology supported by this award is the Pegasus Workflow Management System (Pegasus). This program of work includes the development, support, and maintenance of Pegasus. Pegasus allows users to declaratively describe their workflow, then makes a plan that maps this description onto the available execution resources and executes the plan. This approach is scalable, reliable, and supports applications running on campus resources, clouds, and national cyberinfrastructure. <br/><br/>The work conducted under this award will 1) enhance the sustainability of the Pegasus software through the expanded adoption of sound software engineering practices and improved usability, 2) enhance core capabilities, especially in the area of data management, to meet user requirements and make Pegasus easier to integrate into end-to-end scientific environments, 3) promote the adoption of workflow management technologies within domain and computer sciences. <br/><br/>Intellectual Merit: Pegasus WMS brings innovative and powerful frameworks to the desk of the scientist. Through close collaboration with a broad community of engaged users, experimentation in large-scale distributed computing is made possible. This experimentation supports the development of new scientific workflow management concepts, frameworks and technologies. The proposed work also supports scientific reproducibility by providing a workflow management system that integrates and automates data, metadata, and provenance management functions. <br/><br/>Broader Impact: Pegasus WMS has been adopted by scientists from different domains and has been integrated into end-user environments such as workflow composition tools and portals. The program of outreach and education facilitated by this award will expand the impact of Pegasus through tutorials, workshops, meetings with potential users, and online materials. The proposed interface enhancements will allow more end-user environments to leverage Pegasus? capabilities and will extend the impact of Pegasus to a broader spectrum of users.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148515","fuchsia_atom","SI2-SSI: Distributed Workflow Management Research and Software in Support of Science"
"1147944","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse","ACI","LINGUISTICS|METHOD|MEASURE & STATS|SPECIAL PROJECTS - CISE|COLLABORATIVE RESEARCH|IIS SPECIAL PROJECTS|SOFTWARE & HARDWARE FOUNDATION|Software Institutes|","8/1/2012","5/2/2016","Nancy Ide","NY","Vassar College","Standard Grant","Rajiv Ramnath","7/31/2017","$994,057.00 ","James Pustejovsky, Eric Nyberg, Christopher Cieri","ide@cs.vassar.edu","124 Raymond Avenue","Poughkeepsie","NY","124 Raymond Avenue, Poughkeepsie, NY","126040657","8454377092","CSE","1311|1333|1714|7298|7484|7798|8004|O422","5983|7433|7944|8004|8009|9251|1311|1333","$0.00 ","The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. <br/><br/>The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. <br/><br/>By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147944","fuchsia_atom","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse"
"1148291","SI2-SSE: A GPU-Enabled Toolbox for Solving Hamilton-Jacobi and Level Set Equations on Unstructured Meshes","ACI","DYNAMICAL SYSTEMS|OFFICE OF MULTIDISCIPLINARY AC|Software Institutes|CDS&E-MSS","6/1/2012","3/31/2014","Robert Kirby","UT","University of Utah","Standard Grant","Rajiv Ramnath","5/31/2016","$531,999.00 ","Ross Whitaker","kirby@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","75 S 2000 E, SALT LAKE CITY, UT","841128930","8015816903","CSE","7478|1253|8004|8069","8005|9150|7433|9251|1253|7478|8004","$0.00 ","A variety of application domains from geophysics to biomedicine employ some form of Hamilton-Jacobi (H-J) mathematical models. These models are a natural way to express conservation properties, and the two most prevalent H-J models seen in the literature are the Eikonal equation (a static H-J model based upon Fermat's Principle for determining minimal paths) and the Level-Set equations (a time-dependent H-J model used for addressing moving interface problems). The goal of this<br/>effort is to develop, test, document and distribute a collection of software tools for efficiently solving several classes of equations of H-J type -- in particular, Eikonal (minimal path) equations and Level-set equations -- on unstructured (triangular and tetrahedral) meshes using commodity streaming architectures. The PIs have previously demonstrated the feasibility of efficiently solving H-J equations on GPUs; this effort seeks to both scientific extend previous work as well as solidify the software into a publicly available tool suite.<br/><br/>The intellectual merit of this effort is the development of efficient algorithmic strategies for mapping numerical methods for solving H-J equations on unstructured meshes to commodity streaming architectures. The proposed work will tackle several important technical challenges. One challenge is maintaining sufficient computational density on the parallel computational units (blocks), especially as we move to 3D unstructured meshes. A second technical challenge is the loss in efficiency that comes with communication between blocks. The solutions to these challenges will allow us to exploit currently available commodity streaming architectures that promising to provide teraflop performance on the desktop, which will be a boon for a variety of communities that rely on computationally expensive, simulation-based experiments. By overcoming the tedious and non-trivial step of developing and distributing software for solving H-J equations on unstructured meshes using commodity streaming architectures, the impact of this work has both longevity and ubiquity in a wide range of applications in diverse fields such as basic science, medicine, and engineering.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148291","fuchsia_atom","SI2-SSE: A GPU-Enabled Toolbox for Solving Hamilton-Jacobi and Level Set Equations on Unstructured Meshes"
"1148125","Collaborative Research: SI2-SSI: A Linear Algebra Software Infrastructure for Sustained Innovation in Computational Chemistry and other Sciences","ACI","OFFICE OF MULTIDISCIPLINARY AC|INFORMATION TECHNOLOGY RESEARC|DMR SHORT TERM SUPPORT|CHEMISTRY PROJECTS|Software Institutes","6/1/2012","5/17/2013","Robert van de Geijn","TX","University of Texas at Austin","Standard Grant","Evelyn M. Goldfield","5/31/2016","$1,701,189.00 ","Don Batory, John Stanton, Victor Eijkhout, Margaret Myers","rvdg@cs.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","101 E. 27th Street, Suite 5.300, Austin, TX","787121532","5124716424","CSE","1253|1640|1712|1991|8004","8009|7433|9216|9263|7569|1253|1640|1712|1991|8004","$0.00 ","Linear algebra is a branch of mathematics that provides the foundation for a significant fraction of computations in science and engineering. Historically, the importance of linear algebra is such that highly specialized codes written by computer scientists have been used by the community of scientific programmers as a vital part of their application programs. With the rapid changes in computer architecture during the last several years, it would seem that corresponding modifications in linear algebra routines would be warranted. However, such progress is not in evidence; the development of such routines has been just incremental, involving successive rewrites of routines that had their genesis in the last quarter of the last century. Correspondingly, there is something<br/>of a disconnect between the current `state-of-the-art' linear algebra libraries, modern computer architectures, and applications that utilize the libraries.<br/><br/>The new project will create a new, vertically integrated framework and implementation that revisits every layer of software, from low-level kernels to higher level functionality. The vertical integration is completed with a new generation of software for computational<br/>chemistry applications, guaranteeing that the developed software, to be freely available to the public, supports sustained innovation in that domain and other sciences. The development builds on the FLAME project, which has been funded by NSF and industry for more than a decade.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148125","fuchsia_atom","Collaborative Research: SI2-SSI: A Linear Algebra Software Infrastructure for Sustained Innovation in Computational Chemistry and other Sciences"
"1148493","SI2-SSI: A Sustainable Community Software Framework for Petascale Earthquake Modeling","ACI","Geotechnical Engineering and M|PetaApps|Software Institutes","8/1/2012","8/7/2012","Thomas Jordan","CA","University of Southern California","Standard Grant","Rajiv Ramnath","7/31/2016","$2,522,784.00 ","Kim Olsen, Yifeng Cui, Jacobo Bielak","tjordan@usc.edu","University Park","Los Angeles","CA","University Park, Los Angeles, CA","900890001","2137407762","CSE","1636|7691|8004","037E|043E|1576|7433|8009","$0.00 ","Earthquakes have major economic and societal consequences as can be seen from the aftermath of the recent large earthquakes in Japan, Chile, and New Zealand. This multidisciplinary project, which includes both geoscientists, computer scientists, and structural engineers, integrates high-level and middle-level scientific software elements developed by the Southern California Earthquake Center (SCEC) into a software environment for integrated seismic modeling that can be used for seismic hazard analysis. The framework includes integration of community velocity models, codes for dynamic and pseudo-dynamic rupture generation, deterministic and stochastic earthquake engines, and the applications necessary to employ forward simulations in two types of inverse problems: seismic source imaging and full 3D tomography. Modifications to already existing software packages slated to be significantly enhanced in the course of the workflow will allow simulations to be run on petascale machines and allow the better managing of scientific workflows. The work also focuses on software lifecycle issues such as model formation, verification, prediction, and validation and support the use of petascale computers by earthquake scientists. The goal of the project is to facilitate the incorporation of better theory and data into computationally intensive modeling of earthquake processes. Software will be designed to interface smoothly with OpenSHA, as well as OpenSEES, PEER, and NEES. Project partners will also develop and test two computational platforms, one that will have a user-friendly interface for calculating seismographs and the other will generate large suites of simulations for a layered earthquake hazard model. Models will be validated against datasets for 13 well-recorded historic California earthquakes of magnitude 6.0 or higher. The initial API will take advantage of the asynchronous IO features of Fortran 2003 with plans for adding C/C++ and Python interfaces. All codes developed will be open-source and publicly available and software distribution will be accompanied by sample input datasets and example forecast results. Broader impacts include the development of a new generation of time-dependent earthquake forecasts to produce ground-shake hazard maps, partnership with a federal agency and the private sector. It also includes a component of student and postdoctoral training and outreach to user communities. Undergraduate interns, many of whom have historically been from groups under-represented in STEM fields, will be trained in use of the software during an 8-week summer training course.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148493","fuchsia_atom","SI2-SSI: A Sustainable Community Software Framework for Petascale Earthquake Modeling"
"1147466","SI2-SSI Collaborative Research: A Computational Materials Data and Design Environment","ACI","Software Institutes|DMR SHORT TERM SUPPORT|CHEMISTRY PROJECTS|OFFICE OF MULTIDISCIPLINARY AC","10/1/2012","9/11/2013","Alan Dozier","KY","University of Kentucky Research Foundation","Standard Grant","Rajiv Ramnath","9/30/2017","$298,999.00 ","Raphael Finkel","adozier@uky.edu","109 Kinkead Hall","Lexington","KY","109 Kinkead Hall, Lexington, KY","405260001","8592579420","CSE","8004|1712|1991|1253","7433|8009|9150|7237|7569|7644|9216|9263|1982|1253|1712|1991|8004","$0.00 ","TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. The PIs will use state-of-the-art first principles quantum mechanical methods. Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution. Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems. New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments. This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.<br/><br/>NON-TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147466","fuchsia_atom","SI2-SSI Collaborative Research: A Computational Materials Data and Design Environment"
"1147454","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling","ACI","ECOSYSTEM STUDIES|GEOMORPHOLOGY & LAND USE DYNAM|Software Institutes","6/1/2012","4/25/2014","Gregory Tucker","CO","University of Colorado at Boulder","Continuing grant","Daniel Katz","5/31/2015","$224,124.00 ","","gtucker@cires.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","3100 Marine Street, Room 481, Boulder, CO","803031058","3034926221","CSE","1181|7458|8004","7433|8005|1181|7458|8004","$0.00 ","Presently there are no widely adopted software conventions for holistic computational landscape models. To fill this important gap this project adapts and enhances existing landscape modeling codes by introducing a component-based approach to software development. This project adapts and enhances an existing model -- CHILD (Channel-Hill slope Integrated Landscape Development) -- to provide a set of interoperable, independent modeling components that provide flexible and modular approaches to landscape modeling which are fully compatible with the NSF-funded Community Surface Dynamics Modeling System CSDMS) infrastructure. In accord with the CSDMS architecture, the software to be developed will also employ the standards and tools of the Common Component Architecture (CCA) software architecture. Included is the design of an interface for communication with and between the developed components. The end result will be a set of independent, interoperable C++ software modeling modules that are compatible with the CSDMS modeling toolkit as well as the standards and tools of the CCA structure. The software will be tested against data on post-wildfire erosion.<br/><br/>This approach was selected to provide maximum flexibility to users by allowing them to plug-and-play, seamlessly linking together selected computing modules to enable custom combinations of components to support modeling for a wide variety of research topics. Work will include the development of a gridding engine to handle both regular and unstructured meshes and an interface for space-time rainfall input, as well as a surface hydrology component, a sediment erosion-deposition component, a vegetation component, and a simulation driver. <br/><br/>If successful this project will impact many branches of the Earth and environmental sciences by building a new modeling platform and creating essential software infrastructure for science with applications that span hydrology, soil erosion, tectonics, geomorphology, vegetation ecology, stratigraphy, and planetary science. Broader impacts of the work include creation of classroom modeling exercises for both undergraduate and high schools students, and support of a PI whose gender is under-represented in the STEM fields and who is employed at an institution in an EPSCoR state. It also includes workforce-training in computational geoscience for graduate students and a postdoc as well as minority undergraduates from Xavier University, a Historically Black University in the New Orleans area.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147454","fuchsia_tree","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling"
"1148011","SI2-SSI: Collaborative Research: A Computational Materials Data and Design Environment","ACI","Software Institutes|OFFICE OF MULTIDISCIPLINARY AC|DMR SHORT TERM SUPPORT|CHEMISTRY PROJECTS","10/1/2012","9/20/2012","Dane Morgan","WI","University of Wisconsin-Madison","Standard Grant","Rajiv Ramnath","9/30/2017","$1,050,000.00 ","","ddmorgan@wisc.edu","21 North Park Street","MADISON","WI","21 North Park Street, MADISON, WI","537151218","6082623822","CSE","8004|1253|1712|1991","7433|8009|7237|7569|7644|9216|9263|1982|1253|1712|1991|8004","$0.00 ","TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. The PIs will use state-of-the-art first principles quantum mechanical methods. Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution. Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems. New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments. This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.<br/><br/>NON-TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148011","fuchsia_atom","SI2-SSI: Collaborative Research: A Computational Materials Data and Design Environment"
"1148461","SI2-SSE: Supporting Generic Programming in C++ for Modular and Reliable Large-Scale Software","ACI","INFORMATION TECHNOLOGY RESEARC|Software Institutes","9/1/2012","5/31/2012","Gabriel Dos Reis","TX","Texas A&M Engineering Experiment Station","Standard Grant","Rajiv Ramnath","8/31/2016","$499,999.00 ","Bjarne Stroustrup","gdr@cs.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","TEES State Headquarters Bldg., College Station, TX","778454645","9798477635","CSE","1640|8004","7942|8005|8004","$0.00 ","Generic programming has the potential of an effective methodology for building large-scale, reliable, maintainable, and efficient software artifacts. It is supported by the C++ programming language through the ""template"" mechanism. In the hands of experts, C++ templates are formidable abstraction tools, key to the success of libraries such as the Standard Template Library, and many freely available and commercial software libraries and products. Unfortunately, the practice of template-based structured generic programming remains restricted to relatively few highly trained individuals. <br/><br/>A primary objective of this project is to investigate and develop software tools and programming models that support scalable and modular generic libraries. Bringing structured generic programming methodology to mainstream at the scale done for object-oriented programming entails the invention of new programming language constructs and compiler construction techniques that go beyond conventional technologies. In particular, the apparent complexities of templates and arcane technical details must be concealed; code generation has to surpass C++'s current successful applications of templates both in quality and compile time for industrial scale programs. At the core of this project is the investigation of a direct linguistic support for requirements on template arguments (""concepts""), and their implementations in an open source compiler and libraries made freely available to the public, the research and education community.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148461","fuchsia_atom","SI2-SSE: Supporting Generic Programming in C++ for Modular and Reliable Large-Scale Software"
"1148485","Collaborative Research: SI2-SSI: A Linear Algebra Software Infrastructure for Sustained Innovation in Computational Chemistry and other Sciences","ACI","Software Institutes","6/1/2012","6/28/2012","Jeff Hammond","IL","University of Chicago","Standard Grant","Rajiv Ramnath","5/31/2015","$104,710.00 ","","jeff_hammond@acm.org","5801 South Ellis Avenue","Chicago","IL","5801 South Ellis Avenue, Chicago, IL","606375418","7737028669","CSE","8004","8009|1253|1640|1712|1991|7433|8004|9216|9263","$0.00 ","Linear algebra is a branch of mathematics that provides the foundation for a significant fraction of computations in science and engineering. Historically, the importance of linear algebra is such that highly specialized codes written by computer scientists have been used by the community of scientific programmers as a vital part of their application programs. With the rapid changes in computer architecture during the last several years, it would seem that corresponding modifications in linear algebra routines would be warranted. However, such progress is not in evidence; the development of such routines has been just incremental, involving successive rewrites of routines that had their genesis in the last quarter of the last century. Correspondingly, there is something of a disconnect between the current ""state-of-the-art"" linear algebra libraries, modern computer architectures, and applications that utilize the libraries.<br/><br/>The new project will create a new, vertically integrated framework and implementation that revisits every layer of software, from low-level kernels to higher level functionality. The vertical integration is completed with a new generation of software for computational chemistry applications, guaranteeing that the developed software, to be freely available to the public, supports sustained innovation in that domain and other sciences. The development builds on the FLAME project, which has been funded by NSF and industry for more than a decade.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148485","fuchsia_atom","Collaborative Research: SI2-SSI: A Linear Algebra Software Infrastructure for Sustained Innovation in Computational Chemistry and other Sciences"
"1535070","SI2-SSE: AttackTagger: Early Threat Detection for Scientific Cyberinfrastructure","ACI","Software Institutes","9/1/2015","9/8/2015","Alexander Withers","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rajiv Ramnath","8/31/2018","$499,136.00 ","Ravishankar Iyer, Randal Butler, Zbigniew Kalbarczyk, Adam Slagell","alexw1@illinois.edu","SUITE A","CHAMPAIGN","IL","SUITE A, CHAMPAIGN, IL","618207473","2173332187","CSE","8004","7433|8005|8004","$0.00 ","The cyber infrastructure that supports science research (such as the cyberinfrastructure that provides access to unique scientific instrumentation such as a telescope, or an array of highly distributed sensors placed in the field, or a computational supercomputing center) faces the daunting challenge of defending against cyber attacks. Modest to medium research project teams have little cyber security expertise to defend against the increasingly diverse, advanced and constantly evolving attacks. Even larger facilities that have with security expertise are often overwhelmed with the amount of security log data they need to analyze in order to identify attackers and attacks, which is the first step to defending against them. The challenges of the traditional approach of identifying an attacker are amplified by the lack of tools and time to detect attacks skillfully hidden in the noise of ongoing network traffic. The challenge is not necessarily in deploying additional monitoring but to identify this malicious traffic by utilizing all available information found in the plethora of security, network, and system logs that are already being actively collected. This project proposes to build and deploy, is needed in research environments, an advanced log analysis tool, named AttackTagger, that can scale to be able to address the dramatic increase in security log data, and detect emerging threat patterns in today's constantly evolving security landscape. AttackTagger will make science research in support of national priorities more secure.<br/><br/>AttackTagger will be a sophisticated log analysis tool designed to find potentially malicious activity, such as credential theft, by building factor graph models for advanced pattern matching. AttackTagger will integrate with existing security software so as to be easily deployable within existing security ecosystems and to offload processing and computational work onto better suited components. It can consume a wide variety of system and network security logs. AttackTagger accomplishes advanced pattern matching by utilizing a Factor Graph model, which is a type of probabilistic graphical model that can describe complex dependencies among random variables using an undirected graph representation, specifically a bipartite graph. The bipartite graph representation consists of variable nodes representing random variables, factor nodes representing local functions (or factor functions , and edges connecting the two types of nodes. Variable dependencies in a factor graph are expressed using a global function, which is factored into a product of local functions. In the practice of the security domain, using factor graphs is more flexible to define relations among the events and the user state compared to Bayesian Network and Markov Random Field approaches. Specifically, using factor graphs allows capturing sequential relation among events and enables integration of the external knowledge, e.g., expert knowledge or a user profile.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535070","fuchsia_atom","SI2-SSE: AttackTagger: Early Threat Detection for Scientific Cyberinfrastructure"
"1533581","SI2-SSE: Fast Dynamic Load Balancing Tools for Extreme Scale Systems","ACI","Software Institutes","10/1/2015","7/8/2015","Mark Shephard","NY","Rensselaer Polytechnic Institute","Standard Grant","Rajiv Ramnath","9/30/2018","$500,000.00 ","Cameron Smith","shephard@rpi.edu","110 8TH ST","Troy","NY","110 8TH ST, Troy, NY","121803522","5182766000","CSE","8004","7433|8005","$0.00 ","Massively parallel computing combined with scalable simulation workflows that can reliably model systems of interest are central to the continued quest of scientists, engineers, and other practitioners to address advances in scientific discovery, engineering design, and medical treatment. However, to meet their potential, these methods must be able to operate efficiently and scale on massively parallel computers executing millions of processes. Reaching the goal of millions of parallel processes requires new methods in which the computational workload is extremely well balanced and interprocessor communications overheads are minimized. Attaining such parallel performance is greatly complicated in realistic simulation workflows where the models and their discrete computer representation must evolve to ensure simulation reliability, or to account for changing input streams. To address the need to obtain workload balance with controlled communications, various algorithms and associated software, referred to as load balancing procedures, have been, and continue to be, developed. To be effective in the execution of simulation workflows in which the workload evolves, the load balancing procedures must be applied dynamically at multiple points in the simulation. Current load balancing techniques demonstrate two deficiencies when applied as dynamic load balancing procedures at very large numbers of compute cores (e.g., greater than 100,000 cores): They become a major fraction of the total parallel computation (in some cases never finishing within an allocation) and they do not maintain good load balance for simulation steps that must balance based on multiple criteria. Building on initial efforts to improve dynamic load balancing methods for adaptive unstructured mesh applications, the goal of the proposed research is to develop fast multicriteria dynamic load balancing methods that are capable of quickly producing well balanced computations, with well controlled communications, for a wide variety of applications. <br/><br/>An important characteristic of the dynamic load balancing procedures to be developed is generalizing the graph to account for multiple types of computational entities and interactions. The initial ideas for supporting multiple entity types came from consideration balancing finite element calculations that must consider multiple orders of mesh entities. These concepts will be refined and generalized to support multiple applications areas. An additional development will be fast hybrid dynamic load balancing methods that are combinations of ""geometric"", standard graph, and multicriteria graph methods in which the individual methods can be executed globally of at a more local level (such as at the node level). The dynamic load balancing method to be developed will be demonstrated on three applications in which the workload, and its distribution, is changing as the simulation proceeds. The applications will be adaptive mesh simulations, adaptive multiscale modeling, and massive scale free graphs. These applications will be carried out on available massively parallel computers where examples on >1 million cores will be demonstrated. A goal of the dynamic load balancing methods to be developed will be to attain scalability, and do so with controlled data movement such that the wall clock time and energy used is substantially less than that required for an equivalent accuracy non-adaptive calculation.<br/><br/>The software produced by this project will be made available as open source components. These developments coupled with efforts to support users in applying them in the development of new simulation tools will impact many research communities. Based on past and present efforts, the PIs fully expect that technologies developed in this project will also be integrated into future industrial software systems.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1533581","fuchsia_atom","SI2-SSE: Fast Dynamic Load Balancing Tools for Extreme Scale Systems"
"1535232","SI2-SSE: EASE: Improving Research Accountability through Artifact Evaluation","ACI","Software Institutes|SPECIAL PROJECTS - CCF","9/1/2015","6/16/2015","Bruce Childers","PA","University of Pittsburgh","Standard Grant","Rajiv Ramnath","8/31/2018","$499,515.00 ","Daniel Mosse'","childers@cs.pitt.edu","University Club","Pittsburgh","PA","University Club, Pittsburgh, PA","152132303","4126247400","CSE","8004|2878","7433|8005","$0.00 ","Research in computer systems, particularly in the first stages of creating a new innovation, relies almost exclusively on software prototypes, simulators, benchmarks, and data sets to understand the benefits and costs of new ideas in computers, ranging from consumer devices to exascale systems. These artifacts are used to evaluate new capabilities, algorithms, bottlenecks and trade-offs. Empirical study is behind the rapid pace of innovation in creating faster, lower energy and more reliable systems. This experimental approach lies at the core of development that fuels the nation's information economy. Given the critical importance of experimental study to developing new computer systems, several efforts are underway to curate experimental results through accountable research. One effort, Artifact Evaluation (AE), is being adopted to promote high quality artifacts and experimentation, including making public the experimental information necessary for reproducibility. However, the rapid adoption of AE is hampered by technical challenges that create a high barrier to the process: there is no consistent or simple environment, or mechanism, to package and reproduce experiments for AE. Authors rely on their own approaches, leading to much time consumed, as well as considerable variability in the ways materials are prepared and evaluated, unnecessarily obstructing the AE process. <br/><br/>To overcome the technical challenges with AE, and to more broadly encourage adoption of AE in computer science and engineering research, this project is developing a software infrastructure, Experiment and Artifact System for Evaluation (EASE), to create and run experiments specifically for AE, in which authors create, conduct and share artifacts and experiments. It allows for repeating, modifying, and extending experiments. Authors may also use EASE to package and upload their experiments for archival storage in a digital library. EASE is being developed and deployed for two use cases, namely compilers and real-time systems, keeping the project tractable to address specific needs. These communities have overlapping but also distinct requirements, helping to ensure EASE can also be extended and<br/>used by other computer systems research communities as well.<br/><br/>EASE will be release as open source software, based on an Experiment Management System (EMS) previously developed by the project investigator in a project call Open Curation for Computer Architecture Modeling (OCCAM), used to define and conduct experiments using computer architecture simulators. Using EMS as a starting point, EASE will provide AE support, by: 1) separating EMS from OCCAM's repository and hardware services, transforming the EMS infrastructure into EASE, a fully standalone, sustainable, and extensible platform for AE; 2) supporting record and replay (for repeating and reproducing results, as well as provenance) of artifacts and experiments as part of normal development and experimental practice to ease participation in AE by authors and evaluators; 3) supporting artifacts,workflows of artifacts and experiments that run directly on a machine, including specialized hardware and software, and run indirectly on a simulator or emulator; 4) allowing both user-level (artifacts and experiments as user processes) and system-level (artifacts and experiments involving kernel changes) innovations; 5) providing consistent/uniform access, whether locally or remotely, to artifacts and experiments; 6) simplifying viewing, running, modifying, and comparing experiments by innovators (i.e., during innovation development), artifact evaluators (during AE), and archive users (after publication); 7) enabling indexing (object locators and search tags) and packaging of artifacts and experiments for AE and for archival deployment (e.g., to ACM?s or IEEE?s Digital Library); and 8) refining, expanding, generalizing, and documenting EASE to ensure it is robust, maintainable and extensible, and that it can be used and sustained by different CSR communities (starting with real-time and compilers, given their different artifacts, data and methods).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535232","fuchsia_atom","SI2-SSE: EASE: Improving Research Accountability through Artifact Evaluation"
"1565676","Inspiration, Frustration, and Fascination: An Excursion into Low-Oxidation State Main Group Chemistry","CHE","Chemical Synthesis","6/1/2016","4/11/2016","Gregory Robinson","GA","University of Georgia Research Foundation Inc","Standard Grant","Sarah Stoll","5/31/2019","$510,000.00 ","Yuzhong Wang","robinson@chem.uga.edu","310 East Campus Rd","ATHENS","GA","310 East Campus Rd, ATHENS, GA","306021589","7065425939","MPS","6878","","$0.00 ","The Chemical Synthesis Program of the Chemistry Division supports the research project by Professor Gregory H. Robinson, a faculty member in the Department of Chemistry at The University of Georgia. Professor Robinson and his research team are studying the unique chemistry of low-oxidation state main group chemical compounds. The goal of this research is to exploit the unique stabilizing effects of organic bases (a class of compounds known as carbenes) on highly reactive main group molecules. For example, important molecules like disilicon (Si2) are only detectable at extremely low temperatures. In contrast, diphosphorus (P2) is typically only detectable at very high temperatures. The Robinson team has developed a means to stabilize molecules like Si2 and P2 (and many others) at room temperature, thus allowing the convenient study of the structure and reactivity of these important molecules. In particular, these researchers recently reported the first stable molecular examples of silicon oxides. This project investigates the synthesis of more ambitious silicon oxides. This chemistry has the potential for us to learn more about the silicon-oxygen interface with possible implications to computer chips and semiconductors. These researchers will also attempt to synthesize molecules containing large silicon and arsenic clusters. This project lies at the heart of main group chemistry, a field of inorganic chemistry that has traditionally received more emphasis in Europe. Outreach activities involving women and traditionally under-represented groups is central to this research. The students engaged in this work are acquiring valuable synthetic and experimental skills that make them highly valuable in the employment market.<br/><br/>An ambitious program to explore challenging areas of low-oxidation main group chemistry is underway. The Robinson laboratory has developed N-heterocyclic carbenes (NHC or L:) and N-heterocyclic dicarbene (NHDC) derivatives that are being used as a unique platform from which many unusual low-oxidation state main group species can be synthetically stabilized. Major synthetic goals in this work include: (a) carbene-based multisilylenes; (b) carbene-stabilized silicon atom and clusters; (c) carbene-stabilized heteronuclear diatomic molecules [i.e., silicon carbides, diatomic III-V (13-15) species, arsenic phosphide (AsP)]. The recent report by this laboratory of carbene-stabilization of elusive silicon oxides (Nature Chem. 2015, 7, 509) has encouraged these workers to develop the long-sought molecular chemistry of SOx. Consequently, the syntheses of a series of carbene-stabilized silicon oxides (such as SiO, SiO2, Si2O, Si2O2, and Si3O6, etc.) and silicon hydrides [Si3H2 and Si2H2 (parent disilyne)] are being pursued. These carbene-stabilized silicon oxides may be further utilized to develop the corresponding transition-metal-modified derivatives and transfer silicon oxide clusters into organic or organometallic substrates. In addition, carbene-stabilized bis-silylenes are explored as potential transfer agents for the disilyne unit. The transition metal chemistry of carbene-stabilized zero-oxidation-state main group species are being examined in the work. Research findings from the Robinson laboratory have repeatedly challenged traditional theories of structure and bonding in inorganic chemistry and some of this has begun to appear in chemistry textbooks. Students engaged in this work are acquiring valuable synthetic, crystallographic, and computational skills. The Robinson laboratory has a positive record of extending the chemistry enterprise to larger segments of the human resource as a number of women and African Americans have been trained in his laboratory. In addition, Professor Robinson has developed a popular seminar course entitled ""Molecules That Changed History"".",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1565676","chartreuse_atom","Inspiration, Frustration, and Fascination: An Excursion into Low-Oxidation State Main Group Chemistry"
"1535081","SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data","ACI","Software Institutes","9/1/2015","8/5/2015","Jing Yang","NC","University of North Carolina at Charlotte","Standard Grant","Rajiv Ramnath","8/31/2018","$200,950.00 ","","Jing.Yang@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","9201 University City Boulevard, CHARLOTTE, NC","282230001","7046871888","CSE","8004","7433|8005","$0.00 ","Advanced technologies in sensing and computing have created urban trajectory datasets of humans and vehicles travelling over urban networks. Understanding and analyzing the large-scale, complex data reflecting city dynamics is of great importance to enhance both human lives and urban environments. Domain practitioners, researchers, and decision-makers need to manage, query and visualize such big and dynamics data to conduct both exploratory and analytical tasks. To support these tasks, this project develops a open source software tool, named TrajAnalytics, which integrates computational techniques of scalable trajectory database, intuitive and interactive visualization, and high-end computers, to explicitly facilitate interactive data analytics of the emerging trajectory data.<br/><br/>The software provides a new platform for researchers in transportation assessment and planning to directly study moving populations in urban spaces. Researchers in social, economic and behavior sciences can use the software to understand the complicated mechanisms of urban security, economic activities, behavior trends, and so on. Specifically, this software advances the research activities in the community of transportation studies. The software also acts as an outreach platform allowing government agencies to communicate more effectively with the public with the real-world dynamics of city traffic, vehicles, and networks. The integration of research and education prepares the next generation workforce, where students across multi-disciplines can benefit through their participation in the development and use of the software.<br/><br/>In order to lay the foundations for effective analysis of urban trajectory datasets, this software (1) facilitates easy access and unprecedented capability for researchers and analysts with a cloud-based storage and computing infrastructure; (2) develops a parallel graph based data model, named TrajGraph, where massive trajectories are managed on a large-scale graph created from urban networks which is naturally amenable for studying urban dynamics; and (3) provides a visualization interface, named TrajVis, which supports interactive visual analytics tasks with a set of visualization tools and interaction functions. A variety of transportation-related researchers from geography, civil engineering, computer science participate in the design, evaluation, and use of the software. This robust and easy-to-use software enables the users to conduct iterative, evolving information foraging and sense making.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535081","fuchsia_atom","SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data"
"1534941","SI2-SSE: Enhanced Software Tools for Biomolecular Free Energy Calculations","ACI","DMREF|DMR SHORT TERM SUPPORT|Software Institutes|OFFICE OF MULTIDISCIPLINARY AC","9/1/2015","8/24/2015","Celeste Sagui","NC","North Carolina State University","Standard Grant","Rajiv Ramnath","8/31/2018","$500,000.00 ","Christopher Roland","sagui@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","CAMPUS BOX 7514, RALEIGH, NC","276957514","9195152444","CSE","8292|1712|8004|1253","7433|8005|8400|7237|7569|7573|9215|9216|9263|1982","$0.00 ","The central role of atomistic simulation-based free energy calculations for basic chemical and biological research is now firmly established. The free energy is the central quantity that guides the behavior of a system at, or near, equilibrium, determining such characteristics as molecular conformations, molecular binding, chemical reactions, etc. Unfortunately, accurate and reliable free energies are very difficult to calculate, particularly for many biomolecular systems characterized by rugged free energy landscapes. Hence, special techniques are required for calculating such free energy landscapes. Having previously developed the so-called Adaptively Biased Molecular Dynamics method, enhanced with other methods (either developed or adapted by our group) for phase space sampling, this project will further develop the capabilities of this software thereby enlarging the kinds of simulation problems that can be tackled. The software will be released to the public as open source software and as parts of the AMBER software package. In terms of scientific applications, this project also will investigate the conformation and properties of proteins with Intrinsically Disordered Regions, and the binding of DNA with a special class of transcription factors.<br/><br/>The single most important quantity for describing biomolecular and chemical systems in equilibrium is the free energy. However, calculating free energies is notoriously difficult and computationally expensive. This problem is particularly pressing for many biomolecular systems, which are characterized by complicated free energy landscapes that are hard to explore with regular molecular dynamics simulations. The PIs previously developed the Adaptively Biased Molecular Dynamics method (ABMD) with Multiple Walkers, and Replica Exchange Molecular Dynamics (REMD) extensions. ABMD is an umbrella sampling method with a time-dependent biasing potential for calculating free energy landscapes and conformational sampling. The software suite, along with Steered Molecular Dynamics (SMD) extensions, has been released to the public as part of the AMBER software package. This project will take this set of software tools to the next level by developing the capability to handle quaternion-based collective variables, the so-called Milestoning technique, and self-directed, interacting multiple walkers. Envisioned applications relate to proteins with Intrinsically Disordered Regions (IDRs) and mechanisms of DNA binding by basic Helix-Loop-Helix (bHLH) domains in transcription factors.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1534941","fuchsia_atom","SI2-SSE: Enhanced Software Tools for Biomolecular Free Energy Calculations"
"1535191","SI2-SSE: Algorithms and Tools for Data-Driven Executable Biology","ACI","Software Institutes|ADVANCES IN BIO INFORMATICS","10/1/2015","9/8/2015","Rastislav Bodik","WA","University of Washington","Standard Grant","Rajiv Ramnath","9/30/2018","$499,784.00 ","Aditya Virendra Thakur","bodik@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","4333 Brooklyn Ave NE, Seattle, WA","981950001","2065434043","CSE","8004|1165","7433|8005","$0.00 ","This project seeks to understand the signaling mechanisms that control cellular activities such as cell division, cell growth, and cell differentiation. Errors in cellular signaling cause diseases such as cancer, autoimmunity, and diabetes. Accurate models of cellular signalling are thus necessary for rational drug design and other applications central to national health. This project focuses on inferring models from experimental data. Specifically, it is interested in models of protein signalling because proteins control and mediate the vast majority of biological processes in a living cell. The project follows of the approach of executable biology: models of cell signalling are computer programs, which allows executing the models on the computer and comparing the model behavior against the behavior of the living cell observed in the lab setting. Most importantly for this project, viewing models as programs will allow the team to harness the recent advances in automatic synthesis of computer programs for synthesis of models from experimental measurements of cells. <br/><br/>The goal of the project is to provide biologists with a tool that synthesizes a variety of executable models from varied types of experimental data. To facilitate synthesis of mechanistic models from experimental data, the project will develop a family of modeling languages that will capture complex behaviors of biological systems, such as time and concurrency. The languages will be instances of the more general Boolean-Networks language. The team will investigate how to adjust the modeling abstraction based on the nature of available experimental data; the abstractions will be instantiated as suitably chosen languages from their language family. The modeling framework will be built by leveraging techniques from programming languages and formal methods such as meta-programming and constraint solving.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1535191","fuchsia_atom","SI2-SSE: Algorithms and Tools for Data-Driven Executable Biology"
"1265872","SI2-CHE: Collaborative Research: Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in CP2K Software Suite","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","4/15/2013","7/6/2014","Neeraj Rai","MS","Mississippi State University","Standard Grant","Evelyn M. Goldfield","3/31/2017","$230,558.00 ","","neerajrai@che.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","PO Box 6156, MISSISSIPPI STATE, MS","397629662","6623257404","MPS","1253|8004","5918|5946|5950|7433|8009|8650|9150|9216|9251","$0.00 ","An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), Jrg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre & Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite. These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals. Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement. The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.<br/><br/>The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications. The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base. The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry. The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265872","chartreuse_atom","SI2-CHE: Collaborative Research: Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in CP2K Software Suite"
"1265849","SI2-CHE: Collaborative Research: Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in the CP2K Software Suite","CHE","OFFICE OF MULTIDISCIPLINARY AC|Software Institutes","4/15/2013","4/2/2013","Joern Ilja Siepmann","MN","University of Minnesota-Twin Cities","Standard Grant","Evelyn M. Goldfield","3/31/2017","$349,250.00 ","","siepmann@umn.edu","200 OAK ST SE","Minneapolis","MN","200 OAK ST SE, Minneapolis, MN","554552070","6126245599","MPS","1253|8004","5918|5946|5950|7433|8009|8650","$0.00 ","An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), Jrg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre & Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite. These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals. Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement. The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.<br/><br/>The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications. The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base. The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry. The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes).",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1265849","chartreuse_atom","SI2-CHE: Collaborative Research: Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in the CP2K Software Suite"
"1216853","SI2: Conceptualization: Dynamic Languages for Scalable Data Analytics","ACI","ADVANCES IN BIO INFORMATICS|SPECIAL PROJECTS - CCF|CI-TEAM|Software Institutes","10/1/2012","11/18/2013","Jan Vitek","IN","Purdue University","Standard Grant","Daniel Katz","3/31/2014","$200,000.00 ","Ananth Grama, Suresh Jagannathan, William Cleveland, Olga Vitek","j.vitek@neu.edu","Young Hall","West Lafayette","IN","Young Hall, West Lafayette, IN","479072114","7654941055","CSE","1165|2878|7477|8004","7433|8211","$0.00 ","This planning grant gathers scientific community requirements for a set of capabilities termed Scalable Data Analytics. The project investigates community needs to support scientific discovery by providing an effective interface between extant hardware resources, data sources and repositories, and system software infrastructure. The proposed effort focuses on software environments and tools for data acquisition, management, visualization, sharing, and analysis for the working scientist, which can scale up to massively parallel and cloud fabrics, but, crucially, which can as easily scale down to a single laptop.<br/><br/>Software systems for data analytics are integral to the fabric of scientific innovation. The ability to acquire, process, and analyze large amounts of complex structured and unstructured data is at the core of diverse disciplines. While scientists can exploit large repositories of software tools optimized and refined over the years, significant new challenges are posed by the rapidly evolving characteristics of scientific datasets. These challenges are addressed by software systems that enable development of new software incrementally, modification of existing methods, or techniques for integrating pipelines of off-the-shelf components. For such application needs, scientists increasingly rely on dynamic computer programming languages. These languages facilitate interactive prototyping, support rapid development, and can be embedded or used to manage complex scientific software pipelines.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1216853","fuchsia_atom","SI2: Conceptualization: Dynamic Languages for Scalable Data Analytics"
"1216504","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms","ACI","Software Institutes","10/1/2012","9/13/2012","David Bader","GA","Georgia Tech Research Corporation","Standard Grant","Rudolf Eigenmann","9/30/2014","$104,386.00 ","Edward Riedy, Richard Vuduc","bader@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","Office of Sponsored Programs, Atlanta, GA","303320420","4048944819","CSE","8004","7433|8211","$0.00 ","Solving scientific grand challenges requires effective use of cyber infrastructure. Future computing platforms, including Field Programmable Gate Arrays (FPGAs), General Purpose Graphics Processing Units (GPGPUs), multi-core and multi-threaded processors, and Cloud computing platforms, can dramatically accelerate innovation to solve complex problems of societal importance when supported by a critical mass of sustainable software.<br/><br/>This project will organize scientific communities to help leverage the disruptive potential of future computing platforms through sustainable software. Grand challenge problems in biological science, social science, and security domains will be targeted based on their under-served needs and demonstrated possibilities. Users will be engaged through interdisciplinary workshops that bring together domain experts with software technologists with the goals of identifying core opportunity areas, determining critical software infrastructure, and discovering software sustainability challenges. The outcome will be an in-depth conceptual design for a Center for Sustainable Software on Future Computing Platforms, as part of the Software Infrastructure for Sustained Innovation (SI2) program. The design, scoped toward grand challenge problems, will identify common and specialized software infrastructure, research, development and outreach priorities, and coordination with the SSE and SSI components of the SI2 program. The interactions will offer a comprehensive understanding of grand challenges that best map to future computing platforms and the software infrastructure to best support scientists' needs. The workshops will enhance understanding of future platforms' potential for transformative research and lead to key insights into cross-cutting problems in leveraging their potential. Published results will help guide future research and reduce barriers to entry for under-represented groups.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1216504","fuchsia_atom","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms"
"1448360","Workshop on Supporting Scientific Discovery through Norms and Practices for Software and Data Citation and Attribution","ACI","SCIENCE OF SCIENCE POLICY|Software Institutes|STAR Metrics","9/1/2014","8/18/2014","Stanley Ahalt","NC","University of North Carolina at Chapel Hill","Standard Grant","Daniel Katz","8/31/2015","$99,935.00 ","Thomas Carsey","ahalt@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","104 AIRPORT DR STE 2200, CHAPEL HILL, NC","275991350","9199663411","CSE","7626|8004|8022","7433|7556|8004","$0.00 ","Scientific researchers, and particularly academic researchers, are embedded in a reputation economy. Tenure, promotion, and acclaim are achieved through influential research. There are few incentives for scientists to share data and software, and tenure and promotion decisions lack consideration of such activities; and to compound the problem, there are disincentives such as risking the loss of attribution. Some scientists distrust the public access model for software and data, and prefer to share data and software only by personal request, which assures attribution through personal contact and implicit social contract. There is also a lack of well-developed metrics with which to assess the impact and quality of scientific software and data. New practices and incentives are needed in the research community for software and data citation and attribution, so that data producers, software and tool developers, and data curators are credited for their contributions.<br/><br/>This workshop will facilitate a national, interdisciplinary exploration of new norms and practices for software and data citation and attribution, with the goal of informing the Science of Science and Innovation Policy (SciSIP) and Software Infrastructure for Sustained Innovation (SI2) NSF programs. Social and technical challenges facing current software development and data generation efforts will be identified and participants will explore viable methods and metrics to support software and data attribution in the scientific research community. This workshop will address registration of software and data, repositories for software and data, methods for tracking software and data usage, software and data annotation, collecting and curating metadata on software and data, ensuring appropriate attribution by software and data users, alternatives to traditional publication models for attribution, adaptation of commercial models for software and data attribution, proportioning attribution metrics to match degree of effort and role in software development and data generation, and establishing reward metrics for open science. Workshop outcomes will include actionable plans to enable the broader research community to implement the software and data attribution practices that are identified and advanced by the participants of the workshop.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=1448360","fuchsia_atom","Workshop on Supporting Scientific Discovery through Norms and Practices for Software and Data Citation and Attribution"
"9358367","NSF Young Investigator","ECCS","ELECT|PHOTONICS|& MAG DEVICE|METALS|CERAMICS|& ELEC MATRS|ELECTRONIC/PHOTONIC MATERIALS","8/15/1993","5/27/1997","Gentry Crook","WI","University of Wisconsin-Madison","Continuing grant","Lawrence S. Goldberg","5/31/1999","$233,665.00 ","","crook@engr.wisc.edu","21 North Park Street","MADISON","WI","21 North Park Street, MADISON, WI","537151218","6082623822","ENG","1517|1715|1775","0000|9161|9297|AMPP|OTHR|9162|9165|1775|9231","$0.00 ","9358367 Crook The research performed with this NSF Young Investigator Award involves growth by molecular beam epitaxy (MBE) of novel semiconductor structures for electronic and photonic device applications. One current research program examines the synthesis and properties of interfaces, superlattices, and alloys formed by combing elemental and III-V compound semiconductors. The Young Investigators Award will enhance this program by providing funding for additional experiments to examine the properties of novel materials such as (GaAs)1-x(Si2)x alloys and GaAs/Si short-period superlatties. A research program in planar devices formed by patterning of two-dimensional electron gas structures grown by MBE will be initiated during the period of the award. Interaction with industry will be pursed for research in these areas, as well as on related topics such as reproducible growth conditions for MBE. A course on compound semiconductor devices will be developed and first taught in the Fall of 1993, and other new courses will be developed in subsequent years. ***",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=9358367","azure_tree","NSF Young Investigator"
"953484","Inspiration, Frustration, and Fascination: An Excursion into Low-Oxidation State Main Group Chemistry","CHE","Chemical Synthesis","2/1/2010","1/30/2012","Gregory Robinson","GA","University of Georgia Research Foundation Inc","Continuing grant","Tingyu Li","1/31/2014","$500,000.00 ","Yuzhong Wang","robinson@chem.uga.edu","310 East Campus Rd","ATHENS","GA","310 East Campus Rd, ATHENS, GA","306021589","7065425939","MPS","6878","9146|MANU","$0.00 ","This research award in the Chemical Synthesis (SYN) program supports work by Professor Gregory H. Robinson at The University of Georgia to probe the synthesis of a series of provocative low-oxidation state main group compounds. Importantly, sterically demanding N-heterocyclic carbene ligands will be utilized as stabilizing agents in this project. Major goals include: (a) the synthesis of various low-oxidation state boron compounds; (b) the preparation of interesting phosphorus anions and main group clusters; and (c) the synthesis of carbene-stabilized dimetallic complexes. It is expected that the fundamental discoveries that will result from this work will augment and expand our knowledge of highly reactive diatomic molecules such as B2, P2, and Si2. In addition, the carbene-stabilization of highly reactive molecules suggests unprecedented opportunities in synthetic chemistry from catalysis to materials chemistry. <br/><br/>Undergraduate students, graduate students, and postdoctoral fellows (from diverse backgrounds) engaged in this work will gain valuable experience in basic synthetic chemistry, the manipulation of air-sensitive compounds, single-crystal X-ray diffractometry, and basic computational chemistry.",,"http://www.nsf.gov/awardsearch/showAward?AWD_ID=953484","chartreuse_tree","Inspiration, Frustration, and Fascination: An Excursion into Low-Oxidation State Main Group Chemistry"